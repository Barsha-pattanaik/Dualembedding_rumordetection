{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B9qTU6sO5O_"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw7jioT3PIOE"
      },
      "outputs": [],
      "source": [
        "# !pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar6V5uRhN5so"
      },
      "outputs": [],
      "source": [
        "# # from google.colab import drive\n",
        "# # drive.mount('/content/gdrive')\n",
        "# import os\n",
        "# #os.chdir('/content/gdrive/MyDrive/TextGCN_analysis-main')\n",
        "# #Import Libraries\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# import numpy as np\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# import string\n",
        "# import sklearn\n",
        "# from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# #from sklearn.decomposition import TruncatedSVD\n",
        "# #from sklearn.manifold import TSNE\n",
        "# import seaborn as sns\n",
        "# #from sklearn.model_selection import StratifiedKFold\n",
        "# import scipy.sparse as sp\n",
        "\n",
        "# #from tqdm import tqdm\n",
        "# #import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.nn import CrossEntropyLoss\n",
        "# import torch.nn as nn\n",
        "\n",
        "# #sns.set_theme()\n",
        "# #sns.set_context(\"talk\")\n",
        "# !pip install nltk\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# stopwords=set(stopwords.words('english'))\n",
        "# lemmatizer=WordNetLemmatizer()\n",
        "# seed = 42\n",
        "# #for text pre-processing\n",
        "# import re, string\n",
        "# import nltk\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import SnowballStemmer\n",
        "# from nltk.corpus import wordnet\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# #for model-building\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# # bag of words\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# #from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# #for word embedding\n",
        "# import gensim\n",
        "# from gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets\n",
        "# from google.colab import drive\n",
        "# #drive.mount('/content/gdrive')\n",
        "# import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbsfwDeMUCez"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade tensorflow keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNNuS-Ck7jwu"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall keras tensorflow keras-nightly keras-Preprocessing keras-vis keras-applications\n",
        "# !pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw8bg1RtOXwX"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from keras.models import Model\n",
        "# from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "# from keras.optimizers import RMSprop\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# #from keras.preprocessing import sequence\n",
        "# from keras.utils import to_categorical\n",
        "# from keras.utils import pad_sequences\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# %matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV3CAtwj8dwr"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eesTf0S5UCab"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.utils import to_categorical, pad_sequences\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "# from tensorflow.keras.optimizers import RMSprop\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.utils import to_categorical, pad_sequences\n",
        "# from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2whKGh_t8mO3"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.utils import to_categorical, pad_sequences\n",
        "\n",
        "# print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "-gtoHW30OT5X",
        "outputId": "297194ab-4dc3-42e0-88ff-bbb42c753928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1221, 2)\n",
            "Label\n",
            "non-rumor    699\n",
            "rumor        522\n",
            "Name: count, dtype: int64\n",
            "Index(['non-rumor', 'rumor'], dtype='object', name='Label')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  Label\n",
              "0  BREAKING: Hostages are being forced to hold an...  rumor\n",
              "1  BREAKING: Gunman takes hostages in cafe in Mar...  rumor\n",
              "2  UPDATE: An ISIS flag is being displayed in the...  rumor\n",
              "3  #BREAKING: Hostages are being held and a siege...  rumor\n",
              "4  BREAKING: A Sydney cafe at Martin Place is bei...  rumor"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5131bd74-1af9-4fd0-ac40-376ce2922fef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BREAKING: Hostages are being forced to hold an...</td>\n",
              "      <td>rumor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BREAKING: Gunman takes hostages in cafe in Mar...</td>\n",
              "      <td>rumor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>UPDATE: An ISIS flag is being displayed in the...</td>\n",
              "      <td>rumor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#BREAKING: Hostages are being held and a siege...</td>\n",
              "      <td>rumor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BREAKING: A Sydney cafe at Martin Place is bei...</td>\n",
              "      <td>rumor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5131bd74-1af9-4fd0-ac40-376ce2922fef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5131bd74-1af9-4fd0-ac40-376ce2922fef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5131bd74-1af9-4fd0-ac40-376ce2922fef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d84afe46-f250-42e6-8590-f5f106c28837\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d84afe46-f250-42e6-8590-f5f106c28837')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d84afe46-f250-42e6-8590-f5f106c28837 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "traindf",
              "summary": "{\n  \"name\": \"traindf\",\n  \"rows\": 1221,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1217,\n        \"samples\": [\n          \"Police outside the Lindt cafe in Martin Place. Photo by Fairfax photographer Daniel Munoz. #SydneySiege http://t.co/2bmB5jRqjI\",\n          \"Local media: 3 people appear to escape from Martin Place, Sydney, caf\\u00e9, amid hostage situation - @ABCNews http://t.co/N5mmnilz4Z\",\n          \"If I understand Hillary Clinton &amp; Sen Feinstein correctly, 1st step for current Sydney hostage situation is to empathize w the f'n bastards.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"non-rumor\",\n          \"rumor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 177
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# traindf = pd.read_excel('/content/gdrive/MyDrive/twitter15xlsl.xlsx')\n",
        "#traindf = pd.read_excel('/content/gdrive/MyDrive/twitter16.xlsx')\n",
        "# traindf = pd.read_excel('/content/gdrive/MyDrive/ottawashooting.xlsx')\n",
        "#traindf = pd.read_excel('/content/gdrive/MyDrive/newcollecteddat.xlsx')\n",
        "traindf = pd.read_excel('/content/gdrive/MyDrive/sidneysiedge.xlsx')\n",
        "# traindf = pd.read_excel('/content/gdrive/MyDrive/ferguson.xlsx')\n",
        "# traindf = pd.read_excel('/content/gdrive/MyDrive/rumorbigdata.xlsx')\n",
        "# traindf = pd.read_excel('/content/gdrive/MyDrive/charlihbdo.xlsx')\n",
        "# traindf = pd.read_excel('/content/gdrive/MyDrive/germanwingscrash.xlsx')\n",
        "# traindf = pd.read_excel('/content/gdrive/MyDrive/weibo1594.xlsx')\n",
        "\n",
        "\n",
        "traindf.head()\n",
        "# traindf.columns = ['ID', 'Text', 'Label']\n",
        "traindf.columns = ['Text', 'Label']\n",
        "print(traindf.shape)\n",
        "traindf.head()\n",
        "# CLASS DISTRIBUTION\n",
        "#if dataset is balanced or not\n",
        "x=traindf['Label'].value_counts()\n",
        "print(x)\n",
        "print(x.index)\n",
        "sns.barplot(y=x,x=x.index)\n",
        "traindf.isna().sum()\n",
        "traindf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgPO0HewOZ11",
        "outputId": "6a449fc1-fa1d-447a-e59b-cdb6d1808a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "X = traindf['Text']\n",
        "#Y = traindf.Label\n",
        "Y = traindf['Label']\n",
        "print(type(Y))\n",
        "#print(Y)\n",
        "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
        "\n",
        "# Input: \"reviewText\", \"rating\" and \"time\"\n",
        "# Target: \"log_votes\"\n",
        "X_train, X_test, y_train, y_test = train_test_split(traindf[\"Text\"],\n",
        "                                                  traindf[\"Label\"],\n",
        "                                                  test_size=0.1,\n",
        "                                                  shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI2iefESUCWU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFt0ycAhOb1v",
        "outputId": "fac9227a-94eb-4aae-c956-cd1914563594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1098\n",
            "123\n"
          ]
        }
      ],
      "source": [
        "train=X_train.values.tolist()\n",
        "test=X_test.values.tolist()\n",
        "trainl=y_train.values.tolist()\n",
        "testl=y_test.values.tolist()\n",
        "original_train_sentences = train\n",
        "original_labels_train = trainl\n",
        "original_test_sentences = test\n",
        "original_labels_test =  testl\n",
        "train_size = len(original_train_sentences)\n",
        "test_size = len(original_test_sentences)\n",
        "sentences = original_train_sentences + original_test_sentences\n",
        "print(len(original_train_sentences))\n",
        "print(len(original_test_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#final code.ipynb\n",
        "#original code\n",
        "\n",
        "\n",
        "# Dataset Preparation\n",
        "\n",
        "\n",
        "# original_train_sentences =\n",
        "# original_labels_train =\n",
        "# original_test_sentences =\n",
        "# original_labels_test =\n",
        "\n",
        "# example\n",
        "# original_train_sentences = ['this is sample 1','this is sample 2']\n",
        "# original_labels_train = ['postive','negative']\n",
        "# original_test_sentences = ['this is sample 1','this is sample 2']\n",
        "# original_labels_test = ['postive','negative']\n",
        "\n",
        "train_size = len(original_train_sentences)\n",
        "test_size = len(original_test_sentences)\n",
        "sentences = original_train_sentences + original_test_sentences\n",
        "\n",
        "\n",
        "# Hyper Parameters\n",
        "\n",
        "\n",
        "EDGE = 2 # 0:d2w 1:d2w+w2w 2:d2w+w2w+d2d\n",
        "NODE = 1 # 0:one-hot #1:BERT\n",
        "NUM_LAYERS = 2\n",
        "\n",
        "HIDDEN_DIM = 200\n",
        "DROP_OUT = 0.5\n",
        "LR = 0.02\n",
        "WEIGHT_DECAY = 0\n",
        "EARLY_STOPPING = 10\n",
        "NUM_EPOCHS = 200\n",
        "\n",
        "\n",
        "# Preprocess\n",
        "\n",
        "\n",
        "## Label Encoding\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "unique_labels=np.unique(original_labels_train)\n",
        "\n",
        "num_class = len(unique_labels)\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(unique_labels)\n",
        "\n",
        "print(unique_labels)\n",
        "print(lEnc.transform(unique_labels))\n",
        "\n",
        "train_labels = lEnc.transform(original_labels_train)\n",
        "test_labels = lEnc.transform(original_labels_test)\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "labels = train_labels.tolist()+test_labels.tolist()\n",
        "labels = torch.LongTensor(labels).to(device)\n",
        "\n",
        "\n",
        "## Remove Stopwords and less frequent words, tokenize sentences\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "remove_limit = 2\n",
        "\n",
        "\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "original_word_freq = {}  # to remove rare words\n",
        "for sentence in sentences:\n",
        "    temp = clean_str(sentence)\n",
        "    word_list = temp.split()\n",
        "    for word in word_list:\n",
        "        if word in original_word_freq:\n",
        "            original_word_freq[word] += 1\n",
        "        else:\n",
        "            original_word_freq[word] = 1\n",
        "\n",
        "tokenize_sentences = []\n",
        "word_list_dict = {}\n",
        "for sentence in sentences:\n",
        "    temp = clean_str(sentence)\n",
        "    word_list_temp = temp.split()\n",
        "    doc_words = []\n",
        "    for word in word_list_temp:\n",
        "        if word in original_word_freq and word not in stop_words and original_word_freq[word] >= remove_limit:\n",
        "            doc_words.append(word)\n",
        "            word_list_dict[word] = 1\n",
        "    tokenize_sentences.append(doc_words)\n",
        "word_list = list(word_list_dict.keys())\n",
        "vocab_length = len(word_list)\n",
        "\n",
        "#word to id dict\n",
        "word_id_map = {}\n",
        "for i in range(vocab_length):\n",
        "    word_id_map[word_list[i]] = i\n",
        "\n",
        "\n",
        "node_size = train_size + vocab_length + test_size\n",
        "\n",
        "\n",
        "# Model input\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "## Build Graph\n",
        "\n",
        "\n",
        "from math import log\n",
        "row = []\n",
        "col = []\n",
        "weight = []\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn_Bz_Xx0acd",
        "outputId": "adeab412-1f39-4955-bf8a-6ab843e13308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['non-rumor' 'rumor']\n",
            "[0 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### word-word: PMI\n",
        "\n",
        "if EDGE >= 1:\n",
        "    window_size = 20\n",
        "    total_W = 0\n",
        "    word_occurrence = {}\n",
        "    word_pair_occurrence = {}\n",
        "\n",
        "    def ordered_word_pair(a, b):\n",
        "        if a > b:\n",
        "            return b, a\n",
        "        else:\n",
        "            return a, b\n",
        "\n",
        "    def update_word_and_word_pair_occurrence(q):\n",
        "        unique_q = list(set(q))\n",
        "        for i in unique_q:\n",
        "            try:\n",
        "                word_occurrence[i] += 1\n",
        "            except:\n",
        "                word_occurrence[i] = 1\n",
        "        for i in range(len(unique_q)):\n",
        "            for j in range(i+1, len(unique_q)):\n",
        "                word1 = unique_q[i]\n",
        "                word2 = unique_q[j]\n",
        "                word1, word2 = ordered_word_pair(word1, word2)\n",
        "                try:\n",
        "                    word_pair_occurrence[(word1, word2)] += 1\n",
        "                except:\n",
        "                    word_pair_occurrence[(word1, word2)] = 1\n",
        "\n",
        "\n",
        "    for ind in tqdm(range(train_size+test_size)):\n",
        "        words = tokenize_sentences[ind]\n",
        "\n",
        "        q = []\n",
        "        # push the first (window_size) words into a queue\n",
        "        for i in range(min(window_size, len(words))):\n",
        "            q += [word_id_map[words[i]]]\n",
        "        # update the total number of the sliding windows\n",
        "        total_W += 1\n",
        "        # update the number of sliding windows that contain each word and word pair\n",
        "        update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "        now_next_word_index = window_size\n",
        "        # pop the first word out and let the next word in, keep doing this until the end of the document\n",
        "        while now_next_word_index<len(words):\n",
        "            q.pop(0)\n",
        "            q += [word_id_map[words[now_next_word_index]]]\n",
        "            now_next_word_index += 1\n",
        "            # update the total number of the sliding windows\n",
        "            total_W += 1\n",
        "            # update the number of sliding windows that contain each word and word pair\n",
        "            update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "    for word_pair in word_pair_occurrence:\n",
        "        i = word_pair[0]\n",
        "        j = word_pair[1]\n",
        "        count = word_pair_occurrence[word_pair]\n",
        "        word_freq_i = word_occurrence[i]\n",
        "        word_freq_j = word_occurrence[j]\n",
        "        pmi = log((count * total_W) / (word_freq_i * word_freq_j))\n",
        "        if pmi <=0:\n",
        "            continue\n",
        "        row.append(train_size + i)\n",
        "        col.append(train_size + j)\n",
        "        weight.append(pmi)\n",
        "        row.append(train_size + j)\n",
        "        col.append(train_size + i)\n",
        "        weight.append(pmi)\n",
        "\n",
        "\n",
        "\n",
        "### doc-word: Tf-idf\n",
        "\n",
        "\n",
        "#get each word appears in which document\n",
        "word_doc_list = {}\n",
        "for word in word_list:\n",
        "    word_doc_list[word]=[]\n",
        "\n",
        "for i in range(len(tokenize_sentences)):\n",
        "    doc_words = tokenize_sentences[i]\n",
        "    unique_words = set(doc_words)\n",
        "    for word in unique_words:\n",
        "        exsit_list = word_doc_list[word]\n",
        "        exsit_list.append(i)\n",
        "        word_doc_list[word] = exsit_list\n",
        "\n",
        "#document frequency\n",
        "word_doc_freq = {}\n",
        "for word, doc_list in word_doc_list.items():\n",
        "    word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "# term frequency\n",
        "doc_word_freq = {}\n",
        "\n",
        "for doc_id in range(len(tokenize_sentences)):\n",
        "    words = tokenize_sentences[doc_id]\n",
        "    for word in words:\n",
        "        word_id = word_id_map[word]\n",
        "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "        if doc_word_str in doc_word_freq:\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "        else:\n",
        "            doc_word_freq[doc_word_str] = 1\n",
        "\n",
        "\n",
        "for i in range(len(tokenize_sentences)):\n",
        "    words = tokenize_sentences[i]\n",
        "    doc_word_set = set()\n",
        "    for word in words:\n",
        "        if word in doc_word_set:\n",
        "            continue\n",
        "        j = word_id_map[word]\n",
        "        key = str(i) + ',' + str(j)\n",
        "        freq = doc_word_freq[key]\n",
        "        if i < train_size:\n",
        "            row.append(i)\n",
        "        else:\n",
        "            row.append(i + vocab_length)\n",
        "        col.append(train_size + j)\n",
        "        idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n",
        "        weight.append(freq * idf)\n",
        "        doc_word_set.add(word)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b1c5efec4c76459aac7a63e93c2f63b2",
            "a87e822c76324659b23fad3d3d9412d1",
            "802796ba98e2414cb7898d5956b74bb8",
            "b8b89ac4cd4f4428b1653eb0ee932b22",
            "b4938aa948a6456ea0b520603374849e",
            "4f8b34eabe6d48e28016f5a923019a2a",
            "06100742bd6b4013bcd4db35ac30ddaa",
            "3d858994e267440c900a3896524ae428",
            "8b54ebb8b9094262831ee077adb42542",
            "5e8b34f741744305bf4c1472d2941fcc",
            "c731ec43a7364bc39a15afdad4a9d8a7"
          ]
        },
        "id": "vqZlW1530kvE",
        "outputId": "fc39a401-0839-4596-c70c-945bd1952829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1221 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1c5efec4c76459aac7a63e93c2f63b2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "if EDGE >= 2:\n",
        "    tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
        "    jaccard_threshold = 0.2\n",
        "\n",
        "    for i in tqdm(range(len(tokenize_sentences))):\n",
        "        for j in range(i + 1, len(tokenize_sentences)):\n",
        "            union_set = tokenize_sentences_set[i].union(tokenize_sentences_set[j])\n",
        "\n",
        "            if len(union_set) == 0:\n",
        "                continue  # Skip if the union of both sets is empty\n",
        "\n",
        "            # Calculate Jaccard distance\n",
        "            jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n",
        "\n",
        "            if jaccard_w > jaccard_threshold:\n",
        "                if i < train_size:\n",
        "                    row.append(i)\n",
        "                else:\n",
        "                    row.append(i + vocab_length)\n",
        "\n",
        "                if j < train_size:\n",
        "                    col.append(j)\n",
        "                else:\n",
        "                    col.append(vocab_length + j)\n",
        "\n",
        "                weight.append(jaccard_w)\n",
        "\n",
        "                if j < train_size:\n",
        "                    row.append(j)\n",
        "                else:\n",
        "                    row.append(j + vocab_length)\n",
        "\n",
        "                if i < train_size:\n",
        "                    col.append(i)\n",
        "                else:\n",
        "                    col.append(vocab_length + i)\n",
        "\n",
        "                weight.append(jaccard_w)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2sv6hJ-fBHp",
        "outputId": "48e1a945-fa30-415b-ec04-5e3ef8b10a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1221/1221 [00:03<00:00, 308.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### doc-doc: jaccard\n",
        "\n",
        "# import nltk\n",
        "\n",
        "# if EDGE>=2:\n",
        "#     tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
        "#     jaccard_threshold = 0.2\n",
        "#     for i in tqdm(range(len(tokenize_sentences))):\n",
        "#         for j in range(i+1, len(tokenize_sentences)):\n",
        "#             if len(tokenize_sentences_set[i].union(tokenize_sentences_set[j])) == 0:\n",
        "#                continue  # Skip if both sets are empty\n",
        "#         jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n",
        "#         if jaccard_w > jaccard_threshold:\n",
        "#             if i < train_size:\n",
        "#                row.append(i)\n",
        "#             else:\n",
        "#                 row.append(i + vocab_length)\n",
        "#             if j < train_size:\n",
        "#                col.append(j)\n",
        "#             else:\n",
        "#                 col.append(vocab_length + j)\n",
        "#             weight.append(jaccard_w)\n",
        "#             if j < train_size:\n",
        "#                row.append(j)\n",
        "#             else:\n",
        "#                 row.append(j + vocab_length)\n",
        "#             if i < train_size:\n",
        "#                col.append(i)\n",
        "#             else:\n",
        "#                 col.append(vocab_length + i)\n",
        "#             weight.append(jaccard_w)"
      ],
      "metadata": {
        "id": "3xE2r5NB01JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CZaifnmat9a"
      },
      "outputs": [],
      "source": [
        "### Adjacent matrix\n",
        "\n",
        "\n",
        "import scipy.sparse as sp\n",
        "adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
        "\n",
        "# build symmetric adjacency matrix\n",
        "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo(), d_inv_sqrt\n",
        "\n",
        "adj, norm_item = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape).to(device)\n",
        "\n",
        "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEFsG6sYP4DD",
        "outputId": "c6a534ca-1d59-4518-e58b-d488aabd4d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['updated', 'shots', 'fired', 'parliament', 'hill', ',', 'soldier', 'injured', 'national', 'war', 'memorial', 'http', 'co', 'http', 'co'], ['snipers', 'top', 'ottawa', \"'s\", 'national', 'art', 'gallery', 'gunfire', 'parliament', 'http', 'co', '\\\\(', 'dmatthewmillar', 'pic', '\\\\)'], ['thinking', 'everyone', 'ottawa', 'right', 'please', 'careful', 'stay', 'safe', 'situation', 'control'], ['nhl', 'cancels', 'tonight', \"'s\", 'game', 'ottawa', 'toronto', 'due', 'today', \"'s\", 'shootings', 'senators', 'host', 'maple', 'leafs'], ['things', 'remember', 'ottawashooting', 'http', 'co'], ['new', 'ottawa', 'police', 'reporting', '3rd', 'shooting', 'scene', ',', 'rideau', 'centre', 'mall', ',', 'close', 'parliament', 'hill'], ['canada', \"'s\", 'parliament', 'building', 'ottawa', 'lockdown', 'reports', 'active', 'shooter', 'http', 'co', 'http', 'co'], ['rest', 'peace', ',', 'cpl', 'nathan', 'cirillo', 'killed', 'today', 'ottawashooting', 'http', 'co', 'http', 'co'], ['prayers', 'ottawa', ',', 'hope', 'resolved'], ['ottawa', 'police', 'say', '3', 'shootings', 'ottawa', 'parliament', ',', 'war', 'memorial', 'rideau', 'centre', 'mall', 'http', 'co'], ['breaking', 'news', 'ottawa', 'police', 'confirm', 'one', 'suspect', 'killed', '2', 'suspects', 'remain', 'large'], ['rcmp', 'team', 'members', 'parliament', 'hill', 'http', 'co', 'ao700dweow', 'http', 'co'], ['latest', 'update', 'attacker', 'shot', 'dead', 'parliament', 'soldier', 'shot', 'national', 'war', 'memorial', 'http', 'co', 'pp6hcfwcrw', 'http', 'co'], ['canadian', 'soldier', 'fatally', 'shot', 'identified', 'nathan', 'cirillo', 'ottawashooting', 'hamont', 'rip', 'http', 'co'], ['ottawa', 'police', 'police', 'stations', 'closed', 'due', 'ongoing', 'shooting', 'situation', 'public', 'services', 'available', 'notice', 'worldnews'], ['thoughts', 'go', 'today', \"'s\", 'attacks', 'ottawa', 'westandonguardforthee'], ['breaking', 'soldier', 'shot', 'near', 'canada', \"'s\", 'parliament', 'active', 'shooter', 'large', 'http', 'co', 'http', 'co'], ['thinking', 'friends', 'ottawa', 'morning', 'please', 'safe'], ['canadian', 'coverage', 'ottawa', 'shootings', ',', 'calm', ',', 'facts', 'ca', \"n't\", '\\\\?', 'http', 'co'], ['shooting', 'canadian', 'parliament', 'comes', 'day', 'isis', 'radical', 'canadian', 'soldier', 'also', 'reports', 'shooting', 'war', 'memorial', '1'], ['new', 'sources', 'deceased', 'gunman', 'killed', 'soldier', 'ottawa', 'identified', 'canadian', 'national', 'michael', 'zehaf', 'bibeau', 'worldnews'], ['rt', 'incredible', 'shot', 'woman', 'gives', 'mouth', 'mouth', 'fallen', 'soldier', 'war', 'memorial', 'http', 'co'], ['ottawa', 'police', 'report', 'third', 'shooting', 'incident', 'rideau', 'centre', 'word', 'injuries'], ['latest', 'shooting', 'ottawa', 'w', 'dramatic', 'video', 'inside', 'canadian', 'parliament', 'building', 'globeandmail', 'http', 'co'], ['ottawa', 'police', 'confirms', '1', 'gunman', 'dead', ',', '1', 'soldier', 'killed', 'ottawashooting', 'http', 'co', 'http', 'co'], ['soldier', 'shot', 'war', 'memorial', 'ottawa', 'died', 'live', 'coverage', 'http', 'co'], ['developing', 'story', 'shots', 'fired', 'parliament', 'hill', ',', 'soldier', 'shot', 'war', 'memorial', 'http', 'co', 'ottawa', 'http', 'co'], ['update', 'lockdown', 'rideau', 'centre', 'ottawa'], ['ottawa', ',', 'safe', ',', 'hill', 'attack'], ['police', 'multiple', 'shooters', 'ottawa', 'pm', 'safe', 'downtown', 'buildings', 'lockdown', 'http', 'co', 'http', 'co'], ['photos', 'love', 'ottawa', 'http', 'co', 'http', 'co'], ['canadians', ',', 'thoughts', 'family', 'canadianforces', 'member', 'killed', 'today', 'ottawashooting'], ['thinking', 'affected', 'today', \"'s\", 'tragedy', 'ottawa'], ['hero', 'kevin', 'vickers', ',', 'head', 'security', 'parliament', ',', 'credited', 'taking', 'shooter', 'ottawa', 'http', 'co'], ['thoughts', 'everyone', 'city', 'ottawa', 'today'], ['extended', 'video', 'guns', ',', 'officers', 'rush', 'rideaucentre', 'http', 'co', 'http', 'co'], ['soldier', 'killed', 'ottawa', 'today', 'named', '24', 'year', 'old', 'nathan', 'cirillo', 'latest', 'http', 'co', 'http', 'co'], ['security', 'heightened', 'across', 'canada', 'amid', 'ottawa', 'attack', 'http', 'co', 'http', 'co'], ['security', 'parliament', 'hill', 'ottawa', 'http', 'co'], ['video', 'cbc', 'nn', 'globeandmail', 'video', 'showing', 'shots', 'fired', 'inside', 'parliament', 'hill', 'cdnpoli', 'http', 'co', 'http', 'co'], ['first', 'response', 'ottawashooting', 'make', ',', 'sadly'], ['thoughts', 'go', 'family', 'nathan', 'cirillo', ',', '24', ',', 'identified', 'soldier', 'killed', 'ottawa', 'today', 'http', 'co'], ['soldier', 'shot', 'dead', 'ottawa', 'today', 'cpl', 'nathan', 'cirillo', 'reservist', 'based', 'hamilton', ',', 'ont'], ['sending', 'thoughts', 'prayers', 'everybody', 'ottawa', 'ottawastrong', 'canadastrong', 'http', 'co'], ['breaking', 'terrible', 'news', 'day', 'far', 'soldier', 'died', 'injuries', 'ottawashooting', 'ctvnews', 'ctvottawa'], ['hearing', 'news', ',', 'thoughts', 'today', 'back', 'home', 'citizens', 'dedicated', 'public', 'servants', 'ottawa', 'across', 'country'], ['breaking', 'soldier', 'shot', 'war', 'memorial', 'near', 'canadian', 'parliament', 'http', 'co'], ['soldier', 'died', 'today', 'ottawa', 'war', 'memorial', 'name', 'cpl', 'nathan', 'cirillo', 'rip', 'ottawastrong', 'http', 'co'], ['incredible', 'shot', 'woman', 'gives', 'mouth', 'mouth', 'fallen', 'soldier', 'war', 'memorial', 'http', 'co', 'http', 'co'], ['leafs', 'currently', 'lockdown', 'hotel', 'downtown', 'ottawa'], ['tonight', \"'s\", 'game', 'maple', 'leafs', 'amp', 'senators', 'postponed', 'shootings', 'ottawa', 'morning', 'http', 'co'], ['breaking', 'two', 'shootings', 'parliament', 'hill', 'one', 'victims', 'believed', 'member', 'canadian', 'forces'], ['nhl', 'says', 'game', 'nhl', 'express', 'sympathy', 'prayers', 'affected', 'tragic', 'events'], ['please', 'stay', 'inside', 'stay', 'safe', 'downtown', 'core', 'right', 'ottawa', 'anyone', 'needs', 'talk', '613', '238', '3311'], ['safe', 'ottawa', '!', 'praying', 'everyone', 'involved', 'today'], ['update', 'parliament', 'hill', 'attack', 'soldier', 'dies', 'injuries', ',', 'gunman', 'shot', 'dead', 'http', 'co', 'pp6hcfwcrw', 'ottawashooting', 'http', 'co'], ['ottawa', 'hospital', 'says', \"'s\", 'treating', 'three', 'patients', ',', 'two', 'stable', 'condition', ',', 'shooting', 'incident', 'http', 'co', '1ckihwuczl'], ['breaking', 'minister', 'employment', 'jason', 'kenney', 'confirms', 'soldier', 'shot', 'ottawa', 'dead', 'http', 'co', 'http', 'co'], ['ottawa', 'police', 'say', 'dealing', 'multiple', 'suspects', ',', 'ca', \"n't\", 'confirm', 'whether', 'guns', 'confirming', 'multiple', 'suspects'], ['breaking', 'two', 'three', 'gunmen', 'involved', 'shooting', 'canada', 'war', 'memorial', ',', 'ottawa', 'police', 'say', 'http', 'co', 'qsangnqbew'], ['video', 'seconds', 'shooting', 'nathan', 'cirillo', ',', 'ottawashooting', 'suspect', 'gets', 'car', 'http', 'co'], [\"'re\", 'ottawa', ',', 'scared', ',', 'anxious', 'today', \"'s\", 'events', ',', \"n't\", 'call', '613', '238', '3311', 'ottawashooting'], ['mps', 'sergeant', 'arms', 'lives', 'parliament', 'hill', 'shootings', 'http', 'co', 'http', 'co'], ['michael', 'bibeau', 'caught', 'http', 'co', 'ottawashooting', 'http', 'co'], ['thoughts', 'ottawa', 'morning', 'stay', 'safe'], ['map', 'three', 'ottawa', 'locations', 'police', 'say', 'shots', 'fired', 'http', 'co', 'http', 'co'], ['canada', 'prime', 'minister', 'stephen', 'harper', 'safe', ',', 'left', 'parliament', 'hill', 'tv', 'pm', \"'s\", 'office'], ['ottawa', 'police', 'say', 'dealing', 'multiple', 'suspects', 'confirm', 'whether', 'guns', 'ottawa', 'still', 'much', 'locked'], ['ottawa', 'police', 'stations', 'closed', 'public', 'due', 'shooting', 'war', 'memorial', 'public', 'services', 'available', 'notice', 'ottnews'], ['pittsburgh', 'penguins', 'canada', 'tonight', 'vs', 'honour', 'event', 'ottawa', 'http', 'co'], ['soldier', 'killed', 'canada', 'shooting', 'young', 'reservist', 'six', 'year', 'old', 'son', 'http', 'co', 'http', 'co'], ['breaking', 'news', 'one', 'soldier', 'died', 'ottawa', 'shooting', ',', 'suspect', '\\\\(', '\\\\)', 'still', 'large', 'http', 'co', 'lxfiyavimz'], ['thoughts', 'prayers', 'everyone', 'ottawa', 'prayforottawa'], ['shooting', 'someone', 'already', 'bad', ',', 'shooting', 'soldier', 'guarding', 'war', 'memorial', 'prayforottawa'], ['video', 'reporter', 'captures', 'ottawa', \"'s\", 'parliament', 'hill', 'building', 'watch', 'http', 'co', 'http', 'co'], ['sorry', \"n't\", 'america', 'gun', 'control', 'one', 'ottawa', 'shooting'], ['ottawa', 'shooting', 'events', 'unfolded', ',', 'first', 'shots', 'fired', '9', 'war', 'memorial', 'http', 'co', 'cuzepktlxx', 'http', 'co'], ['nhl', 'postpones', 'wednesday', \"'s\", 'leafs', 'senators', 'game', 'due', 'tragedy', 'ottawa', 'http', 'co', 'http', 'co'], ['penguins', 'also', 'canadian', 'national', 'anthem', 'tonight', \"'s\", 'game', 'gesture', 'pens', ',', 'pittsburgh', 'ottawa'], ['rcmp', 'advises', 'downtown', 'ottawa', 'stay', 'away', 'windows', 'roofs', 'due', 'ongoing', 'police', 'incident'], ['thoughts', 'ottawa', 'everyone', 'affected', 'today', \"'s\", 'events'], ['isis', 'media', 'account', 'posts', 'picture', 'claiming', 'michael', 'zehaf', 'bibeau', ',', 'dead', 'ottawashooting', 'suspect', 'canada', 'http', 'co'], ['still', ',', 'barricaded', 'office', ',', 'ottawashooting'], ['police', 'ottawa', 'searching', 'suspected', 'gunmen', 'shot', 'solider', 'fired', 'shots', 'parliament', 'morning', 'http', 'co', '1ckihwuczl'], ['chairs', 'door', 'mt', 'grahamctv', 'photo', 'inside', 'caucus', 'room', 'shooting', 'http', 'co'], ['thoughts', 'amp', 'prayers', 'family', 'amp', 'friends', 'soldier', 'killed', 'today', 'line', 'duty', 'ottawa', 'nation', 'loss'], ['watch', 'live', 'manhunt', 'canadian', 'parliament', 'shooting', 'suspect', 'http', 'co', 'via', 'ctvnews', 'http', 'co'], ['canada', 'amp', 'israel', ',', 'isis', 'right', 'ottawa', 'jerusalem', 'http', 'co'], ['everyone', 'ottawa', 'safe'], ['refuse', 'ontario', 'face', 'ottawa', 'shooting', 'http', 'co', 'http', 'co'], ['lockdown', 'parliament', ',', 'main', ',', 'u', ',', 'downtown', 'schools', ',', 'ottawa', 'police', 'hq', ',', 'cbc', 'hq', ',', 'rideau', 'centre', ',', 'cbcott', 'ottnews'], ['canadian', 'police', ',', ',', 'parliament', 'building', 'ottawa', 'http', 'co', 'http', 'co'], ['white', 'house', 'official', 'obama', 'briefed', 'shooting', 'incidents', 'canadian', 'capital'], ['breaking', 'soldier', 'shot', 'war', 'memorial', 'ottawa', ',', 'hill', 'lockdown', ',', 'shooter', 'large', 'http', 'co', 'ottnews'], ['ottawa', 'shooting', 'soldier', 'shot', 'memorial', 'parliament', 'hill', 'lockdown', 'harper', ',', ',', ',', 'safe', 'http', 'co'], ['prayers', 'ottawa', 'prayforottawa', 'http', 'co'], ['image', 'shows', 'police', 'running', 'search', 'buildings', 'downtown', 'ottawa', 'http', 'co'], ['parliament', 'hill', 'lockdown', 'soldier', 'shot', 'ottawa', \"'s\", 'war', 'memorial', ',', 'east', 'block'], ['ottawa', 'police', 'actively', 'looking', 'one', 'suspects', 'canadian', 'parliament', 'shooting', 'http', 'co'], ['thoughts', 'ottawa'], ['soldier', 'killed', 'ottawa', 'identified', 'cpl', 'nathan', 'cirillo', 'http', 'co', 'http', 'co'], ['canada', 'police', 'say', 'suspect', 'ottawa', 'shooting', 'still', 'large', ',', 'tactical', 'units', 'scene'], ['saddened', 'events', 'unfolding', 'ottawa', 'morning', 'thoughts', 'residents', 'amp', 'first', 'responders'], ['thoughts', 'prayers', 'go', 'everyone', 'downtown', 'ottawa', 'right', 'prayforottawa', 'special', 'thanks', 'officers', 'protecting', 'us'], ['developing', 'canada', 'officials', 'identify', 'dead', 'gunman', 'parliament', 'shooting', 'michael', 'zehaf', 'bibeau', 'http', 'co'], ['thoughts', 'prayers', 'city', 'ottawa'], ['thoughts', 'prayers', 'ottawa', 'affected', 'today'], ['rcmp', 'advises', 'people', 'stay', 'away', 'parliament', 'hill', 'due', 'ongoing', 'police', 'incident'], ['jason', 'kenney', 'says', 'canadians', 'soldier', 'shot', 'war', 'memorial', 'today', 'died', 'employment', 'minister', 'also', 'says', 'parliamentary', 'guard', 'wounded', 'cdnpoli'], ['always', 'say', 'ottawa', 'best', 'place', 'safe', 'nothing', 'today', 'people', 'change', 'capital', '!', '!'], ['breaking', 'reports', 'second', 'shooting', 'behind', 'peace', 'tower', ',', 'solider', 'shot', 'war', 'memorial', 'time', 'ago', 'shooter', 'loose'], ['ottawa', 'shooting', 'kevin', 'vickers', 'taking', 'gunman', 'canadian', 'parliament', 'http', 'co', 'http', 'co'], ['developing', 'mps', 'tweeting', 'gunman', 'shot', 'dead', 'cbc', 'confirmed', 'condition', 'soldier', 'also', 'unknown', 'http', 'co', 'qcwgn8oghb'], ['isis', 'media', 'account', 'posts', 'picture', 'claiming', 'michael', 'zehaf', 'bibeau', 'suspected', 'shooter', 'ottawa', 'http', 'co'], ['witnesses', 'say', 'several', 'dozen', 'shots', 'fired', 'inside', 'parliament', 'buildings', 'canadian', 'soldier', 'shot', 'nearby', 'war', 'memorial', 'ottawa', 'cdnpoli'], ['prayers', 'ottawa', ',', 'affected', ',', 'family', 'soldier', 'died', 'god', 'keep', 'land', 'free', 'westandonguardforthee'], ['thoughts', 'everyone', 'ottawa', 'affected', 'today', \"'s\", 'events', 'ottawastrong'], ['nhl', 'postpones', 'tonight', \"'s\", 'leafs', 'senators', 'game', 'ottawashooting', 'http', 'co', 'http', 'co'], ['ottawa', ',', 'stay', 'safe', 'amp', 'strong', 'thank', 'god', 'arms', 'kevin', 'vickers', 'amp', 'cdn', 'security', 'forces', 'true', 'heroes', 'cndpoli'], ['ottawa', 'police', 'confirm', 'shooting', 'war', 'memorial', 'reports', 'say', 'victim', 'may', 'soldier'], ['attack', 'parliament', 'soldier', 'shot', 'gunman', 'loose', 'follow', 'developing', 'story', 'http', 'co', 'http', 'co'], ['police', 'confirm', '3rd', 'shooting', 'area', 'rideau', 'centre', ',', 'mall', 'downtown', 'ottawa', 'east', 'parliament', 'hill', 'cdnpoli'], ['photos', 'ottawashooting', 'scene', 'unfolding', 'reports', 'multiple', 'shootings', 'http', 'co', 'http', 'co'], ['sending', 'love', 'amp', 'support', 'hometown', 'ottawa', 'canada', 'pls', 'everyone', 'safe', ',', 'thinking', 'u'], ['university', 'ottawa', 'lockdown', ',', 'cancelled'], ['shots', 'fired', 'parliament', 'hill', 'police', 'everyone', 'take'], ['dramatic', 'photos', 'wcuddington', 'scene', 'ottawa', 'http', 'co', 'm4fe8krpxl', 'http', 'co'], ['soldier', 'shot', 'gunman', 'outside', 'canada', 'parliament', 'dies', ',', 'minister', 'says', ',', 'police', 'search', 'multiple', 'suspects'], ['today', 'everyone', \"'s\", 'please', 'know', '24', '7', 'ottawa', 'amp', 'surroundings', 'areas', '613', '238', '3311'], ['conservative', 'caucus', 'told', 'canadian', 'soldier', 'shot', 'war', 'memorial', 'died', ',', 'soldier', 'named', ',', 'reservist', 'based', 'hamilton'], ['ottawa', 'police', 'statement', 'three', 'separate', 'shooting', 'incidents', 'inside', 'parliament', 'one', 'war', 'memorial', 'one', 'rideau', 'centre'], ['message', 'toronto', 'police', 'imams', 'mosques', 'wake', 'ottawashootings', 'http', 'co'], ['thoughts', 'prayers', 'everyone', 'ottawa', 'morning'], ['police', 'confirm', \"'several'\", 'shootings', 'ottawa', 'pm', 'safe', 'downtown', 'buildings', 'lockdown', 'http', 'co', 'http', 'co'], ['breaking', 'news', 'least', '3', 'shots', 'fired', 'ottawa', 'war', 'memorial', 'one', 'soldier', 'confirmed', 'shot', 'http', 'co', 'http', 'co'], ['anyone', 'photos', 'videos', 'today', \"'s\", 'shootings', ',', 'send', 'rcmp', 'nat', 'div', 'link', 'http', 'co', 'ottawa'], ['sergeant', 'arms', 'kevin', 'vickers', 'hailed', 'hero', 'shooting', 'canadian', 'parliament', 'gunman', 'http', 'co', 'http', 'co'], ['police', 'guns', 'drawn', 'war', 'memorial', 'shooting', 'ctvottawa', 'http', 'co'], ['ottawa', 'hospital', 'confirms', \"'s\", 'treating', '3', 'ottawashooting', 'victims', '2', 'stable', 'condition', 'http', 'co', 'http', 'co'], ['yet', ',', 'sadly', ',', 'evidence', 'brave', 'forces', 'amp', 'police', 'folks', 'courage', 'takes', 'rush', 'others', 'rush', 'ottawa'], ['updated', 'ottawa', 'attack', 'heightened', 'canada', 'target', 'http', 'co', 'http', 'co'], ['ottawapolice', 'incidents', 'occurred', 'national', 'war', 'memorial', ',', 'near', 'rideau', 'centre', 'parliament', 'hill', 'live', 'blog', 'http', 'co', 'q98amohu7t'], ['fear', 'happened', 'ottawa', ',', 'stay', 'safe', ',', 'ottawa', 'cdnpoli'], ['reminder', 'observing', 'shooting', 'parliament', 'hill', 'tweet', 'police', 'locations', 'movements', 'ottcity', 'ottawa'], ['witnesses', 'say', 'several', 'dozen', 'shots', 'fired', 'inside', 'parliament', 'buildings', 'canadian', 'soldier', 'shot', 'nearby', 'war', 'memorial', 'cbcott', 'ottnews'], ['thoughts', 'affected', 'today', 'events', 'ottawa'], ['gunman', 'killed', 'parliament', 'attack', 'named', 'michael', 'zehaf', 'bibeau', ',', 'canadian', 'born', '1982', ',', 'american', 'media', 'report', 'cdnpoli', 'ottawa'], ['ottawa', 'shootings', 'reportedly', 'three', 'locations', 'parliament', ',', 'war', 'memorial', 'shopping', 'mall', 'http', 'co', 'http', 'co'], ['monitoring', 'situation', 'ottawa', 'touch', '\\\\(', '1', '2', '\\\\)'], ['pittsburgh', 'penguins', \"'ll\", 'play', 'canadian', 'national', 'anthem', 'tonight', \"'s\", 'game', 'ottawashooting'], ['breaking', 'one', 'gunman', 'killed', 'following', 'shooting', 'parliament', 'hill', ',', 'sources', 'confirm', 'ctv', 'news'], ['due', 'shooting', 'parliament', 'hill', 'ottawa', 'earlier', ',', 'currently', 'lockdown', 'update', 'information', 'available'], ['shooting', 'soldiers', 'guarding', 'war', 'memorial', 'terrorism', 'really', \"n't\", 'get', 'much'], ['breaking', 'shooting', 'reported', 'war', 'memorial', 'ottawa'], ['video', 'petermansbridge', 'currently', 'know', 'ottawashooting', 'http', 'co', 'http', 'co'], ['ottawa', 'shooting', 'soldier', 'shot', 'war', 'memorial', 'parliament', 'hill', 'locked', 'pm', 'harper', 'safe', ',', 'http', 'co'], ['breaking', 'police', 'say', 'soldier', ',', '1', 'suspected', 'gunman', 'dead', 'ottawa', 'shootings'], ['shooting', 'war', 'memorial', 'ottawa', 'http', 'co'], ['u', 'officials', 'tell', 'cbs', 'news', 'name', 'dead', 'ottawa', 'shooting', 'suspect', 'michael', 'zehaf', 'bibeau', ',', 'canadian', 'born', '1982'], ['leafs', 'lock', 'tragedy', 'ottawa', 'thoughts', 'scene', 'http', 'co', 'http', 'co'], ['rip', 'solider', 'killed', 'ottawa', 'today', ',', 'praying', 'affected', 'shooting', 'staysafeottawa', 'http', 'co'], ['awful', 'day', 'ottawa', 'thoughts', 'prayers', 'senators', 'family', ',', 'entire', 'city', 'ottawa', 'canada'], ['u', 'four', 'blackwater', 'guards', 'guilty', 'every', 'related', '2007', 'baghdad', 'shooting'], ['r', 'told', \"'s\", 'still', 'safe', 'outside', 'downtown', 'shooter', 'loose', 'university', 'ottawa'], ['ottawa', 'police', 'confirm', '3', 'separate', 'shooting', 'incidents', 'today', '1', 'parliament', 'hill', ',', '1', 'war', 'memorial', '1', 'near', 'area', 'mall', 'cp24', ',', 'cbcnews'], ['update', 'attacker', 'shot', 'dead', 'parliament', 'soldier', 'shot', 'war', 'memorial', 'http', 'co', 'pp6hcfwcrw', 'ottawashooting', 'http', 'co'], ['ottnews', 'ctvottawa', 'reports', 'man', 'parliament', 'hill', 'many', '30', 'shots', 'fired', 'soldier', 'shot', 'war', 'memorial'], ['breaking', 'police', 'shooting', 'incident', 'reported', 'near', 'canadian', 'parliament', 'spokesperson', 'prime', 'minister', 'harper', 'safe', 'worldnews'], ['shot', 'ottawa', 'white', ',', 'going', 'terrorism', 'going', '\\\\?'], ['cnn', 'already', 'parliament', 'hill', 'shooting', 'break'], ['norad', 'gone', 'high', 'alert', 'shootings', 'ottawa', ',', 'u', 'military', 'officials', 'say'], ['university', 'ottawa', 'lockdown', 'cbcott', 'ottnews'], ['stay', 'safe', 'ottawa', 'stay', 'strong', 'canada'], ['prime', 'minister', 'stephen', 'harper', 'safe', 'left', 'parliament', 'hill', ',', 'spokesman', 'says', 'https', 'co', 'wcqpcyqmxk'], [\"'ve\", 'shooting', \"'re\", 'run', 'away', 'police', 'http', 'co'], ['ctv', 'news', 'confirms', 'canadian', 'authorities', 'provided', 'us', 'authorities', 'name', 'michael', 'zehaf', 'bibeau', 'ottawa', 'shooting'], ['ottawa', 'shooting', 'soldier', 'dies', 'injuries', ',', 'gunman', 'shot', 'dead', 'http', 'co', 'http', 'co'], ['confirmed', '1', 'shooter', 'shot', 'dead', 'parliament', 'hill', ',', '1', 'suspect', 'cbcott', 'ottnews'], ['reminder', 'observing', 'shooting', 'parliament', 'hill', 'tweet', 'police', 'locations', 'movements', 'ottcity', 'ottawa'], ['correct', 'shooter', 'radical', 'muslim', 'terrorist', 'ottnews', 'ottawashooting', 'cdnpoli'], ['thoughts', 'everyone', 'involved', 'happening', 'parliament', 'hill', 'square', 'morning', 'stay', 'safe', ',', 'ottawa'], ['rip', 'cpl', 'nathan', 'cirillo', 'argyll', 'sutherland', 'highlanders', 'canadastrong', 'http', 'co', 'http', 'co'], ['surreal', 'photo', 'ottawa', 'doors', 'tory', 'caucus', 'room', 'parliament', 'hill', ',', 'furniture', 'wow', 'cdnpoli', 'http', 'co'], ['pic', 'conservative', 'mp', 'nina', 'grewal', 'caucus', 'getting', 'twitter', 'grahamctv', 'ottawashooting', 'http', 'co'], ['episode', 'bts', \"'war\", \"hormone'\", 'mv', 'amp', 'concept', 'photo', 'shooting', '\\\\(', 'eng', 'sub', '\\\\)', '\\\\(', 'http', 'co', '\\\\)', 'hello', 'hello'], ['breaking', 'photo', 'michael', 'zehaf', 'bibeau', ',', 'gunman', 'morning', \"'s\", 'shootings', 'ottawa', 'cbcott', 'ottnews', 'http', 'co'], ['pm', 'says', 'security', 'increased', 'following', 'ottawashootings', 'http', 'co'], ['canadian', 'prime', 'minister', 'stephen', 'harper', 'due', 'make', 'statement', 'shortly', ',', 'reports', 'say', 'http', 'co', 'ottawashootings'], ['confirmed', 'reports', 'three', 'shootings', 'ottawa', 'war', 'memorial', ',', 'parliament', 'shopping', 'mall'], ['u', 'officials', 'suspected', 'shooter', 'ottawa', 'identified', 'canadian', 'born', 'michael', 'zehaf', 'bibeau', 'live', 'blog', 'http', 'co', 'q98amohu7t'], ['soldier', 'shot', 'national', 'war', 'memorial', 'ottawa', 'http', 'co'], ['working', 'ottawa', 'area', 'follow', 'building', 'ottnews'], ['remembrance', 'day', 'ottawa', 'every', 'year', 'ca', \"n't\", 'like', 'year', 'entire', 'country', 'may'], ['breaking', 'news', 'shots', 'fired', 'parliament', 'hill', 'follow', 'developing', 'story', 'http', 'co', 'http', 'co'], ['reports', 'eyewitnesses', 'confirmed', 'ferguson', 'cop', \"'s\", 'story', 'safety', 'http', 'co', 'sickening'], ['soldier', 'shot', 'national', 'war', 'memorial', 'ottawa', 'http', 'co', 'http', 'co'], ['thoughts', 'prayers', 'injured', 'soldier', 'deep', 'thanks', 'security', 'rcmp', 'hill', 'shooting', 'parliament'], ['latest', 'suspected', 'ottawa', 'gunman', 'identified', 'michael', 'joseph', 'hall', ',', 'convert', 'islam', 'also', 'known', 'michael', 'zehaf', 'bibeau'], ['amazing', 'rt', 'things', 'remember', 'ottawashooting', 'http', 'co'], ['police', 'say', 'shooter', 'roof', 'building', 'sparks', '1', 'block', 'parliament', 'hill', 'ottnews', 'cbcott', 'http', 'co'], ['breaking', 'ctvottawa', 'confirms', 'one', 'shooter', 'dead', 'police', 'working', 'one', 'shooter', '3', 'shooting', 'incidents', 'ottnews'], ['please', 'tips', ',', 'photos', 'videos', 'natdiv', 'media', 'divnat', 'rcmp', 'grc', 'gc', 'ca', 'ottawashooting', 'c', 'rcmp', 'nat', 'div', 'jt'], ['live', 'ongoing', 'ctvnews', 'abc', 'coverage', 'active', 'shooting', 'incident', 'near', 'canadian', 'parliament', 'http', 'co', '0nuzhcvr2j', 'http', 'co'], ['snipers', 'national', 'art', 'gallery', 'remain', 'barricaded', 'centre', 'block', 'parliament', 'hill', 'cdnpoli', 'http', 'co'], ['tragedy', 'soldier', 'shot', 'dies', 'wounds', 'ottawa', 'hospital', 'deep', 'sympathy', 'family', 'friends', 'shooting', 'ottawa'], ['breaking', 'multiple', 'sources', 'tell', 'cnn', 'michael', 'zehaf', 'bibeau', 'shooter', 'ottawashooting', 'http', 'co', 'http', 'co'], ['ottawa', 'police', 'confirm', 'cbcnews', 'still', '1', 'suspects', 'shootings', 'parliament', 'hill', ',', 'war', 'memorial', ',', 'nearby', 'mall'], ['shooter', 'still', 'loose', 'uniformed', 'soldier', 'shot', 'ottawa', \"'s\", 'war', 'memorial', ',', 'across', 'parliament', 'mp', \"'s\", 'offices'], ['update', 'one', 'shooter', 'possibly', 'loose', 'ottawa', 'amid', 'reports', 'shootings', 'city', 'http', 'co', 'tjyct5yqcv'], [\"'s\", 'already', 'muslim', 'tweets', 'canada', 'little', 'details', 'know', 'ottawa', 'come', 'better'], ['canadian', 'parliament', 'lockdown', 'shooting', 'live', 'http', 'co'], ['like', 'today', 'make', 'us', 'realize', 'hockey', 'game', 'ottawa', 'safe'], ['times', 'like', 'important', 'stand', 'together', 'nation', 'canadastrong', 'ottawa', 'stay', 'safe', ',', 'remain', 'vigilant', '!', 'http', 'co'], ['ottawa', 'friends', 'canadians', ',', 'coast', 'coast', 'ottawashooting', 'cdnpoli'], ['latest', 'cbcnews', 'ottawa', 'shootings', 'gunman', 'killed', 'inside', 'parliament', ',', 'soldier', 'shot', 'war', 'memorial', 'http', 'co', 'kngapktsce', 'cdnpoli'], ['confirm', ',', 'large', 'ottawa', \"n't\", 'related', 'today', \"'s\", 'shootings', 'http', 'co', 'cbcott', 'ottnews'], ['thoughts', 'prayers', 'affected', 'shootings', 'ottawa', 'stay', 'safe', '!', 'prayforottawa'], ['rt', 'rcmpgrcpolice', 'rcmp', 'advises', 'downtown', 'ottawa', 'stay', 'away', 'windows', 'roofs', 'due', 'ongoing', 'police', 'incident'], ['updated', 'soldier', 'killed', 'ottawashooting', 'identified', 'cpl', 'nathan', 'cirillo', 'hamilton', 'http', 'co', 'http', 'co'], ['shooting', 'also', 'come', 'canadian', 'isis', '\\\\(', 'suspended', '\\\\)', 'attack', 'canada', '2'], ['shots', 'fired', 'outside', 'canadian', 'parliament', 'ottawa', 'soldier', 'wounded', ',', 'local', 'media', 'report', 'http', 'co'], ['case', 'world', ',', \"'re\", 'used', 'seeing', 'parliament', 'hill', 'ottawashooting', 'http', 'co'], ['canadian', 'authorities', 'aware', 'isis', 'attacks', 'http', 'co', 'ottawashooting', 'http', 'co'], [\"'s\", 'first', 'hand', 'account', 'scene', 'ottawa', 'country', 'http', 'co', 'http', 'co'], ['source', 'says', 'one', 'suspect', 'shot', 'killed', 'inside', 'parliament', 'hill', 'security', 'sweep', 'still', 'underway', 'ottnews', 'breaking'], ['careful', 'ottawa', ',', 'via', 'http', 'co'], ['keep', 'breaking', 'news', 'handbook', 'mind', 'coverage', 'ottawa', 'continues', 'http', 'co', 'http', 'co'], ['rcmp', 'everyone', 'back', 'away', 'parliament', 'hill', 'weapons', 'word', 'shooting', 'inside', 'cdnpoli', 'http', 'co'], ['reports', 'shooting', 'rideau', 'centre', 'shooting', 'location'], ['canada', 'remain', 'true', 'north', ',', 'strong', 'free', ',', 'today', 'events', 'ca', \"n't\", ',', 'wo', \"n't\", ',', 'break', 'cdnpoli', 'ottawashooting'], ['rideau', 'centre', 'tell', 'globeandmail', 'shooting', 'inside', 'mall', 'people', 'ca', \"n't\", 'leave', 'ottawashooting'], ['nice', 'touch', 'penguins', 'host', 'flyers', 'tonight', 'canadian', 'national', 'anthem', 'performed', 'game', 'support', 'events', 'ottawa'], ['ottawashooting'], ['watch', 'canadian', 'coverage', 'ottawashooting', \"'ll\", 'realize', 'us', 'politics', 'amp', 'media'], ['ottawashooting', 'isis', '\\\\?', 'please', 'stop', 'politics', 'fear'], ['today', 'officially', 'shooting', '!', '!', \"'s\", 'looking', 'amazing', 'far', '!'], ['game', 'toronto', 'maple', 'leafs', 'ottawa', 'senators', 'scheduled', 'tonight', 'ottawa', 'postponed', ',', 'nhl', 'says'], [\"'ll\", 'never', 'today', 'amp', 'brave', 'men', 'women', 'working', 'protect', 'capital', '!', 'thank', 'ottawa', 'http', 'co'], ['please', ',', 'happening', 'ottawa', 'leave', 'politics', 'another', 'day'], ['canadian', 'soldier', 'shot', 'war', 'memorial', 'ottawa', 'died', ',', 'according', 'two', 'members', 'parliament', 'live', 'blog', 'http', 'co', 'q98amohu7t'], ['rcmp', 'advises', 'downtown', 'ottawa', 'stay', 'away', 'windows', 'roofs', 'due', 'ongoing', 'police', 'incident'], ['canada', 'journalist', 'captures', 'arms', 'fire', 'inside', 'parliament', 'building', 'ottawa', 'http', 'co'], ['seeing', 'reports', 'soldier', 'shot', 'war', 'memorial', 'still', 'alive', ',', 'canada'], ['rt', 'armedresearch', 'isis', 'media', 'account', 'posts', 'picture', 'claiming', 'michael', 'zehaf', 'bibeau', ',', 'dead', 'ottawashooting', 'suspect', 'http', 'co'], ['update', 'one', 'person', 'shot', 'war', 'memorial', 'ottawa', ',', 'police', 'say', 'http', 'co', 'chbtwxvn0q', 'http', 'co'], ['unfolded', 'attack', 'ottawa', 'http', 'co', 'ottawashooting', 'http', 'co'], ['hamont', 'reservist', 'nathan', 'cirillo', 'duty', 'ottawa', 'today', 'fatally', 'shot', 'http', 'co'], ['true', 'love', 'memory', 'cpl', 'nathan', 'cirillo', 'hamilton', ',', 'ontario', 'ottawashooting', 'canadastrong', 'http', 'co'], ['parliament', 'hill', 'attack', 'ottawa', 'residents', 'captures', 'images', 'chaos', 'lockdown', 'http', 'co', 'http', 'co'], ['ottawa', 'police', 'confirm', 'multiple', 'suspects', 'shooting', ',', 'saying', 'possibly', '3', 'shooters', 'http', 'co'], ['3', 'patients', 'injured', 'ottawashooting', 'released', 'hospital', 'ottawa', 'hospital', 'spokesperson', 'cdnpoli'], ['canadian', 'forces', 'bases', 'across', 'country', 'closed', 'public', 'wake', 'shooting', 'soldier', 'ottawa', 'war', 'memorial', 'cdnpoli'], ['remember', 'tweet', 'location', 'police', 'ottawa', 'ottawashooting'], ['downtown', 'man', 'says', \"'s\", 'ottawa', 'ottawashooting', 'http', 'co'], ['university', 'ottawa', 'lockdown', 'earlier', 'shooting', 'canada', \"'s\", 'parliament', 'buildings', ',', 'says', 'http', 'co'], ['ottawa', 'police', 'asking', 'facebook', 'twitter', 'users', 'stop', 'reporting', 'police', 'locations'], ['police', 'warn', 'people', 'ottawa', 'stay', 'away', 'windows', 'amp', 'roofs', 'parliament', 'shooting', 'follow', 'coverage', 'http', 'co'], ['ottpolice', 'rcmpgrcpolice', 'asking', 'public', 'stay', 'away', 'downtown', 'ottawa', 'area', 'ongoing', 'investigation'], ['everyone', 'ottawa', 'canada', 'best', 'wake', 'today', \"'s\", 'shootings', \"'ll\", 'hug', 'tomorrow', 'anyone', 'needs', 'one'], ['cbc', 'news', 'ottawa', 'independently', 'confirmed', 'gunman', 'shot', 'killed', 'michael', 'zehaf', 'bibeau', 'cbcott', 'ottnews'], ['white', 'house', 'obama', 'chance', 'speak', 'harper', 'ottawa', 'shooting', 'http', 'co', 'http', 'co'], ['live', 'special', 'coverage', 'parliament', 'hill', 'shooting', 'http', 'co', 'http', 'co'], ['alert', 'press', 'gallery', 'email', 'rcmp', 'advises', 'u', 'r', 'downtown', 'ottawa', 'stay', 'away', 'windows', 'amp', 'roofs', 'due', 'ongoing', 'police', 'incident'], ['breaking', 'news', 'soldier', 'shot', 'national', 'war', 'memorial', 'ottawa', 'http', 'co', 'pp6hcfwcrw'], ['canadian', 'soldier', 'killed', 'today', \"'s\", 'ottawa', 'shooting', 'cpl', 'nathan', 'cirillo', ',', 'family', 'source', 'told', 'cnn', 'live', 'blog', 'http', 'co', 'd58rrfrbwq'], ['last', '3', '27', 'pts', '26', 'pts', '4reb', '27', 'pts', '4reb', \"'s\", 'top', 'shooting', 'guard', 'http', 'co'], ['ottawa', 'police', 'press', 'http', 'co'], ['thoughts', 'nation', \"'s\", 'capital', 'right', 'praying', 'affected', 'today', \"'s\", 'tragic', 'events', 'parliament', 'hill', 'ottawa'], ['say', ',', ',', 'tomorrow', ',', 'people', 'parliament', 'hill'], ['media', 'update', 'current', 'operation', 'respect', 'shootings', 'ottawa', 'downtown', 'core', 'http', 'co'], ['stay', 'strong', 'canada', 'ottawashooting', 'http', 'co'], ['saw', 'member', 'public', 'go', 'member', 'hand', 'thank', 'service', 'nice', 'ottawashooting', 'ctvnews'], ['photo', 'cpl', 'nathan', 'cirillo', ',', 'killed', 'ottawa', 'today', 'rip', 'http', 'co'], ['ottawa', 'police', 'report', 'third', 'shooting', 'rideau', 'centre', ',', 'reports', 'injuries'], ['ctv', 'news', 'confirmed', 'name', 'canadian', 'authorities', 'provided', 'u', 'authorities', 'michael', 'zehaf', 'bibeau', 'ottawashooting'], ['witness', 'says', 'say', 'three', 'four', 'shots', 'fired', 'ottawa', \"'s\", 'war', 'memorial', ',', 'soldier', 'wounded', ',', 'several'], ['canada', \"'s\", 'parliament', 'building', 'locked', 'shooting', 'least', 'one', 'guard', 'injured', 'http', 'co', 'photo', 'ap', 'http', 'co'], ['ctvottawa', 'confirms', '3', 'separate', 'shootings', 'one', 'parliament', 'hill', ',', 'one', 'national', 'war', 'memorial', ',', 'one', 'rideau', 'centre'], ['developing', 'story', 'soldier', 'shot', 'ottawa', 'war', 'memorial', 'http', 'co'], ['everyone', 'ottawa', ',', 'please', 'stay', 'safe', 'http', 'co'], ['pmharper', 'around', 'world', 'pm', 'address', 'nation', 'ottawashooting'], ['us', 'army', 'increases', 'security', 'tomb', 'unknown', 'soldier', \"nat'l\", 'precaution', 'due', 'ottawa', 'shootings'], [\"'re\", 'parliament', 'hill', ',', 'please', \"n't\", 'tweet', 'location', 'police', 'safety', 'incident', 'ottawa'], ['breaking', 'shots', 'fired', 'three', 'separate', 'locations', 'ottawa', ',', 'say', 'ottawa', 'police', 'http', 'co'], [\"'s\", 'flags', 'lowered', 'memory', 'today', \"'s\", 'shooting', 'victim', 'ottawa', 'http', 'co'], ['rcmp', 'hold', 'news', 'conference', 'ottawa', 'shootings', '2', 'pm', 'et', ',', '11', 'watch', 'live', 'coverage', 'http', 'co', 'kngapktsce'], ['heart', 'family', 'canadian', 'soldier', 'murdered', 'morning', 'ottawa', 'rip'], ['canada', 'identifies', 'muslim', 'convert', 'michael', 'zehaf', 'bibeau', 'suspected', 'shooter', 'ottawa', 'attack', ',', 'u', 'officials', 'tell', 'cnn', 'ottawashooting'], ['photo', ',', 'reported', 'suspected', 'ottawa', 'shooter', 'michael', 'bibeau', ',', 'suspended', 'isis', 'twitter', 'account', 'http', 'co'], ['new', 'ottawashooting', ',', 'people', ',', 'go', '\\\\?', 'http', 'co', 'http', 'co'], ['updated', 'full', 'story', 'nathan', 'cirillo', ',', 'soldier', 'killed', 'ottawashooting', 'http', 'co', 'http', 'co'], ['thoughts', 'everyone', 'ottawa', 'today', 'stay', 'safe'], ['ottawa', 'shooting', 'police', 'multiple', 'suspects', 'large', ',', 'ask', 'people', 'avoid', 'area', 'http', 'co', 'zp9akplh9p', 'http', 'co'], ['thoughts', 'ottawa', 'today', 'http', 'co'], ['breaking', 'michael', 'zehaf', 'bebeau', 'high', 'risk', 'cdn', ',', 'http', 'co'], ['update', 'ottawa', 'police', 'confirm', 'gunman', 'canada', 'shooting', 'shot', 'killed', 'search', 'possible', 'gunmen', 'http', 'co'], ['reuters', 'one', 'gunman', 'shot', 'dead', 'inside', 'canadian', 'parliament', 'building', 'ottawashooting'], ['ottawa', 'mp', 'inside', 'building', 'lockdown', 'says', 'tactical', 'team', 'live', 'coverage', 'http', 'co', 'http', 'co'], ['photo', 'tweet', 'cpr', 'performed', 'soldier', 'heard', 'four', 'shots', 'ottawa', 'http', 'co'], [',', 'michael', 'zehaf', 'bibeau', 'ottawa', 'shooting', 'people'], ['heartbreaking', 'photo', 'shows', 'fallen', 'soldier', 'nathan', 'cirillo', 'moments', 'shooting', 'http', 'co', 'http', 'co'], ['thinking', 'layton', \"'s\", 'words', 'love', 'better', 'anger', 'hope', 'better', 'fear', 'optimism', 'better', 'despair', 'ottawashooting'], ['photos', 'scene', 'ottawa', ',', 'police', 'confirm', \"'several'\", 'shootings', 'http', 'co', 'http', 'co'], [',', 'horrible', 'events', 'ottawa', 'thoughts', 'prayers', '\\\\?', '\\\\?', '\\\\?'], ['obama', 'ottawa', 'shooting', \"'we\", \"'re\", 'shaken', \"it'\", 'http', 'co', 'http', 'co'], ['prayers', 'go', 'everyone', 'affected', 'shootings', 'ottawa', 'today'], ['prime', 'minister', 'stephen', 'harper', 'address', 'country', 'tonight', 'fatal', 'shooting', 'war', 'memorial', 'attack', 'parliament', 'cbcnews', 'cdnpoli'], ['ottawa', 'need', 'talk', '24', '7', 'crisis', 'line', '613', '1', '\\\\(', 'free', 'ontario', '\\\\)', 'crisis', 'ca'], ['breaking', 'ctv', 'news', 'reports', 'solider', 'wounded', 'shooting', 'national', 'war', 'memorial', 'died'], ['police', 'canada', 'confirmed', 'multiple', 'gunmen', 'involved', 'coordinated', 'shootings', 'ottawa'], ['uniformed', 'canadian', 'soldier', 'shot', 'war', 'memorial', 'ottawa'], ['events', 'ottawa', 'game', 'yet', 'postponed', 'schools', 'closed'], ['important', 'ottawa', 'police', 'urging', 'people', 'tweet', 'location', 'police', 'activity', 'shooting', 'scene', 'remains', 'active'], ['nhl', 'reportedly', 'monitoring', 'situation', 'ottawa', 'shootings', ',', 'could', 'postpone', 'sens', 'leafs', 'game', 'http', 'co', 'http', 'co'], [\"'re\", 'thinking', 'tweet', 'situation', 'ottawa', 'party', 'away', 'twitter', 'cdnpoli'], ['thoughts', 'ottawa', 'today', 'happening', 'terrible', 'staysafeottawa'], ['update', 'shots', 'fired', 'inside', 'parliament', 'hill', 'gun', 'fire', 'hill', 'security', 'cbcott', 'ottnews'], ['recap', 'parliament', 'hill', 'lockdown', 'uniformed', 'canadian', 'soldier', 'shot', 'war', 'memorial', 'suspect', 'loose', 'witnesses', 'say', 'rifle'], ['update', 'uottawa', 'officially', 'cancelled', 'whole', 'day', 'lockdown', 'still', 'stay', ',', 'stay', 'safe', 'ottawa', 'ottcity'], ['stunning', 'photo', 'inside', 'caucus', 'room', 'shooting', 'http', 'co', 'http', 'co', 'operh3gimu'], ['canadian', 'media', 'gunman', 'shot', 'soldier', 'war', 'memorial', 'http', 'co', 'znhxk6wboy'], ['parliament', 'shooting', 'active', 'situation', 'still', 'moment', 'searching', 'building', ',', 'suspect', 'suspects'], ['one', 'shooting', 'victim', 'succumbed', 'injuries', 'member', 'canadian', 'forces', 'thoughts', 'prayers'], ['one', 'hurt', 'shooting', 'ottawa', \"'s\", 'rideau', 'centre', 'scene', '1', '3', 'attacks', ',', 'canadian', 'police', 'say', 'http', 'co', 'qsangnqbew', 'ottawashooting'], ['police', 'confirm', 'multiple', 'suspects', 'involved', 'shooting', 'incident', 'ottawa', 'war', 'memorial', ',', 'http', 'co', 'af9hyt9bgx'], ['shooting', 'downtown', 'ottawa', ',', 'police', 'rideau', 'centre', 'stay', 'safe', 'uottawa'], ['official', '3', 'patients', 'injured', 'ottawa', 'shootings', 'released', 'hospital', 'http', 'co', 'af9hyt9bgx'], ['tense', 'situation', 'ottawa', 'morning', 'multiple', 'gun', 'shots', 'fired', 'outside', 'caucus', 'room', 'safe', 'lockdown'], ['thoughts', 'prayers', 'everyone', 'ottawa', '\\\\(', 'office', 'capital', '\\\\)', 'right', 'parliament', 'hill', 'shots', 'fired', 'prayforottawa'], ['prayers', 'ottawa'], ['breaking', 'news', 'u', 'advised', 'ottawa', 'shooter', 'believed', 'canadian', 'convert', 'islam', ',', 'confirms'], ['thoughts', 'amp', 'prayers', 'go', 'everyone', 'affected', 'events', 'nation', \"'s\", 'capital', 'ottawastrong', 'ottawa', 'http', 'co'], ['breaking', 'spokesperson', 'ottawa', 'hospital', 'confirms', 'received', 'three', 'patients', ',', 'two', 'stable', 'condition'], ['ottawa', 'shooting', 'used', 'away', 'http', 'co', 'http', 'co'], ['cbs', 'reports', 'ottawa', 'shooting', 'suspect', 'born', 'michael', 'joseph', 'hall', \"'s\", 'convert', 'islam', 'http', 'co', '58ik7fmca1'], ['leafs', 'could', 'hear', 'shots', ',', 'told', 'ottawashooting'], ['running', 'parliament', 'hill', 'http', 'co'], ['ctvnews', 'reporting', 'second', 'shooting', 'behind', 'peace', 'tower', 'parliament', 'hill', 'ottnews', 'shooter', 'loose'], ['canadian', 'soldier', 'standing', 'guard', 'gone', 'thoughts', 'w', 'everyone', 'affected', 'shootings', 'ottawa', 'canadastrong'], ['kevin', 'vickers', 'hailed', 'hero', 'took', 'attacker', 'http', 'co', 'ottawashooting', 'http', 'co'], ['canadian', 'officials', 'calling', \"today's\", 'parliament', 'shooting', 'terrorist', 'attack', 'http', 'co', 'http', 'co'], ['anyone', 'downtown', 'ottawa', 'stay', 'inside', ',', 'away', 'roofs', ',', 'windows', 'doors', 'please', 'stay', 'safe', 'cbcott', 'ottnews'], ['terrible', 'news', 'ottawa', 'thoughts'], ['shooting', 'public', 'places', 'claiming', ',', 'people', \"'s\", 'land', ',', 'rt', \"'s\", 'white', '\\\\?'], ['heart', 'goes', 'ottawa', 'violence', \"'s\", 'way', 'lives'], ['ottawa', '!', 'sending', 'stay', 'away', 'rooftops', 'windows', 'downtown', 'shooter', 'still', 'parliament', 'hill', 'http', 'co'], ['wilson', 'assistance', 'http', 'co', 'http', 'co', 'ferguson'], ['coverage', 'ottawa', 'shooting', ',', 'one', 'http', 'co', 'http', 'co'], ['thoughts', 'amp', 'prayers', 'go', 'soldier', 'killed', 'ottawa', 'shooting', 'amp', 'everyone', 'affected', 'ottawa', 'prayforottawa', 'ottawashooting'], ['breaking', 'soldier', 'shot', 'dead', 'ottawa', 'attack', 'identified', 'hamilton', ',', 'ontario', 'based', 'reservist', 'cpl', 'nathan', 'cirillo', 'ottawashooting'], ['pmharper', 'safe', 'left', 'parliament', 'hill'], ['matter', 'ottawa', ',', \"'s\", 'chance', 'mom', 'email', 'call', 'family'], ['please', 'restraint', 'looking', 'someone', 'ottawashooting'], ['canadian', 'cabinet', 'minister', 'jason', 'kenney', 'says', 'canada', 'following', 'ottawashooting'], ['breaking', 'update', 'soldier', 'shot', 'national', 'war', 'memorial', 'ottawa', ',', 'canada', ',', 'police', 'tell', 'abc', 'news', 'http', 'co'], ['kevinvickers', 'hailed', 'hero', 'took', 'attacker', 'parliament', 'hill', 'http', 'co', 'ottawashooting', 'http', 'co'], ['police', 'believe', 'three', 'gunmen', 'involved', 'shootings', 'ottawa', 'morning', 'looking', 'two', 'shooters', 'http', 'co'], ['chairs', 'mps', 'protect', 'ottawashootings', 'http', 'co', 'http', 'co'], ['received', 'email', 'message', 'ottawapolice', 'chief', 'ottawashooting', 'http', 'co'], ['saddened', 'death', 'young', 'man', 'serving', 'us', 'great', 'nation', 'god', 'bless', 'ottawa', 'ottawashooting'], ['thinking', 'everyone', 'ottawa', 'morning', 'thoughts', 'prayers', 'solider', 'family', 'canada'], ['officer', 'wilson', 'face', 'criminal', 'shooting', 'people'], ['breaking', 'reported', 'shooting', 'canada', 'war', 'memorial', ',', 'near', 'country', 'parliament', ',', 'police', 'say', 'http', 'co'], ['update', 'ottawa', 'cdn', 'soldier', 'dies', 'shooting', 'parliamentary', 'guard', 'wounded', 'parliament', 'hill', 'still', 'lockdown', 'http', 'co', 'chbtwxvn0q'], ['ottawa', 'shootings', 'suspect', 'identified', 'michael', 'zehaf', 'bibeau', ',', '32', 'shooters', 'http', 'co'], ['conservative', 'informed', 'soldier', 'shot', 'war', 'memorial', 'ottawa', 'morning', 'died', 'sad', 'day'], ['rip', 'nathan', 'cirillo', 'soldier', 'ottawa', 'canada', 'love', 'http', 'co'], ['everyone', 'take', 'moment', 'honour', 'cpl', 'nathan', 'cirillo', 'solider', 'killed', 'ottawa', 'today', 'hero', 'http', 'co'], ['words', 'u', 'cdn', 'coverage', 'ottawashooting', 'kudos', 'cbcnews', 'day', 'http', 'co'], ['incredible', 'footage', 'josh', 'wingrove', 'shots', 'fired', 'parliament', 'building', 'ottawa', 'https', 'co'], ['canadian', 'anthem', 'performed', 'flyers', 'penguins', 'tonight', 'support', 'ottawa', 'http', 'co', 'http', 'co'], ['breaking', 'obama', 'canada', 'shooting', \"'we\", \"'re\", 'shaken', \"it'\", 'information'], ['police', ',', 'bystanders', 'soldiers', 'help', 'memorial', 'shooting', 'ottawa', 'http', 'co', 'ao700dweow', 'http', 'co'], ['gun', 'fire', 'exchange', 'parliament', 'hill', 'building', 'ottawa', 'parliament', 'shooting', 'canada', 'https', 'co', 'via'], ['saw', 'one', 'soldiers', 'ground', 'witnesses', 'chaos', 'ottawashooting', 'http', 'co', 'http', 'co'], ['breaking', 'canada', 'soldier', 'shot', 'near', 'parliament', 'died', ',', 'official', 'says'], ['live', 'cbc', 'news', 'coverage', 'ottawashooting', 'http', 'co', 'mobile', 'http', 'co', 'cbcnn', 'http', 'co'], ['university', 'ottawa', ',', 'five', 'minute', 'hill', ',', 'lockdown'], [\"i'm\", 'like', 'shooting', ',', \"'ve\", 'come', 'far', 'ca', \"n't\", 'go', 'back', 'used', '\\\\(', '\\\\)'], ['update', 'reports', '1', 'shooter', 'shot', 'one', 'shot', 'killed', 'earlier', 'parliament', 'hill', 'ottnews', 'ottawashooting'], ['police', 'say', 'shots', 'fired', '3', 'places', 'national', 'war', 'memorial', ',', 'parliament', 'hill', 'near', 'rideau', 'centre', 'mall', 'http', 'co'], ['canada', 'violence', 'ottawa', ',', 'brave', 'men', 'women', 'keeping', 'capital', 'safe'], ['ottawa', 'hospital', 'says', 'received', '3', 'patients', 'shootings', '2', 'stable', ',', 'wo', \"n't\", 'give', 'update', 'soldier', 'statement', 'http', 'co', 'af9hyt9bgx'], ['police', 'officer', 'death', 'video', 'shows', 'fatally', 'shooting', 'http', 'co', 'http', 'co'], ['everyone', 'stand', 'together', 'day', 'like', 'stay', 'strong', 'ottawa'], ['full', 'lock', 'notice', 'ottawa', 'police'], ['ottawa', 'police', 'rcmp', 'press', 'conference', 'time', 'shortly', 'cbcott', 'ottnews'], ['gunman', 'loose', 'downtown', 'ottawa', 'canadian', 'citizens', 'government', 'target', 'environment', 'ottawashooting'], ['makes', 'sad', \"'s\", 'going', 'ottawa', 'right', 'thoughts', 'everyone', 'involved', 'everybody', 'stay', 'safe'], ['rcmp', 'advises', 'people', 'stay', 'away', 'parliament', 'hill', 'due', 'ongoing', 'police', 'incident'], ['nhl', 'postpone', 'tonight', \"'s\", 'sens', 'leafs', 'game', 'ottawa', 'right', 'decision', 'decision'], ['developing', 'police', 'shooting', 'near', 'canadian', 'parliament', 'soldier', 'wounded', 'active', 'situation', 'http', 'co', '9maaywtu03', 'http', 'co'], ['cnn', 'ottawashooting', 'cdnpoli'], ['update', 'reuters', 'reports', 'witness', 'says', 'police', 'suspect', 'parliament', 'ottawa', 'amp', 'heard', '30', 'shots', 'fired', 'inside'], ['breaking', 'least', 'shots', 'fired', 'canadian', 'parliament', 'ottawa', ',', 'according', 'witnesses'], ['thoughts', 'prayers', 'canadian', 'soldier', 'shot', 'guarding', 'national', 'war', 'memorial', 'ottawa', 'ableg', 'cdnpoli'], ['tip', 'kids', 'media', 'coverage', 'war', 'events', 'http', 'co', 'ottawashooting', 'http', 'co'], [',', 'named', 'best', '2014', '15', 'gm', 'survey', 'story', 'http', 'co', 'http', 'co'], ['breaking', 'news', 'shots', 'fired', 'parliament', 'hill', 'follow', 'developing', 'story', 'http', 'co', 'http', 'co'], [\"'s\", 'dramatic', 'video', 'via', 'josh', 'wingrove', 'shots', 'fired', 'inside', 'parliament', 'building', 'https', 'co', 'ottawa', 'shooting'], ['ottawashooting', 'suspect', 'killed', 'gun', \"'re\", ','], ['tonight', 'ottawa', 'latest', 'shootings', 'live', '11pm'], ['ottawa', 'shooting', 'soldier', 'shot', 'memorial', 'died', 'globe', ',', 'ctv', 'harper', 'speak', 'later', 'today', 'http', 'co', 'dknjoduorr'], ['rt', 'bts', 'bighit', 'episode', 'bts', \"'war\", \"hormone'\", 'mv', 'amp', 'concept', 'photo', 'shooting', '\\\\(', 'eng', 'sub', '\\\\)', '\\\\(', 'http', 'co', '\\\\)', 'hello', 'hello'], ['ottawa', 'closed', '!', 'active', 'search', 'still', 'underway', '\\\\(', '11', '\\\\)', 'parliamenthill', 'http', 'co'], ['ottawa', 'shooting', 'soldier', 'shot', 'memorial', 'gunfire', 'parliament', 'pm', 'safe', ',', 'speak', 'later', 'today', 'http', 'co', 'dknjoduorr'], ['breaking', 'pmo', 'says', 'pm', 'harper', 'safe', 'left', 'parliament', 'hill'], ['incidents', 'occurred', 'national', 'war', 'memorial', ',', 'near', 'rideau', 'centre', 'parliament', 'hill', 'morning', 'ottnews'], ['photos', 'parliament', 'shooting', 'scene', 'http', 'co', 'http', 'co'], ['active', 'shooting', 'canada', ',', 'call', 'america', ',', 'wednesday'], [\"'s\", 'unfolding', 'canada', \"'s\", 'capital', 'thinking', ',', 'ottawa'], ['rip', 'canadian', 'soldier', 'killed', 'today', 'ottawa', 'thoughts', 'prayers', 'family', 'amp', 'friends', 'canada', 'http', 'co'], ['rest', 'peace', 'corporal', 'nathan', 'cirillo', 'respect', 'ottawa', 'canada', 'http', 'co'], ['thoughts', 'emergency', 'responders', 'bystanders', 'ottawa', 'please', 'stay', 'safe'], ['reports', 'police', 'suspect', 'two', 'three', 'shooters', 'parliament', 'hill', 'sergeant', 'arms', 'kevin', 'vickers', 'shot', 'one', 'assailant'], ['ottawapolice', 'email', 'muslim', 'call', 'feel', \"'re\", 'risk', 'ottawashooting'], ['stay', 'away', 'downtown', 'area', 'operation', 'continues', 'ottnews', 'ottawa', 'ottcity'], ['spokesperson', 'pmo', 'says', 'prime', 'minister', 'stephen', 'harper', 'safe', ',', 'left', 'parliament', 'hill', 'cdnpoli'], ['extended', 'dramatic', 'video', 'gunfire', 'inside', 'hallways', 'parliament', 'hill', '\\\\(', 'globe', '\\\\)', 'http', 'co', 'http', 'co'], ['ottawa', 'footage', 'video', 'captures', 'moment', 'shots', 'fired', 'canadian', 'parliament', 'http', 'co', 'zp9akplh9p', 'http', 'co'], ['thoughts', 'prayers', 'go', 'ottawa'], ['royal', 'canadian', 'police', 'people', 'downtown', 'ottawa', 'stay', 'away', 'windows', 'rooftops', 'http', 'co'], ['praying', 'everyone', 'ottawa', 'right'], ['breaking', 'news', 'suspected', 'ottawa', 'gunman', 'identified', 'michael', 'joseph', 'hall', ',', 'isis', 'ottawashooting'], ['rcmp', 'asking', 'anyone', 'pictures', 'videos', 'today', \"'s\", 'events', 'ottawa', 'following', 'link', 'http', 'co'], ['live', 'terror', 'shooting', 'going', 'canada', 'pray', 'neighbors', 'north'], ['canadian', 'police', 'warn', 'people', 'ottawa', 'stay', 'away', 'windows', 'roofs', 'due', 'ongoing', 'shooter', 'incident', 'ctvnews', 'http', 'co', 'af9hyt9bgx'], ['cpl', 'nathan', 'cirillo', 'identified', 'victim', 'ottawashooting', 'national', 'war', 'memorial', 'http', 'co', 'http', 'co'], ['breaking', 'nhl', 'postpones', 'tonight', \"'s\", 'senators', 'leafs', 'game', 'ottawa'], ['canadian', 'military', 'bases', 'closed', 'soldier', 'shot', \"nat'l\", 'war', 'memorial', ',', 'ongoing', 'situation', 'near', 'parliament', 'williamsjon'], ['rcmp', 'saying', 'stay', 'away', 'windows', 'downtown', 'ottawa'], ['breaking', 'us', 'embassy', 'ottawa', 'lockdown', 'due', 'shooting', 'parliament', 'hill'], ['breaking', 'news', 'ottawa', 'police', 'confirm', 'least', '3', 'separate', 'shooting', 'sites', 'multiple', 'suspects', 'http', 'co', 'lxfiyavimz'], ['areas', 'ottawashootings', 'reported', 'http', 'co', 'http', 'co'], ['war', 'memorial', 'mouth', 'mouth', 'image', \"'s\", 'ottnews', 'ottawa', 'http', 'co'], ['watch', 'shots', 'fired', 'inside', 'ottawa', \"'s\", 'parliament', 'building', 'canadian', 'police', 'say', 'shooting', 'suspect', 'still', 'large', 'https', 'co'], ['footage', 'globe', 'reporter', 'captures', 'exchange', 'gun', 'fire', 'parliament', 'hill', 'building', 'https', 'co', 'fou4pbncdq'], ['cpl', 'nathan', 'cirillo', ',', '24', ',', 'identified', 'soldier', 'shot', ',', 'killed', 'war', 'memorial', 'today', 'cbcott', 'ottnews', 'http', 'co'], ['canadians', 'amp', 'today', \"'s\", 'ottawashooting', 'tragedy', ',', 'let', 'us', 'resolved', 'stand', 'strong', ',', 'together', 'cdnpoli'], ['served', 'michael', 'gets', 'life', 'shooting', 'http', 'co', '\\\\?', 'http', 'co'], ['condolences', 'family', 'fallen', 'officer', 'ottawa', 'hero', 'bravely', 'stood', 'defence', 'country'], ['breaking', 'confirms', 'soldier', 'shot', 'morning', 'died', ',', 'parliamentary', 'guard', 'also', 'wounded', 'ottawashooting'], ['breaking', 'reports', 'shots', 'fired', 'parliament', 'hill'], ['terror', 'canada', 'amp', 'israel', 'today', 'soldier', 'murdered', 'ottawa', ',', 'murdered', 'jerusalem', 'terror', 'world'], ['ottawa', 'prayers', '!', 'canada'], ['breaking', 'ottawa', 'police', 'confirm', 'least', 'three', 'separate', 'shootings'], ['canadian', 'soldier', 'shot', 'ottawa', \"'s\", 'war', 'memorial', 'reservist', 'serving', 'argyll', 'sutherland', 'highlanders', 'hamilton', ',', 'ont'], ['watch', 'video', 'ottawa', 'shooting', 'witness', 'http', 'co'], ['watch', 'live', 'cbc', 'nn', 'coverage', 'shooting', 'soldier', 'ottawa', 'cenotaph', 'shooter', 'still', 'loose', 'around', 'parliament', 'hill', 'http', 'co'], ['thoughts', 'everyone', 'ottawa', 'time'], ['breaking', 'shooter', 'dead', ',', 'according', 'sources', 'parliament', 'hill'], ['breaking', 'canadian', 'embassy', ',', 'c', 'locked', 'precaution', 'ottawashooting', 'http', 'co', 'http', 'co'], ['see', 'parliament', 'hill', ',', \"'re\", ',', 'cop', ',', 'behind', 'chateau', 'laurier'], ['sad', 'soldier', 'cenotaph', 'remembrance', 'day', 'feel', 'different', 'ottawashooting'], ['thoughts', 'amp', 'prayers', 'families', 'victims', 'today', 'shootings', 'ottawa', ',', 'members', 'parliament', 'amp', 'staff'], ['watch', 'gunfire', 'inside', 'halls', 'canada', \"'s\", 'parliament', 'hill', 'http', 'co', 'http', 'co'], ['ottawa', 'police', 'confirm', '1', 'shooting', 'suspect', 'died', 'one', 'custody', 'ottawapolice', 'http', 'co'], ['four', 'year', 'old', 'cpl', 'nathan', 'cirillo', 'shot', 'dead', 'today', 'war', 'memorial', 'ottawa', 'cdnpoli', 'ottawashooting', 'http', 'co'], ['good', 'news', 'canadian', 'zehaf', 'bebeau', 'isis', 'bad', 'news', 'ottawa', 'today'], ['images', 'scene', 'shooting', 'ottawa', 'http', 'co', 'http', 'co'], ['stay', 'safe', 'ottawa', 'world', 'getting', 'worse', 'minute', 'like', 'needs', 'figure', 'prayforottawa'], ['cpl', 'nathan', 'cirillo', 'identified', 'soldier', 'shot', ',', 'killed', 'ottawa', 'today', ',', 'says', 'globeandmail', 'http', 'co'], ['rcmp', 'asking', 'assistance', 'photos', 'videos', 'ottawa', 'shooting', 'please', 'send', 'natdiv', 'media', 'divnat', 'rcmp', 'grc', 'gc', 'ca'], ['ottawa', 'police', ',', 'map', 'downtown', 'ottawa', 'safety', 'perimeter', 'cbcott', 'ottnews', 'http', 'co'], ['parliament', 'shootings', 'later', 'mps', 'dozens', 'reporters', 'hall', 'shooting', 'took', 'place'], ['rcmp', 'public', 'post', 'photos', ',', 'videos', 'ottawashooting', 'http', 'co', 'http', 'co'], ['raw', 'footage', 'live', 'coverage', 'shooting', 'parliament', 'http', 'co', 'http', 'co'], ['recap', 'gunman', 'shot', 'dead', 'inside', 'parliament', 'buildings', 'police', 'believe', 'may', 'others', 'loose', 'cdn', 'soldier', 'shot', 'earlier', 'war', 'memorial'], ['news', 'man', 'reporting', 'shots', 'fired', 'chateau', 'laurier', 'hotel', 'ottawa'], ['ottawashooting', 'rcmp', 'nat', 'div', 'press', 'conference', '2', 'pm', 'today'], ['please', 'tweet', 'photos', 'locations', 'police', 'ottawa', 'parliament', 'buildings', '!', 'ottawapolice'], ['nhl', 'tonight', \"'s\", 'game', 'leafs', 'sens', 'ottawa', 'postponed'], ['episode', \"'war\", \"hormone'\", 'mv', 'amp', 'photo', 'shooting', 'http', 'co'], ['ok', 'war', 'ottawa', 'minutes', 'ago', ',', 'seconds', 'later', 'shooting', 'http', 'co'], ['canada', \"'s\", 'parliament', 'hill', 'lockdown', 'police', 'shooting', 'suspect', 'least', '1', 'injured', 'cbcnews', ',', 'torontostar', 'http', 'co', 'af9hyt9bgx'], ['joerayment', 'dramatic', 'photos', 'wcuddington', 'scene', 'ottawa', 'http', 'co', 'http', 'co', 'prayforottawa', '\\\\('], [',', 'shootings', 'remember', ',', 'least', 'half', 'hear', 'ottawa', 'shootings'], ['parliament', 'least', 'one', 'gunman', 'shot', 'soldier', 'standing', 'guard', 'canada', \"'s\", 'war', 'memorial', 'cbcott', 'ottnews'], ['chairs', 'door', 'surreal', 'rt', 'grahamctv', 'shot', 'inside', 'caucus', 'room', 'shooting', 'http', 'co'], ['list', 'reporters', ',', 'mps', ',', 'police', 'actively', 'tweeting', 'shooting', 'ottawa', 'https', 'co'], ['ctv', 'reports', 'soldier', 'shot', 'war', 'memorial', 'still', 'alive'], ['rip', 'cpl', 'nathan', 'cirillo', 'canada', 'always', 'remember', 'ottawashooting', 'http', 'co'], ['latest', 'photos', 'ottawa', \"'s\", 'parliament', 'hill', 'http', 'co', 'http', 'co'], ['rcmp', 'advises', 'downtown', 'ottawa', 'stay', 'away', 'windows', 'roofs', 'due', 'ongoing', 'police', 'incident'], ['canadian', 'authorities', 'name', 'suspect', 'ottawa', 'attacks', 'u', 'ask', 'fbi', 'assistance', 'u', 'law', 'official'], ['breaking', 'cnn', 'confirms', 'name', 'soldier', 'killed', 'ottawashooting', 'nathan', 'cirillo', 'http', 'co'], ['photo', 'parliament', 'hill', 'caucus', 'room', 'barricaded', 'furniture', 'due', 'active', 'shooter', 'situation', 'http', 'co', 'http', 'co'], ['events', 'like', 'today', \"'s\", 'ottawa', ',', 'us', 'tips', 'help', 'support', 'children', 'http', 'co'], ['breaking', 'ottawapolice', 'confirm', 'one', 'death', 'parliament', 'hill', '3', 'separate', 'shootings', 'war', 'memorial', ',', 'hill', 'near', 'rideau', 'centre', 'ctvottawa'], ['cpl', 'nathan', 'cirillo', ',', 'soldier', 'shot', 'dead', 'today', 'ottawa', 'rip', 'thank', 'service', 'ottawashooting', 'http', 'co'], ['evacuated', 'roof', 'parliament', 'hill', 'centre', 'block', 'dmatthewmillar', 'https', 'co', 'http', 'co', 'ottawashooting'], ['witness', 'tells', 'cbcnews', 'suspected', 'shooter', 'uniformed', 'soldier', 'ottawa', \"'s\", 'war', 'memorial', 'rifle'], ['breaking', 'soldier', 'shot', 'canadian', 'capital', 'ottawa', 'gunman', 'believed', 'entered', 'parliament', 'http', 'co'], ['updated', 'list', 'reporters', ',', 'witnesses', ',', 'citizens', ',', 'police', 'military', 'tweeting', 'downtown', 'ottawa', 'https', 'co'], ['three', 'people', 'ottawa', 'hospital', 'today', 'shooting', 'released', 'stable', 'condition', 'live', 'blog', 'http', 'co', 'd58rrfrbwq'], ['police', 'say', 'car', 'suspect', 'ottawa', 'shooting', 'uniformed', 'soldier', 'shot', 'war', 'memorial', ',', 'condition', 'unknown'], ['new', 'evidence', ',', 'autopsy', 'report', ',', 'supports', 'wilson', \"'s\", 'account', 'ferguson', 'shooting', 'http', 'co'], ['calls', ',', 'r', 'terrorist', 'change', 'terrorism', 'terrorism', 'ottawa'], ['dramatic', 'video', 'gunfire', 'inside', 'parliament', 'hill', 'http', 'co', 'http', 'co'], ['ottawa', 'police', 'asking', 'twitter', 'users', 'stop', 'reporting', 'police', 'locations', 'movements'], ['breaking', 'police', 'source', 'told', 'globe', 'second', 'shooter', 'shot', 'ottawashooting'], ['police', 'hold', 'news', 'conference', 'ottawashooting', 'follow', 'coverage', 'http', 'co'], ['update', 'police', 'several', 'shooting', 'incidents', 'downtown', 'ottawa', 'http', 'co', '9maaywtu03', 'http', 'co'], ['new', 'u', 'embassy', 'ottawa', 'lockdown', 'due', 'shooting', 'near', 'canadian', 'parliament'], ['thoughts', 'ottawa', 'today', 'important', 'always', 'look', 'http', 'co'], ['image', 'rt', 'acarvin', 'parliament', 'caucus', 'room', 'barricaded', 'chairs', ',', 'via', 'grahamctv', 'http', 'co', 'ottawa'], ['scene', 'ottawa', 'http', 'co', 'ao700dweow', 'http', 'co'], ['stunning', 'photos', 'ottawa', 'http', 'co', 'http', 'co'], ['friend', \"'s\", 'mom', 'right', 'met', ',', 'shooting', 'go', 'http', 'co'], ['rcmp', 'asking', 'assistance', 'photos', 'videos', 'ottawa', 'shooting', 'please', 'send', 'natdiv', 'media', 'divnat', 'rcmp', 'grc', 'gc', 'ca'], ['canada', 'investigating', 'michael', 'zehaf', 'bibeau', 'possible', 'suspect', 'ottawa', 'shooting', 'source', 'http', 'co', 'http', 'co'], ['shots', 'fired', 'parliament', 'hill', 'man', 'gun', \"i'm\", 'locked', 'security', 'office', 'parliament', 'hill'], [',', 'people', 'talk', ',', 'scared', ',', 'anxious', ',', 'need', 'someone', '613', '238', '3311', 'ottawa'], ['watch', 'live', 'cbcnn', 'coverage', 'shooting', 'ottawa', 'http', 'co', 'mobile', 'http', 'co', 'http', 'co'], ['rt', 'told', 'cbc', 'ottawa', 'situation', 'evolving', ',', 'told', 'remain', 'current', 'location', 'notice', 'ottawashooting'], ['thoughts', 'prayers', 'everybody', 'ottawa', 'stay', 'safe', 'ottawastrong', 'canadastrong'], ['breaking', 'cbc', 'hamilton', 'soldier', 'shot', 'ottawa', 'wednesday', 'morning', 'reservist', 'serving', 'hamilton'], ['shooting', 'parliament', 'hill', ',', 'use', 'words', ',', 'weapons'], ['israeli', 'pm', 'calls', 'pm', 'harper', 'condolences', 'ottawa', 'shooting', ',', 'express', 'full', 'support', 'canada', \"'s\", 'terror'], ['witnesses', 'say', 'many', 'shots', 'fired', 'parliament', 'hill', 'gunfire', 'reported', 'parliament', 'building', 'ottawa', 'http', 'co'], ['statement', 'shooting', 'incident', 'ottawa', 'ca', ',', '2014', 'http', 'co'], ['watch', 'inside', 'hallways', 'parliament', 'hill', 'http', 'co', 'ottawa'], ['breaking', 'ctv', 'ottawa', 'confirms', '3', 'separate', 'shootings', 'one', 'parliament', 'hill', ',', 'one', 'national', 'war', 'memorial', ',', 'one', 'near', 'rideau', 'centre'], ['thoughts', 'prayers', 'city', 'ottawa', 'affected', 'today', \"'s\", 'events'], ['thoughts', 'good', 'friends', 'amp', 'neighbors', 'north', 'ottawashooting'], ['canadian', 'soldier', 'shot', 'country', \"'s\", 'national', 'war', 'memorial', 'near', 'parliament', 'http', 'co', 'http', 'co'], ['developing', 'news', 'soldier', 'shot', 'war', 'memorial', 'watch', 'cbc', 'nn', 'coverage', 'follow', 'http', 'co', 'http', 'co', 'qcwgn8oghb'], ['breaking', 'ottawa', 'police', 'confirming', '3rd', 'shooting', 'one', 'rideau', 'centre', 'mall', 'word', 'injuries', 'one', 'ottawashooting'], ['police', 'news', 'conference', 'provide', 'latest', 'today', \"'s\", 'incidents', 'ottawa', 'watch', 'live', 'http', 'co'], ['shooting', 'investigation', 'going', '3', 'much', 'downtown', 'ottawa', 'still', 'lockdown', 'http', 'co', 'alz6qu6mca', 'cbcott', 'ottnews'], ['police', 'ottawa', 'call', 'situation', ',', \"'\", ',', \"'\", \"'\", 'confirm', 'soldier', ',', 'gunman', 'cdnpoli'], ['authorities', 'refuse', 'say', 'whether', 'parliament', 'hill', 'gunman', 'canadian', ',', 'whether', 'one', 'people', 'terror', 'list'], ['1', 'gunman', 'shot', 'dead', 'parliament', 'hill', 'attack', ',', 'soldier', 'shot', 'ottawa', 'http', 'co', 'pp6hcfwcrw', 'report', 'soldier', 'dead', 'ottawashooting'], ['canadian', 'police', 'say', 'ottawashooting', 'caught', 'us', 'http', 'co', 'http', 'co'], ['still', 'security', 'ottawa', 'police', 'search', 'buildings', 'follow', 'coverage', 'http', 'co', 'http', 'co'], [\"'s\", 'scared', '613', '238', '3311', 'ottawa'], ['comes', 'watch', ',', 'shooting', 'http', 'co'], ['breaking', 'news', 'new', 'times', 'reporting', 'canadian', 'soldier', 'shot', 'died', 'injuries', 'heartbreaking', 'cdnpoli', 'ableg'], ['breaking', 'update', 'canadian', 'soldier', 'injured', 'parliament', 'hill', 'shooting', 'dies', 'http', 'co', 'zp9akplh9p', 'ottawa'], [\"n't\", 'ottawa'], ['important', 'name', 'right', 'cpl', 'nathan', 'cirillo', 'ottawa'], ['breaking', 'military', 'sources', ',', 'friends', 'nathan', 'frank', 'cirillo', 'soldier', 'killed', 'ottawa', 'http', 'co', 'http', 'co'], ['important', 'canadians', 'vigilant', 'report', 'suspicious', 'local', 'police', 'ottawashooting'], ['ottawa', 'hospital', 'says', 'three', 'patients', 'involved', 'today', \"'s\", 'incident', 'http', 'co', 'ottawashootings'], ['half', 'dozen', 'witnesses', 'provided', 'supports', 'officer', \"'s\", 'account', 'michael', 'shooting', 'http', 'co'], ['several', 'mps', 'tweeting', 'gunman', 'shot', 'dead', 'centre', 'block', 'mps', 'reportedly', 'safe', 'cdnpoli', 'ottawa'], ['updated', 'obama', 'ottawa', 'shootings', ',', \"'\", 'offers', 'help', 'response', 'http', 'co'], ['canada', \"'s\", 'minister', 'employment', 'jason', 'kenney', 'says', 'soldier', 'shot', 'ottawa', 'died', 'torontostar', 'http', 'co'], ['rip', 'corporal', 'nathan', 'cirillo', 'thank', 'serving', 'country', 'gone', 'never', 'forgotten', 'ottawashooting', 'http', 'co'], ['ottawa', 'police', 'service', 'one', 'hurt', 'shooting', 'near', 'ottawa', \"'s\", 'rideau', 'centre', 'live', 'blog', 'http', 'co', 'q98amohu7t'], ['ottawa', 'police', 'confirm', 'fatal', 'shooting', 'canadian', 'soldier', 'war', 'memorial', ',', 'shooting', 'death', 'gunman', 'parliament', 'one', 'custody'], ['leafs', 'officers', 'ottawa', ',', 'team', 'remains', 'lock', 'http', 'co', 'http', 'co'], ['rcmp', 'asking', 'assistance', 'photos', 'videos', 'ottawa', 'shooting', 'send', 'natdiv', 'media', 'divnat', 'rcmp', 'grc', 'gc', 'ca'], ['breaking', 'nhl', 'game', 'toronto', 'maple', 'leafs', 'ottawa', 'senators', ',', 'scheduled', 'tonight', 'ottawa', ',', 'postponed'], ['doors', 'barricaded', 'rt', 'grahamctv', 'shot', 'inside', 'caucus', 'room', 'shooting', 'ctvottawa', 'ottawashooting', 'http', 'co', 'operh3gimu'], ['breaking', 'news', 'canadian', 'officials', 'calling', \"today's\", 'parliament', 'shooting', 'terrorist', 'attack', 'http', 'co', 'lxfiyavimz'], ['breaking', 'four', 'blackwater', 'guards', 'manslaughter', '2007', 'baghdad', 'shooting', 'http', 'co'], ['thoughts', 'prayers', 'everyone', 'hometown', 'today', 'ottawa'], ['know', 'far', 'canadian', 'parliament', 'shooting', 'ottawa', 'http', 'co', 'http', 'co'], ['response', 'ottawa', 'incidents', ',', 'norad', 'increased', 'number', 'planes', 'higher', 'alert', 'status', 'ready', 'respond', 'http', 'co', 'q98amohu7t'], ['rest', 'peace', 'soldier', 'killed', 'ottawa', 'today', 'http', 'co'], ['true', 'north', 'strong', 'free', 'stay', 'safe', 'ottawa', 'prayforottawa', 'canadastrong', 'http', 'co'], ['scene', 'ottawa', 'war', 'memorial', 'soldier', 'duty', 'shot', 'http', 'co'], ['ottawa', 'shooting', 'suspect', 'named', 'michael', 'zehaf', 'bibeau', '2', 'u', 'sources', 'tell', 'abc', 'news'], ['sending', 'thoughts', 'prayers', 'everyone', 'ottawa', 'difficult', 'time', 'pls', 'follow', 'ottawapolice', 'amp', 'rcmpgrcpolice', 'latest', 'info'], ['kevin', 'vickers', ',', 'sergeant', 'arms', 'reportedly', 'killed', 'shooter', 'inside', 'parliament', 'buildings', 'ottawa', 'http', 'co'], ['ottawashooting', 'canada', \"'s\", '9', '11', 'let', 'officials', 'figure', \"'s\", 'going', 'american', 'media', 'fear', 'mongering'], ['breaking', 'ottawa', 'police', 'asking', 'facebook', 'twitter', 'users', 'stop', 'reporting', 'police', 'locations', 'parliamenthill'], ['mayor', 'ottawa', 'says', \"'s\", 'sad', 'amp', 'tragic', 'day', 'city', 'canada', 'http', 'co'], ['sorry', 'ottawa'], ['corporal', 'nathan', 'cirillo', ',', 'shot', 'dead', 'ottawa', 'war', 'memorial', 'today', ',', '24', 'reservist', 'member', 'argyll', 'sutherland', 'highlanders', 'cdnpoli'], ['cancelled', ',', 'advised', 'show', 'business', 'ottawashooting'], ['witnesses', 'today', 'shooting', 'incidents', ',', 'call', '613', '236', '1222'], ['2', 'new', 'victims', 'ottawa', 'hospital', '1', 'shot', ',', 'life', ',', 'hospital', 'says', ',', 'cbcnews', 'https', 'co', 'wcqpcyqmxk'], ['says', 'heightened', 'security', 'across', 'entire', 'light', 'shootings', 'ottawa'], ['stand', 'w', 'canadian', 'scene', 'ottawa', 'stand', '2', 'first', 'responders', 'thoughts'], ['breaking', 'news', 'ottawa', 'police', 'confirm', 'abc', 'news', 'looking', '3', 'suspects', ',', 'one', 'http', 'co', 'lxfiyavimz'], ['ottawashooting', 'situation', 'ongoing', 'remain', 'vigilant', 'aware', 'tip', 'line', '613', '236', '1222', 'x5493', 'c', 'rcmp', 'nat', 'div', 'jt'], ['breaking', '3rd', 'shooting', 'incident', 'reported', 'ottawa', 'rideau', 'centre', 'mall'], ['thoughts', 'family', 'friends', 'cpl', 'nathan', 'cirillo', 'affected', 'events', 'ottawa', 'today', 'ottawastrong'], ['american', 'canadian', 'covering', 'shooting', 'ottawa', 'http', 'co'], ['stay', 'safe', 'ottawa'], ['watch', 'video', 'showing', 'gunfire', 'inside', 'canada', \"'s\", 'parliament', 'ottawa', 'http', 'co', 'http', 'co'], ['justin', 'bieber', 'hockey', 'nice', 'could', \"n't\", 'keep', 'shooting', 'https', 'co'], ['key', 'nothing', 'wilson', \"'s\", 'story', 'new', 'autopsy', 'says', 'life', 'life', 'another', ',', 'shooting'], ['update', 'reports', 'gunfire', 'inside', 'halls', 'canadian', 'parliament', 'building', 'ottawa', 'http', 'co', 'tjyct5yqcv'], ['parliament', 'hill', 'never', 'going'], ['war', 'memorial', 'ottawa', 'soldier', 'shot', 'breaking', 'http', 'co'], ['friend', 'parliament', 'hill', ',', 'amp', 'response', 'ottawashooting', 'http', 'co'], ['ottawa', 'police', 'wo', \"n't\", 'say', 'still', 'suspects', 'large', 'say', 'still', 'active', 'operation', 'live', 'video', 'http', 'co', 'af9hyt9bgx'], ['official', 'says', 'canadian', 'soldier', 'died', 'shot', 'ottawashooting'], ['breaking', 'soldier', 'shot', 'national', 'war', 'memorial', 'died', ',', 'ctv', 'confirmed'], ['today', 'ottawa', 'safe', 'place', 'family', \"'ll\", 'go', 'tonight', 'thinking', 'stay', 'strong', ',', 'stay', 'calm'], ['kevin', 'vickers', ',', 'hero', 'shot', 'parliament', 'hill', 'gunman', 'ottawashooting', 'http', 'co', 'http', 'co'], ['watch', 'live', 'cp24', \"'s\", 'special', 'coverage', 'shootings', 'ottawa', 'http', 'co', 'http', 'co'], ['raw', 'video', 'war', 'memorial', 'shooting', 'http', 'co', 'cbcnews'], ['soldier', 'shot', 'national', 'war', 'memorial', 'ottawa', 'http', 'co', 'cdnpoli'], ['emergency', 'responders', 'building', 'near', 'parliament', 'hill', 'following', 'ottawa', 'shooting', '\\\\(', 'photo', '\\\\)', 'http', 'co'], ['thoughts', 'friends', 'ottawa', 'stay', 'safe', 'everyone'], ['harper', 'make', 'statement', 'today', 'gunman', 'shot', 'dead', 'parliament', 'pm', 'safe', ',', 'parliament', 'hill', ',', 'briefed', 'security', 'officials'], ['canada', \"'s\", 'prime', 'minister', 'stephen', 'harper', 'safe', 'amp', 'left', 'parliament', 'hill', ',', 'lockdown', 'shooting', 'http', 'co'], ['sad', 'someone', 'died', 'ottawa', 'country', 'war', '\\\\?', 'takes', '\\\\?', 'going', '\\\\?'], ['breaking', 'ottawa', 'police', 'confirm', 'ctv', 'news', 'least', 'three', 'separate', 'shootings'], ['video', 'key', 'moments', 'today', \"'s\", 'parliament', 'hill', 'shootings', 'http', 'co', 'http', 'co'], ['police', 'confirming', '3rd', 'shooting', 'rideau', 'center', 'ottawa', ',', 'shopping', 'mall', 'downtown', 'tense', 'city', 'ottawastrong'], ['harper', 'cancels', 'malala', 'citizenship', 'event', 'wake', 'ottawa', 'shootings', 'http', 'co', 'ottawashooting', 'http', 'co'], ['police', 'say', 'two', 'shooting', 'incidents', 'ottawa', 'one', 'war', 'memorial', ',', 'parliament', 'hill', 'http', 'co', 'q98amohu7t'], ['post', 'nathancirillo', 'nation', 'thanks', 'family', 'ottawashooting', 'http', 'co'], ['sergeant', 'arms', 'kevin', 'vickers', 'reportedly', 'shot', 'suspect', 'hailed', 'hero', 'http', 'co', 'ottawashooting', 'http', 'co'], ['update', 'morning', 'shooting', 'incidents', 'occurred', 'national', 'war', 'memorial', 'parliament', 'hill', 'rideau', 'centre', 'ottnews'], ['pictures', 'shots', 'fired', 'parliament', 'hill', ',', 'soldier', 'shot', 'national', 'war', 'memorial', 'http', 'co', 'http', 'co'], ['ottawa', 'police', 'tactical', 'officers', 'guns', 'every', 'reporter', ',', 'us', 'ground'], ['stay', 'safe', 'ottawa', 'need', 'never', 'world'], ['ottawa', 'mayor', 'says', 'sad', 'tragic', 'day', 'city', 'country'], ['\\\\(', '\\\\)', 'horrible', 'day', 'someone', 'life', 'place', 'honour', 'us', 'ottawa'], ['cpl', 'nathan', 'cirillo', ',', 'brave', 'canadian', 'soldier', 'killed', 'ottawashooting', 'today', 'http', 'co'], ['update', 'shots', 'fired', 'parliament', 'hill', ',', 'soldier', 'injured', 'national', 'war', 'memorial', 'ottawa', 'http', 'co'], [\"'we\", 'could', 'gun', 'mps', ',', 'eyewitnesses', 'ottawa', 'shooting', 'scene', 'http', 'co', 'http', 'co'], ['ottawa', 'police', 'chief', 'confirms', 'gunman', 'soldier', 'war', 'memorial', 'dead'], ['locked', 'centre', 'block', 'parliament', 'hill', 'least', 'one', 'shooter', 'fire'], ['breaking', 'police', 'confirm', 'multiple', 'suspects', 'ottawa', 'shooting', 'http', 'co'], ['24', '7', 'use', ',', 'need', 'someone', 'talk', '613', '238', '3311', 'ottawa'], ['updated', 'attacker', 'shot', 'dead', 'parliament', 'soldier', 'shot', 'national', 'war', 'memorial', 'http', 'co', 'cbcott', 'ottnews'], ['building', 'ottawashooting'], ['police', 'asking', 'tweet', ',', ',', 'report', 'police', 'locations', 'amp', 'activity', 'ottawa', 'help', 'jobs', 'please'], ['sad', 'ottawashooting', 'worse', ',', 'shooter', 'calls', 'muslim', 'pls', 'world', '1', '5'], ['norad', 'increases', 'number', 'planes', 'higher', 'alert', 'status', 'ready', 'respond', 'necessary', ',', 'official', 'says', 'http', 'co', 'qsangnqbew', 'ottawashooting'], ['cbcnews', 'independently', 'confirmed', 'gunman', 'killed', 'parliament', 'attack', 'canadian', 'born', 'michael', 'zehaf', 'bibeau', 'ottawashooting'], ['breaking', 'ctv', 'news', 'confirms', 'shots', 'fired', 'parliament', 'hill'], ['soldier', 'killed', 'canada', 'shooting', 'young', 'reservist', 'six', 'year', 'old', 'son', 'http', 'co', 'http', 'co', 'pray'], ['canada', \"'s\", 'coverage', 'ottawa', 'shootings', 'put', 'american', 'cable', 'news', 'shame', 'http', 'co'], ['asking', \"i'm\", 'ok', 'awful', 'events', 'ottawa'], ['hearing', 'shooter', 'still', 'loose', 'ottawa', 'rifle', 'please', 'pray', 'police', 'soldiers', 'prayforottawa'], ['saw', 'gun', ',', 'heard', 'one', 'shot', 'witness', 'ottawashooting', 'canadian', 'parliament', 'http', 'co', 'http', 'co'], [\"'ve\", 'left', 'hill', 'want', 'say', \"i'm\", 'really', 'way', 'ottawa', 'police', 'rcmp', 'business'], ['updated', 'gunman', 'loose', 'soldier', 'shot', 'ottawa', 'war', 'memorial', 'http', 'co', 'http', 'co'], ['breaking', 'soldier', 'shot', 'canadian', 'parliament', ',', 'authorities', 'say', 'http', 'co', 'http', 'co'], ['canada', 'soldier', 'shot', 'today', 'mt', 'guard', 'picture', 'friend', 'rip', 'nathan', 'cirillo', 'http', 'co'], ['earlier', 'reports', 'incident', 'occurred', 'near', 'rideau', 'centre', ',', 'ottawa', 'police', 'say', 'cbcott', 'ottnews'], ['canadian', 'media', 'identify', 'soldier', 'killed', 'ottawa', 'nathan', 'cirillo', ',', '24', 'http', 'co'], ['soldier', 'killed', 'war', 'memorial', 'identified', 'cpl', 'nathan', 'cirillo', 'ottawashooting', 'http', 'co', 'http', 'co'], ['moment', 'silence', 'ottawa', 'http', 'co'], ['hold', 'moment', 'silence', 'honour', 'cpl', 'nathan', 'cirillo', 'affected', 'today', \"'s\", 'events', 'ottawa', 'http', 'co'], ['update', '2', 'dead', 'houston', 'hospital', 'shooting', 'http', 'co'], ['breaking', 'canada', 'police', 'investigating', 'michael', 'zehaf', 'bibeau', 'possible', 'suspect', 'ottawa', 'shooting', 'source', 'matter'], ['u', 'official', 'canadian', 'government', 'informed', 'u', 'one', 'shooter', 'dead', 'ottawa', 'live', 'blog', 'http', 'co', 'q98amohu7t'], ['updated', 'shooting', 'incident', 'ottawa', 'coordinated', 'attack', 'http', 'co', 'http', 'co'], ['us', 'embassy', 'ottawa', 'lockdown', 'following', 'shooting', 'incidents', 'near', 'canadian', 'parliament', 'williamsjon'], [\"'s\", 'footage', 'shot', 'globe', 'reporter', 'josh', 'wingrove', 'gun', 'fire', 'parliament', 'hill', 'building', 'https', 'co', 'fou4pbncdq'], ['happening', 'shots', 'heard', 'near', 'parliament', 'ottawa', ',', 'suspect', 'still', 'large', 'police', 'http', 'co', 'zp9akplh9p', 'http', 'co'], ['today', 'ottawa', 'need', 'talk', ',', ',', ',', '613', '238', '3311'], ['nhl', 'postpones', 'maple', 'leafs', 'senators', 'game', 'tragic', 'shootings', 'ottawa', 'http', 'co', 'http', 'co'], ['kevin', 'vickers', ',', 'head', 'security', 'parliament', 'amp', 'rcmp', 'credited', 'taking', 'shooter', 'ottawa', 'http', 'co'], ['ottawa', 'hospital', 'received', '3', 'patients', ',', '2', 'stable', 'condition', 'wo', \"n't\", 'give', 'update', 'soldier', ',', 'cbcott', 'ottnews'], ['important', 'message', 'children', 'today', 'ottawashooting', 'http', 'co'], ['coming', 'live', 'ottawa', 'shooting', 'stephen', 'harper', 'address', 'nation', 'http', 'co', 'http', 'co'], ['heartbreaking', 'photo', 'shows', 'nathan', 'cirillo', 'moments', 'shooting', 'http', 'co', 'http', 'co'], ['scene', 'war', 'memorial', 'ottawa', 'http', 'co', 'http', 'co'], ['canadian', 'soldier', 'shot', 'war', 'memorial', 'died', 'one', 'three', 'suspected', 'gunmen', 'also', 'dead', ',', 'police', 'say', 'http', 'co'], ['gunman', 'killed', 'shooting', 'war', 'memorial', 'canadian', 'parliament', 'ottawa', ',', 'media', 'reports', 'say', 'http', 'co'], ['breaking', 'peter', 'mansbridge', 'reporting', 'one', 'suspect', 'shot', 'dead', 'inside', 'parliament', 'hill'], ['breaking', 'obama', 'briefed', 'situation', 'ottawa', ',', 'white', 'house', 'official', 'says', 'http', 'co', 'qsangnqbew'], ['players', 'say', 'heard', 'shots', 'morning', 'tomb', 'unknown', 'soldier', 'events', 'ottawa'], ['source', 'parliament', 'hill', 'tells', 'assailant', 'killed', 'cdnpoli', 'ottnews'], ['ottawa', 'police', 'confirm', 'looking', 'one', 'suspect', 'parliament', 'hill', 'shooting', 'http', 'co', '1ckihwuczl'], ['according', 'reports', ',', 'one', 'gunman', 'dead', 'ottawa', 'parliament', 'shooting', 'live', 'coverage', 'http', 'co', 'http', 'co'], ['lockdown', 'many', 'buildings', 'downtown', 'ottawa', 'lockdown', 'shooting', 'http', 'co', 'cbcott', 'ottnews'], ['gunman', 'ottawa', 'shot', 'killed', \"i'm\", 'loss', 'words', 'morning', \"n't\", 'canada'], ['ottawa', 'police', 'service', 'numerous', 'gunmen', 'canada', 'war', 'memorial', 'shooting', 'one', 'person', 'shot', 'http', 'co', 'znhxk6wboy'], ['kudos', 'penguins', ',', 'national', 'tonight', 'flyers', 'pens', 'game', 'gesture', 'ottawa', 'tragedy'], ['sergeant', 'arms', 'kevin', 'vickers', 'hailed', 'hero', 'shooter', 'parliament', 'hill', 'http', 'co', 'http', 'co'], ['rt', 'live', 'cops', 'parliament', 'hill', 'following', 'reports', 'shooting', 'http', 'co', 'http', 'co'], ['thoughts', 'ottawa', 'today', 'stay', 'safe', ',', 'everyone'], ['developing', 'story', 'soldier', 'shot', 'ottawa', 'war', 'memorial', 'http', 'co', 'http', 'co'], ['players', 'staff', 'city', 'ottawa', 'amp', 'canadian', 'armed', 'forces', 'thoughts', 'today'], ['rest', 'peace', 'cpl', 'nathan', 'cirillo', 'thank', 'serving', 'country', 'ottawashooting', 'http', 'co'], ['canada', 'nice', 'people', 'ottawashooting'], ['scene', 'outside', 'canada', \"'s\", 'parliament', 'soldier', 'shot', 'gunman', 'national', 'war', 'memorial', ',', 'http', 'co', 'http', 'co'], ['ottawa', 'shooting', 'police', 'killed', '1', ',', 'believe', '2', '3', 'still', 'large', 'reports', 'http', 'co', 'zp9akplh9p', 'http', 'co'], ['breaking', 'news', 'photo', 'gallery', 'parliament', 'hill', ',', 'soldier', 'shot', 'national', 'war', 'memorial', 'http', 'co', 'http', 'co'], ['breaking', 'source', 'confirms', 'cp24', 'deceased', 'soldier', 'ottawa', 'cpl', 'nathan', 'cirillo', 'cirillo', 'member', 'hamilton', 'argylls'], ['thoughts', 'ottawa', 'today', 'parliament'], ['raw', 'video', 'parliament', 'hill', 'shooting', 'video', 'inside', 'parliament', 'hill', 'shots', 'fired', 'breaking', 'http', 'co', 'cbcnews'], ['justin', 'bieber', 'shooting', '11', 'http', 'co'], ['shooting', 'canadian', 'parliament', 'one', 'suspect', 'reported', 'dead', 'http', 'co'], ['reports', 'shooter', 'shot', 'downtown', 'ottawa', 'ottnews'], ['ottawa', \"n't\", 'tweet', 'location', \"n't\", 'tweet', 'location', 'police', 'units', 'resist', 'urge', 'look', \"'s\", 'happening', ',', 'get', 'area'], ['rcmp', 'hold', 'news', 'conference', 'close', '2', 'provide', 'update', 'shootings', 'ottawa'], ['watch', 'cbcnews', 'coverage', 'ottawa', 'shootings', 'put', 'american', 'cable', 'news', 'shame', 'http', 'co', 'via'], ['caucus', 'room', 'barricaded', 'shooting', 'http', 'co'], ['canada', \"'s\", 'parliament', 'lockdown', 'shooting', 'follow', 'live', 'coverage', 'http', 'co', 'http', 'co'], ['witness', 'tells', 'cnn', 'gunman', 'shot', 'one', 'two', 'soldiers', 'standing', 'guard', 'war', 'memorial', 'ottawa', 'live', 'blog', 'http', 'co', 'q98amohu7t'], ['pictures', 'video', 'suspect', ',', 'email', 'natdiv', 'media', 'divnat', 'rcmp', 'grc', 'gc', 'ca', 'ottawa', 'ottcity', 'ottnews'], ['rt', 'globeandmail', 'soldier', 'killed', 'war', 'memorial', 'identified', 'cpl', 'nathan', 'cirillo', 'ottawashooting', 'http', 'co'], ['incredible', 'photo', 'woman', 'gives', 'mouth', 'mouth', 'wounded', 'solider', 'ottawashooting', 'http', 'co', 'http', 'co'], ['canadian', 'soldier', 'killed', 'ottawashooting', 'today', 'cpl', 'nathan', 'cirillo', 'thoughts', 'amp', 'prayers', 'family', 'http', 'co'], ['folks', \"'s\", 'nhl', 'game', 'ottawa', \"'re\", 'canada', 'whole', 'nation', 'let', 'rip'], ['ottpolice', 'rcmpgrcpolice', 'investigating', 'several', 'shooting', 'incidents', 'downtown', 'ottawa'], [',', 'soldier', 'killed', 'ottawa', 'identified', 'cpl', 'nathan', 'frank', 'cirillo', 'http', 'co', 'http', 'co'], ['ottawa', 'police', 'say', 'area', 'downtown', 'hill', '\\\\(', 'see', 'map', '\\\\)', 'still', 'security', 'perimeter', 'lockdown', '5', 'pm', 'et', 'http', 'co'], ['prayers', 'shot', 'soldier', 'ottawa', 'god', 'bless', 'canadian', 'army', 'canada'], ['let', 'nathancirillo', 'like', 'part', 'time', 'soldier', 'job', 'amp', 'little', 'ottawa', 'http', 'co'], ['thoughts', 'prayers', 'go', 'nathan', 'cirillo', 'died', 'today', 'ottawa', 'protecting', 'country', '24', 'http', 'co'], ['violence', 'ottawashooting', 'left', 'city', 'chaos', ',', \"'s\", 'know', 'http', 'co', 'http', 'co'], ['ottawa', 'police', 'spokesman', 'tells', 'abc', 'news', '3', 'suspects', 'following', 'shootings', 'canadian', 'capital', 'worldnews'], ['ctv', 'received', 'ottawa', 'police', 'three', 'different', 'shooting', 'incidents', 'ottnews', 'ottawashooting'], ['prayers', 'go', 'victims', 'ottawa', 'shooting'], ['canadian', 'soldier', 'shot', 'ottawa', 'reservist', 'hamilton', 'http', 'co', 'hamont', 'ottawashooting', 'http', 'co'], ['canada', 'parliament', 'shooting', 'still', 'active', 'shooter', '3', 'separate', 'shooting', 'sites', 'multiple', 'gunmen', '1', 'gunmen', 'dead', 'http', 'co', 'lxfiyavimz'], ['soldiers', 'back', 'guarding', 'tomb', 'unknown', 'soldier', 'today', \"'s\", 'shooting', 'http', 'co'], ['breaking', 'dozens', 'shots', 'fired', 'canada', \"'s\", 'parliament', 'gunman', 'entered', 'main', 'door', ',', 'ottawa', 'journalist', 'says', 'http', 'co', 'qsangnqbew'], ['fbi', 'working', 'canadian', 'authorities', 'ottawa', 'shooting', 'act', 'terrorism', 'http', 'co'], ['soldier', 'killed', 'war', 'memorial', 'identified', 'cpl', 'nathan', 'cirillo', 'ottawashooting', 'http', 'co', 'http', 'co'], ['breaking', 'hospital', 'says', 'received', '3', 'victims', 'ottawa', 'shootings', ',', '2', 'stable', 'condition'], ['nhl', 'game', 'tonight', 'ottawa'], ['working', 'shooting', 'victim', 'cbc', \"'s\", 'jason', 'cbcott', 'ottnews', 'http', 'co'], ['way', 'canadians', 'ottawashooting', 'keeping', 'victims'], ['thoughts', 'friends', 'ottawa', 'today', 'difficult', 'time', 'staysafeottawa'], ['breaking', 'centre', 'block', 'east', 'block', 'parliament', 'hill', 'locked', 'cbcott', 'ottnews'], ['canadian', 'soldier', 'shot', 'war', 'memorial', 'died', ',', 'officials', 'reported', 'http', 'co', 'ottawashooting', 'http', 'co'], ['developing', 'shooting', 'suspect', 'large', 'inside', 'canadian', 'parliament', 'reports', 'http', 'co', 'http', 'co'], ['thoughts', 'tragic', 'day', 'ottawa', 'http', 'co'], ['breaking', 'reports', 'dead', 'shooting', 'suspect', 'michael', 'zehaf', 'bibeau', ',', 'born', '1982', 'reportedly', 'canadian', 'born', 'ottawashooting'], ['ottawa', 'police', 'stations', 'closed', 'university', 'ottawa', 'lockdown', 'campus', 'closed'], ['police', 'two', 'shootings', 'ottawa', 'today', ',', 'three', 'war', 'memorial', 'parliament', 'hill'], ['shots', 'fired', 'near', 'parliament', 'ottawa', ',', 'suspect', 'still', 'large', 'tv'], ['ottawa', 'police', 'correct', 'info', 'shooting', 'rideau', 'shopping', 'centre', 'points', 'one', 'gunman', 'two', 'locations'], ['rest', 'peace', 'cpl', 'nathan', 'cirillo', 'ottawa', 'ottawashooting', 'http', 'co'], ['full', 'white', 'house', 'statement', 'obama', \"'s\", 'call', 'ottawa', 'http', 'co'], ['least', '30', 'shots', 'fired', 'inside', 'parliament', 'ottawa', 'http', 'co', '1ckihwuczl'], ['moment', 'silence', 'also', 'affected', 'tragedy', 'ottawa', 'today'], ['hamilton', 'argylls', 'soldier', ',', 'nathan', 'cirillo', ',', 'right', 'picture', ',', 'dies', 'ottawa', 'attack', 'http', 'co', 'http', 'co'], ['one', 'person', 'shot', 'outside', 'centre', 'block', ',', 'second', 'wounded', 'inside', 'building', 'parliament', 'hill', 'shooting', 'http', 'co'], ['rip', 'cpl', 'nathan', 'cirillo', 'today', \"'s\", 'tragic', 'events', 'put', 'ottawashooting', 'canadianforces', 'http', 'co'], ['ok', 'stop', 'calling', 'royal', 'canadian', 'police', 'ottawashooting', 'heroes', 'kevinvickers'], ['breaking', 'shooting', 'parliament', 'hill', 'rcmp', 'weapons', 'drawn', 'cdnpoli', 'http', 'co'], ['rt', 'cbcottawa', 'confirmed', '1', 'shooter', 'shot', 'dead', 'parliament', 'hill', ',', '1', 'suspect', 'cbcott', 'ottnews'], ['rip', 'cpl', 'nathancirillo', 'service', 'never', 'forgotten', 'prayers', 'family', 'canadianforces', 'ottawashooting', 'http', 'co'], ['thoughts', 'prayers', ',', 'canadians', ',', 'everyone', 'ottawa', 'brave', 'women', 'men', 'armed', 'forces', 'today'], ['soldier', 'shot', 'national', 'war', 'memorial', 'ottawa', 'http', 'co', 'http', 'co'], ['police', 'say', 'shots', 'fired', '3', 'ottawa', 'sites', 'national', 'war', 'memorial', ',', 'parliament', 'hill', ',', 'rideau', 'shopping', 'centre', 'http', 'co'], ['thinking', 'layton', \"'s\", 'words', 'love', 'better', 'anger', 'hope', 'better', 'fear', 'optimism', 'better', 'despair', 'ottawashooting'], ['canada', 'great', 'rt', 'message', 'toronto', 'police', 'imams', 'mosques', 'wake', 'ottawashootings', 'http', 'co'], ['news', 'attack', 'ottawa', 'pray', 'everyone', \"'s\", 'safety'], ['true', 'north', 'free', 'rip', 'solider', 'shot', 'ottawa', 'shooting', 'canadastrong', 'ottawashooting', 'http', 'co'], ['pm', 'stephen', 'harper', 'address', 'nation', 'ottawashooting', '7', 'pm', 'et', 'pmharper', 'cdnpoli', 'http', 'co'], ['gunman', 'shot', 'dead', 'ottawashootings', 'named', 'us', 'officials', '32', 'year', 'old', ',', 'canadian', 'born', ',', 'michael', 'zehaf', 'bibeau', 'http', 'co'], ['according', 'http', 'co', \"'s\", '2014', '15', 'gm', 'survey', ',', 'kobe', '3rd', 'best', 'shooting', 'guard', 'http', 'co', 'http', 'co'], ['service', 'downtown', 'ottawa', 'currently', 'details', 'http', 'co'], ['pittsburgh', 'play', 'canadian', 'national', 'anthem', 'game', 'support', 'ottawa', 'makes', 'http', 'co'], ['sources', 'tell', 'may', 'many', 'five', 'active', 'shooters', 'ottawa', 'http', 'co', 'http', 'co'], ['thoughts', 'face', 'tragic', 'events', 'ottawa', ',', 'nation', 'capital', 'http', 'co'], ['updated', 'canadian', 'soldier', 'killed', 'attack', 'parliament', 'hill', 'ottawa', 'http', 'co'], ['canadian', 'soldier', 'killed', 'ottawa', 'identified', 'http', 'co', 'cdnpoli', 'ottawashooting', 'http', 'co'], ['breaking', 'news', 'cbc', 'confirms', 'photo', 'michael', 'zehaf', 'bibeau', ',', 'gunman', 'ottawa', 'shooting', 'http', 'co', '58ik7fmca1', 'http', 'co'], ['\\\\?', 'malala', 'canadian', 'citizenship', 'ottawa', 'today', 'http', 'co'], ['ottawa', ',', \"'ve\", 'mind', 'day', 'keeping', 'heart', 'staysafeottawa', 'http', 'co'], ['thoughts', 'city', 'ottawa', 'affected', 'today', \"'s\", 'events', 'safe'], ['kudos', 'first', 'responders', 'working', 'ottawa', 'today', 'heroes', 'http', 'co'], ['remember', 'cpl', 'nathan', 'cirillo', 'never', 'speak', 'shooter', 'ottawashooting', 'staysafeottawa', 'http', 'co'], ['chief', 'defence', 'staff', 'mt', 'joerayment', 'surreal', 'photos', 'ottawa', 'vip', 'escorted', 'sparks', 'http', 'co', 'http', 'co'], ['extended', 'video', 'numerous', 'heard', 'inside', 'centre', 'block', 'parliament', 'hill', 'http', 'co'], ['canada', 'footage', 'globe', 'reporter', 'captures', 'exchange', 'gun', 'fire', 'parliament', 'hill', 'building', 'http', 'co'], ['live', 'coverage', 'several', 'shooting', 'incidents', 'near', 'canadian', 'parliament', 'http', 'co', '0nuzhcvr2j', 'http', 'co'], ['party', 'wow', 'acarvin', 'parliament', 'caucus', 'room', 'barricaded', 'chairs', ',', 'via', 'grahamctv', 'http', 'co', 'ottawa'], ['shots', 'fired', 'canada', \"'s\", 'national', 'war', 'memorial', 'ottawa', 'guard', 'shot', ',', 'police', 'say', 'http', 'co'], ['shooting', 'incidents', 'ottawa', 'today', 'ongoing', 'investigation', 'http', 'co'], ['chaos', 'ottawa', 'shooting', 'war', 'memorial', 'reports', 'gunfire', 'parliament', 'http', 'co'], ['white', 'house', 'says', 'us', 'ready', 'offer', 'canada', 'necessary', 'assistance', 'shootings', 'ottawa', 'speak', 'pm', 'harper', 'today'], ['ottawa', 'shooting', 'nathan', 'cirillo', ',', 'reservist', 'hamilton', ',', 'killed', 'attack', 'http', 'co', 'cuzepktlxx', 'http', 'co'], ['surreal', 'photos', 'ottawa', 'vip', 'escorted', 'sparks', 'http', 'co', 'm4fe8krpxl', 'http', 'co'], ['rip', 'cpl', 'nathan', 'cirillo', 'identified', 'canadian', 'soldier', 'killed', 'ottawashooting', 'canada', 'http', 'co'], ['pmharper', 'met', 'cabinet', 'shootings', 'ottawa', 'today', 'http', 'co'], ['please', 'post', 'videos', 'photos', 'going', 'incident', 'ensure', 'safety', 'first', 'responders', 'public', 'ottawa', 'jt'], ['ottawa', 'police', 'building', 'remain', 'lockdown', 'closed', 'public', 'ottnews'], ['breaking', 'news', 'shots', 'fired', 'parliament', 'hill', 'follow', 'developing', 'story', 'http', 'co', 'http', 'co'], ['terrible', 'news', 'ottawa', 'today', 'thoughts', 'prayers', 'everyone', 'involved', 'ottawastrong', 'http', 'co'], ['soldier', 'shot', 'canada', 'national', 'war', 'memorial', 'died', 'police', 'looking', 'several', 'suspects', 'http', 'co', 'http', 'co'], ['guilty', 'blackwater', 'case', ',', 'manslaughter', ',', 'weapons', '2007', 'square', 'shooting', 'criminal', 'act'], ['fbi', 'case', 'ottawa', 'shooting', ',', 'sources', 'confirmed', 'ctv', 'news'], ['watch', 'live', 'pm', 'stephen', 'harper', 'expected', 'speak', 'nation', 'ottawashooting', 'http', 'co', 'cbcnews', 'cdnpoli'], ['updated', 'soldier', 'dead', ',', '2', 'others', 'injured', 'parliament', 'hill', '1', 'gunman', 'dead', 'http', 'co', 'http', 'co'], ['shooting', 'near', 'rideau', 'centre', ',', 'police', 'say', 'inside', 'cbcott', 'ottnews'], ['breaking', 'canadian', 'official', 'identifies', 'dead', 'ottawa', 'gunman', 'michael', 'zehaf', 'bibeau'], ['breaking', 'canadian', 'officials', 'soldier', 'shot', 'ottawa', 'died', 'http', 'co'], ['stay', 'safe', 'everyone', 'back', 'home', 'ottawa', 'prayforottawa'], ['soldiers', 'cenotaph', 'ottawashooting', 'http', 'co'], ['right', 'friends', 'monday', \"'s\", 'car', 'incident', ',', 'ottawa', 'shooting', 'morning', 'https', 'co'], ['3', 'locations', 'shootings', 'ottawa', 'around', 'parliament', 'hill', '\\\\(', '\\\\)', 'http', 'co'], ['rt', 'bts', 'bighit', 'episode', 'bts', \"'war\", \"hormone'\", 'mv', 'amp', 'concept', 'photo', 'shooting', '\\\\(', 'eng', 'sub', '\\\\)', '\\\\(', 'http', 'co', '\\\\)', 'hello', 'hello'], ['map', 'areas', 'ottawa', 'shootings', 'reported', 'national', 'war', 'memorial', ',', 'near', 'rideau', 'centre', 'parliament', 'hill', 'http', 'co'], ['home', 'true', 'north', 'strong', 'amp', 'free', 'thoughts', 'amp', 'prayers', 'people', 'ottawa', 'amp', 'canada', 'ottawastrong', 'http', 'co'], ['two', 'tory', 'mps', 'say', 'gunman', 'parliament', 'hill', 'http', 'co', 'http', 'co'], ['sickening', 'news', 'morning', 'thoughts', 'downtown', 'law', 'men', 'women', 'working', 'keep', 'city', 'safe', 'ottawa'], ['monitoring', 'situation', 'ensure', 'jobs', 'keeping', 'canadians', 'safe', 'ottawashooting'], ['4', 'dedicated', 'ottawashooting', '5', ',', 'amp', '11pm', 'et', 'watch', 'live', 'http', 'co', 'http', 'co'], ['ottawa', 'police', 'confirm', 'shooting', 'canada', \"'s\", 'national', 'war', 'memorial', 'cbcottawa', 'http', 'co'], ['live', 'blog', 'tweets', 'reporters', 'covering', 'shootings', 'ottawa', ',', 'http', 'co', 'cbcott', 'ottnews', 'http', 'co'], ['photos', 'scenes', 'canada', \"'s\", 'day', 'ottawashooting', 'http', 'co', 'http', 'co'], ['thoughts', 'prayers', 'everyone', 'affected', 'tragic', 'events', 'ottawa', 'morning', 'stay', 'safe', '!', 'ottawastrong'], ['petermansbridge', 'tells', 'us', 'know', 'right', 'ottawashooting', 'http', 'co'], ['ottawa', 'hospital', 'confirms', 'received', '3', 'patients', '2', 'stable', 'condition', 'live', 'blog', 'http', 'co', 'q98amohu7t'], ['thoughts', 'goes', 'involved', 'tragic', 'events', 'morning', 'ottawa'], ['people', 'watching', 'covering', 'events', 'ottawa', 'today', '\\\\(', 'via', 'onthemedia', '\\\\)', 'safe', 'everyone', 'cdnpoli', 'http', 'co'], ['ottawa', 'canada', 'identifies', 'gunman', 'muslim', 'http', 'co', 'http', 'co'], ['anyone', 'tips', 'shooting', 'incidents', ',', 'looking', 'report', 'suspicious', 'activity', ',', 'call', '613', '236', '1222', 'x5493', 'call', '9', '1', '1'], ['police', 'say', '3', 'shooting', 'scenes', 'parliament', ',', 'war', 'memorial', ',', 'rideau', 'centre', 'happened', 'close', 'together', ',', 'police', 'say', 'cbcott', 'ottnews'], ['police', 'investigating', 'three', 'shooting', 'incidents', 'ottawa', 'parliament', ',', 'shopping', 'mall', 'war', 'memorial', 'http', 'co', '1ckihwuczl'], ['ottawa', 'city', 'hall', 'currently', 'lock', 'please', 'avoid', 'area'], ['rest', 'peace', 'cpl', 'nathan', 'cirillo', 'via', 'globeandmail', 'ottawashooting', 'http', 'co', 'http', 'co'], ['statement', 'hamilton', 'mayor', 'death', 'soldier', 'ottawashooting', 'http', 'co'], ['ap', 'video', 'gunman', 'canadian', 'soldier', 'standing', 'guard', 'national', 'war', 'memorial', 'ottawa', 'http', 'co'], ['want', 'courage', 'first', 'responders', 'ground', 'ottawa', 'right'], ['rcmp', 'ottawa', 'police', 'news', 'conference', 'watch', 'live', 'http', 'co', 'alz6qu6mca', 'cbcott', 'ottnews'], ['rt', 'ottawa', \"n't\", 'tweet', 'location', \"n't\", 'tweet', 'location', 'police', 'units', 'resist', 'urge', 'look', \"'s\", 'happening'], ['ottawa', 'police', ',', 'twitter', 'users', 'urging', 'scene', 'stop', 'tweeting', 'police', 'officer', 'locations', 'ottawa'], ['one', 'strong', 'prayforottawa', 'ottawashooting', 'http', 'co'], ['kevin', 'vickers', 'hailed', 'hero', 'took', 'attacker', 'parliament', 'hill', 'http', 'co', 'cbcott', 'ottnews', 'http', 'co'], ['ottawa', 'shootings', 'check', 'breaking', 'news', 'handbook', 'onthemedia', 'http', 'co', 'http', 'co'], ['breaking', 'ottawa', 'police', 'say', 'believe', 'one', 'shooter'], ['thoughts', 'prayers', 'toronto', 'ottawa', 'right', ',', 'families', 'affected', 'attacks'], ['please', 'tweet', 'locations', 'officers', 'ottawa', 'ottawapolice', 'light', '!', 'safety'], ['monday', ',', 'back', 'shooting', ',', '1', '1', 'game', 'vs', 'http', 'co'], ['statement', 'ottawa', 'hospital', 'received', '3', 'patients', '2', 'stable', 'condition', 'http', 'co'], ['talk', 'kids', 'ottawa', 'shootings', ',', 'tv', ',', 'say', 'http', 'co', 'http', 'co'], ['pm', 'canadian', 'pm', 'harper', 'condolences', 'israeli', 'people', 'wake', 'terrorist', 'attack', 'ottawa'], ['follow', 'live', 'coverage', 'shootings', 'ottawa', 'http', 'co', 'ottawashooting', 'http', 'co'], ['anyone', 'needs', 'talk', ',', 'call', 'us', '613', '238', '3311', 'city', 'ottawa'], ['breaking', 'two', 'new', 'patients', 'coming', 'ottawa', 'hospital', 'civic', 'campus', 'one', 'gunshot', 'wounds', 'confirmed', 'ottnews'], ['watching', 'peter', 'mansbridge', 'cbc', 'canadian', 'calm', ',', ',', 'fear', 'mongering', 'way', 'ottawashooting', 'cbc'], ['breaking', 'rcmp', 'rcmp', 'advises', 'downtown', 'ottawa', 'stay', 'away', 'windows', 'roofs', 'due', 'ongoing', 'police', 'incident'], ['american', 'media', 'stop', 'calling', 'ottawashooting', 'canada', \"'s\", \"n't\", 'know', 'yet', 'canadians', 'facts', ',', 'fear', 'cdnpoli'], ['canada', 'parliament', 'shooting', 'active', 'shooter', '3', 'separate', 'shooting', 'sites', '3', 'multiple', 'gunmen', '1', 'soldier', 'dead', 'http', 'co', 'lxfiyavimz'], ['ottawa', 'today', ',', \"'s\", 'amazing', 'job', 'keeping', 'young', 'soldier', 'alive'], ['photos', 'scene', 'unfolding', 'multiple', 'shootings', 'ottawa', 'morning', 'http', 'co', 'http', 'co'], ['everyone', 'ottawa', ',', 'please', 'stay', 'safe', 'prayforottawa'], ['soldier', 'killed', 'ottawashooting', 'identified', 'cpl', 'nathan', 'cirillo', '\\\\(', 'via', 'globeandmail', '\\\\)', 'http', 'co', 'http', 'co'], ['shot', 'inside', 'caucus', 'room', 'shooting', 'ctvottawa', 'ottawashooting', 'http', 'co'], ['last', 'three', ',', 'kobe', '26', '7', 'points', 'shooting', ',', '4', '3', '3', '7', '3', 'minutes'], ['sending', 'love', 'people', 'ottawa', 'safe', 'everyone'], ['cbcnews', 'reporting', 'known', 'american', 'media', 'could', 'use', 'restraint', 'ottawashooting'], ['breaking', 'police', 'asking', 'media', 'run', 'get', 'away', 'still', 'active', 'shooting', 'reports', 'police', 'guns', 'drawn', 'rideaucentre', 'ottnews'], ['rcmp', 'news', 'conference', 'ottawa', 'shootings', 'expected', 'watch', 'live', 'http', 'co', 'kngapktsce'], ['map', 'locations', 'shootings', 'ottawa', ',', 'war', 'memorial', ',', 'house', 'nearby', 'mall', 'http', 'co', 'http', 'co'], ['spokesman', 'says', 'prime', 'minister', 'stephen', 'harper', 'safe', 'left', 'parliament', 'hill', 'http', 'co'], ['thoughts', 'amp', 'prayers', 'family', 'cpl', 'nathan', 'frank', 'cirillo', 'killed', 'today', 'ottawa', 'rip', 'pls', 'rt', 'http', 'co'], ['latest', 'numerous', 'gunmen', 'involved', 'ottawa', 'shooting', 'attacks', ',', 'manhunt', 'police', 'spokesman', 'http', 'co', 'zp9akplh9p', 'http', 'co'], ['may', 'want', 'stop', 'take', 'deep', 'tweeting', 'reporters', 'ottawa', 'today', ',', 'ok', '\\\\?'], [\"'ve\", 'lowered', 'flags', 'half', 'honour', 'canadian', 'reservist', 'life', 'ottawa', 'http', 'co'], ['shots', 'fired', 'parliament', 'hill'], ['city', 'hamilton', 'offers', 'condolences', 'family', 'cpl', 'nathan', 'cirillo', ',', 'hamont', 'bravely', 'served', 'ottawa'], ['leafs', 'sens', 'game', 'postponed', 'shootings', 'parliament', 'hill', 'http', 'co'], ['prayforottawa', 'american', ',', 'prayers', 'w', 'canadian', 'soldier', 'died', 'today', \"'s\", 'shooting', 'standing', 'duty', 'war', 'memorial'], [\"i'm\", 'today', \"'s\", 'attack', 'ottawa', 'offer', 'full', 'support', 'pmharper', 'canadian', 'people', 'incident'], ['cops', 'running', 'guns', 'drawn', ',', \"'re\", 'downtown', 'ottawa', ',', 'snipers', 'rooftops'], ['wow', 'photo', 'mp', 'nina', 'grewal', 'showing', 'doors', 'room', 'barricaded', 'parliamenthill', 'ottawa', 'http', 'co'], ['hear', 'soldier', 'guarding', 'national', 'war', 'memorial', 'died', 'rest', 'peace', 'cpl', 'nathan', 'cirillo', 'ottawashooting'], ['breaking', 'one', 'assailant', 'shot', 'killed', 'parliament', 'hill', 'still', 'active', 'shooting', 'situation', 'amp', 'shots', 'fired', 'ottnews'], ['thoughts', 'affected', 'today', \"'s\", 'events', 'ottawa'], ['soldier', 'killed', 'war', 'memorial', 'identified', 'nathan', 'cirillo', 'hamont', 'ottawashooting', 'cndpoli', 'http', 'co', 'http', 'co'], ['soldier', 'killed', 'ottawa', 'could', \"n't\", 'country', 'rip', 'cpl', 'nathan', 'frank', 'cirillo', 'http', 'co'], ['parliament', 'hill', 'security', 'situation', 'evolving', 'lockdown', 'offices', 'around', 'hill', 'ottawashooting'], ['canada', 'showing', 'great', 'nation', 'http', 'co', 'ottawashooting', 'canadastrong'], ['parliament', 'caucus', 'room', 'barricaded', 'chairs', ',', 'via', 'grahamctv', 'http', 'co', 'ottawa'], ['public', 'servants', 'asked', 'stay', 'inside', 'buildings', 'active', 'shooter', 'investigation', 'continues', 'ottawa', 'cbcott', 'ottnews'], ['isis', 'media', 'account', 'posts', 'pic', 'claiming', 'michael', 'zehaf', 'bibeau', ',', 'dead', 'ottawashooting', 'terrorist', 'http', 'co', 'via', 'armedresearch'], ['ottawashooting', 'police', 'say', 'investigating', '3', 'shootings', ',', 'one', 'war', 'memorial', ',', 'one', 'parliament', 'hill', 'one', 'near', 'rideau', 'centre'], ['soldier', 'killed', 'war', 'memorial', 'identified', 'cpl', 'nathan', 'cirillo', 'ottawashooting', 'http', 'co', 'http', 'co'], ['inside', 'parliament', 'building', 'ottawa', 'shots', 'fired', 'police', 'sweep', 'https', 'co', 'fou4pbncdq', 'footage', 'shot', 'josh', 'wingrove'], ['asked', 'police', 'still', 'gunman', 'loose', 'officer', 'safe', 'environment', 'everyone', 'go', 'home', 'hug', 'family', 'ottawa'], ['soldier', 'shot', ',', 'parliament', 'locked', 'gunfire', 'war', 'memorial', 'developing', 'story', 'http', 'co', 'http', 'co'], ['ottawa', 'stood', 'us', 'amp', 'called', 'us', 'today', 'stand', 'ottawastrong', 'ottawashooting'], ['breaking', 'ottawa', 'police', 'confirm', 'member', 'canadian', 'forces', 'succumbed', 'injuries', 'hospital', 'following', 'shooting'], ['soldier', 'shot', 'dead', 'wednesday', \"'s\", 'ottawa', 'attacks', 'named', 'cpl', 'nathan', 'cirillo', 'hamont', 'ottawashooting', 'http', 'co'], ['sergeant', 'arms', 'canada', \"'s\", 'parliament', 'called', 'hero', 'shooting', 'gunman', 'entered', 'building', 'http', 'co', 'http', 'co'], ['condolences', 'family', 'cpl', 'nathan', 'cirillo', ',', 'killed', 'ottawashooting', 'http', 'co', 'http', 'co'], ['soldier', 'gets', 'cpr', 'shot', 'war', 'memorial', 'ottawa', 'https', 'co'], ['shooting', 'war', 'memorial', 'part', 'area', 'evacuated', 'avoid', 'rideau', 'center', 'surroundings', 'uottawa', 'monitoring'], ['least', '2', 'shot', ',', '1', 'fatally', ',', 'houston', 'hospital', 'shooting', ',', 'reports', 'say', 'http', 'co'], ['updated', 'photos', 'shooting', 'parliament', 'hill', 'http', 'co', 'http', 'co'], ['ottawa', 'one', 'world', \"'s\", 'great', 'today', \"'s\", 'events', 'change'], ['breaking', 'ottawa', 'civic', 'official', 'says', 'two', 'new', 'patients', 'hospital', '1', 'w', 'gunshot', 'wounds', 'stable', 'ctvottawa', 'ottnews'], ['woman', 'hamilton', 'reservist', 'shot', 'ottawa', 'ottawashooting', 'http', 'co'], ['stay', 'us', 'live', 'coverage', 'active', 'shooting', 'canadian', 'parliament', 'http', 'co', 'http', 'co'], ['breaking', 'soldier', 'killed', 'war', 'memorial', 'identified', 'cpl', 'nathan', 'cirillo', 'ottawashooting', 'http', 'co', 'http', 'co'], ['developing', 'story', 'gunman', 'killed', 'ottawa', '3', 'shootings', 'confirmed', 'http', 'co'], ['please', 'feel', 'free', 'post', 'status', 'show', 'support', 'ottawa', 'ottawastrong', 'http', 'co'], ['watch', 'live', 'scene', 'outside', 'parliament', 'hill', 'soldier', 'shot', 'http', 'co'], ['ottawa', 'police', 'confirming', 'shooting', 'war', 'memorial', 'minutes', 'ago', 'info', 'cbcott', 'ottnews'], ['still', 'lockdown', 'safe', 'check', 'globeandmail', 'video', 'attack', 'parliament', 'hill', 'hear']]\n"
          ]
        }
      ],
      "source": [
        "print(tokenize_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvQOUXX-Q83p"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging prints\n",
        "# print(f\"gpt_embedding: {gpt_embedding}, type: {type(gpt_embedding)}, shape: {gpt_embedding.shape}\")\n",
        "# print(f\"bert_embedding: {bert_embedding}, type: {type(bert_embedding)}, shape: {bert_embedding.shape}\")\n"
      ],
      "metadata": {
        "id": "bgAQV1BQfv7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swcU1zJNfxxD",
        "outputId": "2e7d662b-7d3c-42d1-8dbb-f5dec62df5d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "within else\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import scipy.sparse as sp\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel\n",
        "\n",
        "# Assuming these variables are defined earlier in your code\n",
        "# device, node_size, train_size, test_size, tokenize_sentences, word_list\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return features\n",
        "\n",
        "def pad_or_truncate_embedding(embedding, max_length):\n",
        "    \"\"\"Pad or truncate embedding to a fixed length\"\"\"\n",
        "    if len(embedding) > max_length:\n",
        "        return embedding[:max_length]\n",
        "    else:\n",
        "        padding = np.zeros((max_length - len(embedding),))\n",
        "        return np.concatenate((embedding, padding))\n",
        "\n",
        "if NODE == 0:\n",
        "    features = torch.arange(node_size, dtype=torch.float32).to(device)\n",
        "    print(\"within if\")\n",
        "else:\n",
        "    print(\"within else\")\n",
        "\n",
        "    # Load BERT and GPT-2 models and tokenizers\n",
        "    gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    gpt_model = GPT2Model.from_pretrained('gpt2').to(device)\n",
        "\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "    sent_embs = []\n",
        "    word_embs = {}\n",
        "\n",
        "    gpt_embedding_dim = gpt_model.config.hidden_size\n",
        "    bert_embedding_dim = bert_model.config.hidden_size\n",
        "    combined_embedding_dim = gpt_embedding_dim + bert_embedding_dim\n",
        "\n",
        "    # for ind in tqdm(range(train_size + test_size)):\n",
        "    #     sent = tokenize_sentences[ind]\n",
        "    #     sentence_str = \" \".join(sent[:512])\n",
        "\n",
        "    #     # GPT-2 embeddings\n",
        "    #     gpt_inputs = gpt_tokenizer(sentence_str, return_tensors='pt').to(device)\n",
        "    #     with torch.no_grad():\n",
        "    #         gpt_outputs = gpt_model(**gpt_inputs)\n",
        "    #     gpt_sentence_embedding = gpt_outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    #     gpt_tokens = gpt_tokenizer.tokenize(sentence_str)\n",
        "    #     gpt_token_embeddings = gpt_outputs.last_hidden_state.squeeze().cpu().numpy()\n",
        "\n",
        "    #     # BERT embeddings\n",
        "    #     bert_inputs = bert_tokenizer(sentence_str, return_tensors='pt').to(device)\n",
        "    #     with torch.no_grad():\n",
        "    #         bert_outputs = bert_model(**bert_inputs)\n",
        "    #     bert_sentence_embedding = bert_outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    #     bert_tokens = bert_tokenizer.tokenize(sentence_str)\n",
        "    #     bert_token_embeddings = bert_outputs.last_hidden_state.squeeze().cpu().numpy()\n",
        "\n",
        "    #     # Combined sentence embedding\n",
        "    #     combined_sentence_embedding = np.concatenate((gpt_sentence_embedding, bert_sentence_embedding))\n",
        "    #     sent_embs.append(combined_sentence_embedding)\n",
        "\n",
        "    #     # Combined word embeddings\n",
        "    #     for gpt_token, gpt_embedding in zip(gpt_tokens, gpt_token_embeddings):\n",
        "    #         gpt_embedding = pad_or_truncate_embedding(gpt_embedding, gpt_embedding_dim)\n",
        "    #         if gpt_token not in word_embs:\n",
        "    #             word_embs[gpt_token] = gpt_embedding\n",
        "    #         else:\n",
        "    #             word_embs[gpt_token] = np.minimum(word_embs[gpt_token], gpt_embedding)\n",
        "\n",
        "    #     for bert_token, bert_embedding in zip(bert_tokens, bert_token_embeddings):\n",
        "    #         bert_embedding = pad_or_truncate_embedding(bert_embedding, bert_embedding_dim)\n",
        "    #         if bert_token not in word_embs:\n",
        "    #             word_embs[bert_token] = bert_embedding\n",
        "    #         else:\n",
        "    #             word_embs[bert_token] = np.minimum(word_embs[bert_token], bert_embedding)\n",
        "\n",
        "    # word_embs_list = [pad_or_truncate_embedding(word_embs[word], combined_embedding_dim) for word in word_list if word in word_embs]\n",
        "\n",
        "    # # Ensure all combined embeddings have the same length\n",
        "    # sent_embs = [pad_or_truncate_embedding(emb, combined_embedding_dim) for emb in sent_embs]\n",
        "\n",
        "    # combined_features = np.array(sent_embs[:train_size] + word_embs_list + sent_embs[train_size:])\n",
        "\n",
        "    # features = preprocess_features(sp.csr_matrix(combined_features)).todense()\n",
        "    # features = torch.FloatTensor(features).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ind in tqdm(range(train_size + test_size)):\n",
        "    sent = tokenize_sentences[ind]\n",
        "    sentence_str = \" \".join(sent[:512])\n",
        "\n",
        "    # GPT-2 embeddings\n",
        "    gpt_inputs = gpt_tokenizer(sentence_str, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        gpt_outputs = gpt_model(**gpt_inputs)\n",
        "    gpt_sentence_embedding = gpt_outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    gpt_tokens = gpt_tokenizer.tokenize(sentence_str)\n",
        "    gpt_token_embeddings = gpt_outputs.last_hidden_state.squeeze().cpu().numpy()\n",
        "\n",
        "    # BERT embeddings\n",
        "    bert_inputs = bert_tokenizer(sentence_str, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        bert_outputs = bert_model(**bert_inputs)\n",
        "    bert_sentence_embedding = bert_outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    bert_tokens = bert_tokenizer.tokenize(sentence_str)\n",
        "    bert_token_embeddings = bert_outputs.last_hidden_state.squeeze().cpu().numpy()\n",
        "\n",
        "    # Debugging prints\n",
        "    # print(f\"gpt_embedding: {gpt_sentence_embedding}, type: {type(gpt_sentence_embedding)}, shape: {gpt_sentence_embedding.shape}\")\n",
        "    # print(f\"bert_embedding: {bert_sentence_embedding}, type: {type(bert_sentence_embedding)}, shape: {bert_sentence_embedding.shape}\")\n",
        "\n",
        "    # Combined sentence embedding\n",
        "    combined_sentence_embedding = np.concatenate((gpt_sentence_embedding, bert_sentence_embedding))\n",
        "    sent_embs.append(combined_sentence_embedding)\n",
        "\n",
        "    # Combined word embeddings\n",
        "    for gpt_token, gpt_embedding in zip(gpt_tokens, gpt_token_embeddings):\n",
        "        gpt_embedding = pad_or_truncate_embedding(gpt_embedding, gpt_embedding_dim)\n",
        "        if gpt_token not in word_embs:\n",
        "            word_embs[gpt_token] = gpt_embedding\n",
        "        else:\n",
        "            word_embs[gpt_token] = np.minimum(word_embs[gpt_token], gpt_embedding)\n",
        "\n",
        "    for bert_token, bert_embedding in zip(bert_tokens, bert_token_embeddings):\n",
        "        bert_embedding = pad_or_truncate_embedding(bert_embedding, bert_embedding_dim)\n",
        "        if bert_token not in word_embs:\n",
        "            word_embs[bert_token] = bert_embedding\n",
        "        else:\n",
        "            word_embs[bert_token] = np.minimum(word_embs[bert_token], bert_embedding)\n"
      ],
      "metadata": {
        "id": "4J1pJYAsgIKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77f4208-3d8d-4454-e308-27638235986d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1221/1221 [06:40<00:00,  3.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check the type and shape of gpt_embedding\n",
        "# for gpt_token, gpt_embedding in zip(gpt_tokens, gpt_token_embeddings):\n",
        "#     if isinstance(gpt_embedding, np.ndarray) and len(gpt_embedding.shape) > 0:\n",
        "#         gpt_embedding = pad_or_truncate_embedding(gpt_embedding, gpt_embedding_dim)\n",
        "#         if gpt_token not in word_embs:\n",
        "#             word_embs[gpt_token] = gpt_embedding\n",
        "#     else:\n",
        "#         print(f\"Unexpected gpt_embedding format for token: {gpt_token}, embedding: {gpt_embedding}\")\n"
      ],
      "metadata": {
        "id": "S9CCpBCWiD4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embs_list = [pad_or_truncate_embedding(word_embs[word], combined_embedding_dim) for word in word_list if word in word_embs]\n",
        "# Ensure all combined embeddings have the same length\n",
        "sent_embs = [pad_or_truncate_embedding(emb, combined_embedding_dim) for emb in sent_embs]\n",
        "\n",
        "combined_features = np.array(sent_embs[:train_size] + word_embs_list + sent_embs[train_size:])\n",
        "features = preprocess_features(sp.csr_matrix(combined_features)).todense()\n",
        "features = torch.FloatTensor(features).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xr1EVGu8sleu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for gpt_token, gpt_embedding in zip(gpt_tokens, gpt_token_embeddings):\n",
        "#     if isinstance(gpt_embedding, np.ndarray):\n",
        "#         if gpt_embedding.ndim == 1:  # This ensures you're getting a vector\n",
        "#             gpt_embedding = pad_or_truncate_embedding(gpt_embedding, gpt_embedding_dim)\n",
        "#             if gpt_token not in word_embs:\n",
        "#                 word_embs[gpt_token] = gpt_embedding\n",
        "#         else:\n",
        "#             print(f\"Warning: Token {gpt_token} has scalar embedding. Assigning default embedding.\")\n",
        "#             word_embs[gpt_token] = np.zeros(gpt_embedding_dim)  # Assign zero vector if embedding is scalar\n",
        "#     else:\n",
        "#         print(f\"Unexpected gpt_embedding type for token: {gpt_token}, embedding: {gpt_embedding}, type: {type(gpt_embedding)}\")\n"
      ],
      "metadata": {
        "id": "n8Jw8xS0iSiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for gpt_token, gpt_embedding in zip(gpt_tokens, gpt_token_embeddings):\n",
        "#     if isinstance(gpt_embedding, np.ndarray) and gpt_embedding.ndim == 1:\n",
        "#         gpt_embedding = pad_or_truncate_embedding(gpt_embedding, gpt_embedding_dim)\n",
        "#         if gpt_token not in word_embs:\n",
        "#             word_embs[gpt_token] = gpt_embedding\n",
        "#     else:\n",
        "#         # Assign a default embedding for scalar or unexpected values\n",
        "#         print(f\"Scalar or unexpected embedding for token {gpt_token}. Assigning zero vector.\")\n",
        "#         word_embs[gpt_token] = np.zeros(gpt_embedding_dim)\n"
      ],
      "metadata": {
        "id": "WhwDde7xiX9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for gpt_token, gpt_embedding in zip(gpt_tokens, gpt_token_embeddings):\n",
        "#     print(f\"Token: {gpt_token}, Embedding type: {type(gpt_embedding)}, Embedding shape: {getattr(gpt_embedding, 'shape', 'Scalar')}\")\n"
      ],
      "metadata": {
        "id": "vMtp3W36ih74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if len(gpt_tokens) == 0 or len(bert_tokens) == 0:\n",
        "#     print(f\"Empty tokens for sentence: {sentence_str}\")\n",
        "#     continue\n"
      ],
      "metadata": {
        "id": "_2VIwiM7_6vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KZ30nfRcnzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2fdc328-c0b3-4a7a-b8a6-abf1e38cebc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 0.6931 acc_train: 0.5389 loss_val: 0.6840 acc_val: 0.4815 time: 0.0673s\n",
            "Epoch: 0002 loss_train: 0.6737 acc_train: 0.5333 loss_val: 0.6618 acc_val: 0.4815 time: 0.0662s\n",
            "Epoch: 0003 loss_train: 0.6469 acc_train: 0.5333 loss_val: 0.6167 acc_val: 0.7284 time: 0.0759s\n",
            "Epoch: 0004 loss_train: 0.6071 acc_train: 0.7250 loss_val: 0.5637 acc_val: 0.7654 time: 0.0721s\n",
            "Epoch: 0005 loss_train: 0.5568 acc_train: 0.8014 loss_val: 0.5049 acc_val: 0.7901 time: 0.0646s\n",
            "Epoch: 0006 loss_train: 0.5000 acc_train: 0.8292 loss_val: 0.4436 acc_val: 0.8272 time: 0.0658s\n",
            "Epoch: 0007 loss_train: 0.4406 acc_train: 0.8556 loss_val: 0.3821 acc_val: 0.8642 time: 0.0627s\n",
            "Epoch: 0008 loss_train: 0.3845 acc_train: 0.8722 loss_val: 0.3306 acc_val: 0.9136 time: 0.0661s\n",
            "Epoch: 0009 loss_train: 0.3376 acc_train: 0.8819 loss_val: 0.2925 acc_val: 0.9136 time: 0.0629s\n",
            "Epoch: 0010 loss_train: 0.2942 acc_train: 0.8917 loss_val: 0.2679 acc_val: 0.9259 time: 0.0676s\n",
            "Epoch: 0011 loss_train: 0.2667 acc_train: 0.8917 loss_val: 0.2510 acc_val: 0.9136 time: 0.0858s\n",
            "Epoch: 0012 loss_train: 0.2421 acc_train: 0.8972 loss_val: 0.2392 acc_val: 0.9259 time: 0.1155s\n",
            "Epoch: 0013 loss_train: 0.2240 acc_train: 0.9083 loss_val: 0.2335 acc_val: 0.9259 time: 0.1073s\n",
            "Epoch: 0014 loss_train: 0.2093 acc_train: 0.9194 loss_val: 0.2328 acc_val: 0.9259 time: 0.1137s\n",
            "Epoch: 0015 loss_train: 0.1952 acc_train: 0.9236 loss_val: 0.2358 acc_val: 0.9136 time: 0.1544s\n",
            "Epoch: 0016 loss_train: 0.1847 acc_train: 0.9222 loss_val: 0.2347 acc_val: 0.9136 time: 0.1886s\n",
            "Epoch: 0017 loss_train: 0.1773 acc_train: 0.9333 loss_val: 0.2344 acc_val: 0.9136 time: 0.1290s\n",
            "Epoch: 0018 loss_train: 0.1632 acc_train: 0.9375 loss_val: 0.2392 acc_val: 0.9136 time: 0.1016s\n",
            "Epoch: 0019 loss_train: 0.1546 acc_train: 0.9472 loss_val: 0.2449 acc_val: 0.9136 time: 0.1276s\n",
            "Epoch: 0020 loss_train: 0.1441 acc_train: 0.9458 loss_val: 0.2501 acc_val: 0.9259 time: 0.0847s\n",
            "Epoch: 0021 loss_train: 0.1340 acc_train: 0.9528 loss_val: 0.2558 acc_val: 0.9136 time: 0.1028s\n",
            "Epoch: 0022 loss_train: 0.1230 acc_train: 0.9500 loss_val: 0.2624 acc_val: 0.9136 time: 0.1281s\n",
            "Epoch: 0023 loss_train: 0.1221 acc_train: 0.9569 loss_val: 0.2662 acc_val: 0.9259 time: 0.1356s\n",
            "Epoch: 0024 loss_train: 0.1108 acc_train: 0.9583 loss_val: 0.2773 acc_val: 0.9136 time: 0.1623s\n",
            "Epoch: 0025 loss_train: 0.1076 acc_train: 0.9639 loss_val: 0.2949 acc_val: 0.9012 time: 0.1433s\n",
            "Epoch: 0026 loss_train: 0.1022 acc_train: 0.9625 loss_val: 0.2930 acc_val: 0.9136 time: 0.1930s\n",
            "Epoch: 0027 loss_train: 0.0884 acc_train: 0.9653 loss_val: 0.2914 acc_val: 0.9136 time: 0.1873s\n",
            "Epoch: 0028 loss_train: 0.0902 acc_train: 0.9653 loss_val: 0.2960 acc_val: 0.9136 time: 0.1300s\n",
            "Epoch: 0029 loss_train: 0.0833 acc_train: 0.9681 loss_val: 0.3119 acc_val: 0.9136 time: 0.0960s\n",
            "Epoch: 0030 loss_train: 0.0753 acc_train: 0.9736 loss_val: 0.3114 acc_val: 0.9136 time: 0.1262s\n",
            "Epoch: 0031 loss_train: 0.0716 acc_train: 0.9806 loss_val: 0.3106 acc_val: 0.9136 time: 0.1770s\n",
            "Epoch: 0032 loss_train: 0.0673 acc_train: 0.9806 loss_val: 0.3126 acc_val: 0.9136 time: 0.1371s\n",
            "Epoch: 0033 loss_train: 0.0669 acc_train: 0.9764 loss_val: 0.3217 acc_val: 0.9136 time: 0.1394s\n",
            "Epoch: 0034 loss_train: 0.0624 acc_train: 0.9764 loss_val: 0.3143 acc_val: 0.9136 time: 0.1456s\n",
            "Epoch: 0035 loss_train: 0.0611 acc_train: 0.9736 loss_val: 0.3021 acc_val: 0.9259 time: 0.1282s\n",
            "Epoch: 0036 loss_train: 0.0596 acc_train: 0.9819 loss_val: 0.3065 acc_val: 0.9259 time: 0.1092s\n",
            "Epoch: 0037 loss_train: 0.0551 acc_train: 0.9764 loss_val: 0.3376 acc_val: 0.9012 time: 0.1297s\n",
            "Epoch: 0038 loss_train: 0.0553 acc_train: 0.9736 loss_val: 0.3161 acc_val: 0.9136 time: 0.2129s\n",
            "Epoch: 0039 loss_train: 0.0457 acc_train: 0.9917 loss_val: 0.2936 acc_val: 0.9136 time: 0.2244s\n",
            "Epoch: 0040 loss_train: 0.0459 acc_train: 0.9875 loss_val: 0.2928 acc_val: 0.9136 time: 0.2509s\n",
            "Epoch: 0041 loss_train: 0.0501 acc_train: 0.9847 loss_val: 0.3095 acc_val: 0.9259 time: 0.2360s\n",
            "Epoch: 0042 loss_train: 0.0402 acc_train: 0.9889 loss_val: 0.3286 acc_val: 0.9012 time: 0.1778s\n",
            "Epoch: 0043 loss_train: 0.0420 acc_train: 0.9861 loss_val: 0.3124 acc_val: 0.9259 time: 0.3385s\n",
            "Epoch: 0044 loss_train: 0.0354 acc_train: 0.9931 loss_val: 0.3036 acc_val: 0.9259 time: 0.2919s\n",
            "Epoch: 0045 loss_train: 0.0319 acc_train: 0.9944 loss_val: 0.3036 acc_val: 0.9136 time: 0.2078s\n",
            "Epoch: 0046 loss_train: 0.0348 acc_train: 0.9917 loss_val: 0.3141 acc_val: 0.9259 time: 0.3518s\n",
            "Epoch: 0047 loss_train: 0.0275 acc_train: 0.9986 loss_val: 0.3313 acc_val: 0.9136 time: 0.2968s\n",
            "Epoch: 0048 loss_train: 0.0324 acc_train: 0.9931 loss_val: 0.3357 acc_val: 0.9136 time: 0.3987s\n",
            "Epoch: 0049 loss_train: 0.0288 acc_train: 0.9917 loss_val: 0.3263 acc_val: 0.9259 time: 0.2368s\n",
            "Epoch: 0050 loss_train: 0.0248 acc_train: 0.9958 loss_val: 0.3246 acc_val: 0.9259 time: 0.3346s\n",
            "Epoch: 0051 loss_train: 0.0289 acc_train: 0.9944 loss_val: 0.3310 acc_val: 0.9259 time: 0.2313s\n",
            "Epoch: 0052 loss_train: 0.0240 acc_train: 0.9986 loss_val: 0.3400 acc_val: 0.9136 time: 0.1290s\n",
            "Epoch: 0053 loss_train: 0.0231 acc_train: 0.9958 loss_val: 0.3519 acc_val: 0.9136 time: 0.1836s\n",
            "Epoch: 0054 loss_train: 0.0254 acc_train: 0.9958 loss_val: 0.3473 acc_val: 0.9136 time: 0.1641s\n",
            "Epoch: 0055 loss_train: 0.0231 acc_train: 0.9972 loss_val: 0.3349 acc_val: 0.9259 time: 0.2644s\n",
            "Epoch: 0056 loss_train: 0.0203 acc_train: 1.0000 loss_val: 0.3332 acc_val: 0.9136 time: 0.2245s\n",
            "Epoch: 0057 loss_train: 0.0245 acc_train: 0.9931 loss_val: 0.3386 acc_val: 0.9136 time: 0.2481s\n",
            "Epoch: 0058 loss_train: 0.0200 acc_train: 1.0000 loss_val: 0.3479 acc_val: 0.9136 time: 0.2520s\n",
            "Epoch: 0059 loss_train: 0.0198 acc_train: 0.9986 loss_val: 0.3505 acc_val: 0.9136 time: 0.2660s\n",
            "Epoch: 0060 loss_train: 0.0196 acc_train: 0.9972 loss_val: 0.3409 acc_val: 0.9136 time: 0.1906s\n",
            "Epoch: 0061 loss_train: 0.0175 acc_train: 1.0000 loss_val: 0.3317 acc_val: 0.9012 time: 0.1532s\n",
            "Epoch: 0062 loss_train: 0.0161 acc_train: 1.0000 loss_val: 0.3291 acc_val: 0.9012 time: 0.1481s\n",
            "Epoch: 0063 loss_train: 0.0179 acc_train: 1.0000 loss_val: 0.3306 acc_val: 0.9012 time: 0.2212s\n",
            "Epoch: 0064 loss_train: 0.0163 acc_train: 0.9986 loss_val: 0.3374 acc_val: 0.9012 time: 0.1477s\n",
            "Epoch: 0065 loss_train: 0.0164 acc_train: 1.0000 loss_val: 0.3410 acc_val: 0.9012 time: 0.0964s\n",
            "Epoch: 0066 loss_train: 0.0150 acc_train: 1.0000 loss_val: 0.3413 acc_val: 0.9012 time: 0.0928s\n",
            "Epoch: 0067 loss_train: 0.0161 acc_train: 1.0000 loss_val: 0.3376 acc_val: 0.9012 time: 0.0889s\n",
            "Epoch: 0068 loss_train: 0.0127 acc_train: 1.0000 loss_val: 0.3368 acc_val: 0.9012 time: 0.0910s\n",
            "Epoch: 0069 loss_train: 0.0141 acc_train: 1.0000 loss_val: 0.3424 acc_val: 0.9012 time: 0.0872s\n",
            "Epoch: 0070 loss_train: 0.0130 acc_train: 1.0000 loss_val: 0.3457 acc_val: 0.9012 time: 0.1002s\n",
            "Epoch: 0071 loss_train: 0.0115 acc_train: 1.0000 loss_val: 0.3457 acc_val: 0.9012 time: 0.1043s\n",
            "Epoch: 0072 loss_train: 0.0113 acc_train: 1.0000 loss_val: 0.3440 acc_val: 0.9012 time: 0.1266s\n",
            "Epoch: 0073 loss_train: 0.0115 acc_train: 0.9986 loss_val: 0.3445 acc_val: 0.9012 time: 0.1121s\n",
            "Epoch: 0074 loss_train: 0.0116 acc_train: 0.9986 loss_val: 0.3470 acc_val: 0.9012 time: 0.0932s\n",
            "Epoch: 0075 loss_train: 0.0113 acc_train: 1.0000 loss_val: 0.3520 acc_val: 0.9012 time: 0.1010s\n",
            "Epoch: 0076 loss_train: 0.0103 acc_train: 1.0000 loss_val: 0.3599 acc_val: 0.9012 time: 0.0929s\n",
            "Epoch: 0077 loss_train: 0.0109 acc_train: 1.0000 loss_val: 0.3594 acc_val: 0.9012 time: 0.0910s\n",
            "Epoch: 0078 loss_train: 0.0095 acc_train: 1.0000 loss_val: 0.3558 acc_val: 0.9012 time: 0.0959s\n",
            "Epoch: 0079 loss_train: 0.0094 acc_train: 1.0000 loss_val: 0.3530 acc_val: 0.9012 time: 0.2032s\n",
            "Epoch: 0080 loss_train: 0.0096 acc_train: 1.0000 loss_val: 0.3526 acc_val: 0.9012 time: 0.0926s\n",
            "Epoch: 0081 loss_train: 0.0101 acc_train: 1.0000 loss_val: 0.3550 acc_val: 0.9012 time: 0.1123s\n",
            "Epoch: 0082 loss_train: 0.0091 acc_train: 1.0000 loss_val: 0.3605 acc_val: 0.9012 time: 0.0911s\n",
            "Epoch: 0083 loss_train: 0.0094 acc_train: 1.0000 loss_val: 0.3610 acc_val: 0.9012 time: 0.1317s\n",
            "Epoch: 0084 loss_train: 0.0091 acc_train: 1.0000 loss_val: 0.3600 acc_val: 0.9012 time: 0.0845s\n",
            "Epoch: 0085 loss_train: 0.0098 acc_train: 1.0000 loss_val: 0.3564 acc_val: 0.9012 time: 0.1035s\n",
            "Epoch: 0086 loss_train: 0.0080 acc_train: 1.0000 loss_val: 0.3578 acc_val: 0.9012 time: 0.1006s\n",
            "Epoch: 0087 loss_train: 0.0093 acc_train: 1.0000 loss_val: 0.3587 acc_val: 0.8889 time: 0.0970s\n",
            "Epoch: 0088 loss_train: 0.0102 acc_train: 1.0000 loss_val: 0.3618 acc_val: 0.9012 time: 0.1956s\n",
            "Epoch: 0089 loss_train: 0.0079 acc_train: 1.0000 loss_val: 0.3752 acc_val: 0.9012 time: 0.1206s\n",
            "Epoch: 0090 loss_train: 0.0085 acc_train: 1.0000 loss_val: 0.3918 acc_val: 0.9012 time: 0.1155s\n",
            "Epoch: 0091 loss_train: 0.0090 acc_train: 1.0000 loss_val: 0.3925 acc_val: 0.9012 time: 0.1243s\n",
            "Epoch: 0092 loss_train: 0.0093 acc_train: 0.9986 loss_val: 0.3786 acc_val: 0.9012 time: 0.1054s\n",
            "Epoch: 0093 loss_train: 0.0077 acc_train: 1.0000 loss_val: 0.3703 acc_val: 0.9012 time: 0.1245s\n",
            "Epoch: 0094 loss_train: 0.0075 acc_train: 1.0000 loss_val: 0.3707 acc_val: 0.9012 time: 0.1022s\n",
            "Epoch: 0095 loss_train: 0.0092 acc_train: 1.0000 loss_val: 0.3721 acc_val: 0.9012 time: 0.1153s\n",
            "Epoch: 0096 loss_train: 0.0087 acc_train: 1.0000 loss_val: 0.3824 acc_val: 0.9012 time: 0.0989s\n",
            "Epoch: 0097 loss_train: 0.0066 acc_train: 1.0000 loss_val: 0.3955 acc_val: 0.9012 time: 0.1056s\n",
            "Epoch: 0098 loss_train: 0.0076 acc_train: 1.0000 loss_val: 0.3992 acc_val: 0.9012 time: 0.0894s\n",
            "Epoch: 0099 loss_train: 0.0107 acc_train: 0.9986 loss_val: 0.3816 acc_val: 0.9012 time: 0.1175s\n",
            "Epoch: 0100 loss_train: 0.0065 acc_train: 1.0000 loss_val: 0.3756 acc_val: 0.9136 time: 0.1314s\n",
            "Epoch: 0101 loss_train: 0.0058 acc_train: 1.0000 loss_val: 0.3759 acc_val: 0.9012 time: 0.1236s\n",
            "Epoch: 0102 loss_train: 0.0082 acc_train: 1.0000 loss_val: 0.3768 acc_val: 0.9136 time: 0.1068s\n",
            "Epoch: 0103 loss_train: 0.0057 acc_train: 1.0000 loss_val: 0.3806 acc_val: 0.9136 time: 0.2178s\n",
            "Epoch: 0104 loss_train: 0.0058 acc_train: 1.0000 loss_val: 0.3861 acc_val: 0.9012 time: 0.1339s\n",
            "Epoch: 0105 loss_train: 0.0068 acc_train: 0.9986 loss_val: 0.3900 acc_val: 0.9012 time: 0.1055s\n",
            "Epoch: 0106 loss_train: 0.0065 acc_train: 1.0000 loss_val: 0.3892 acc_val: 0.9012 time: 0.0964s\n",
            "Epoch: 0107 loss_train: 0.0062 acc_train: 1.0000 loss_val: 0.3856 acc_val: 0.9136 time: 0.1015s\n",
            "Epoch: 0108 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.3829 acc_val: 0.9136 time: 0.0940s\n",
            "Epoch: 0109 loss_train: 0.0050 acc_train: 1.0000 loss_val: 0.3822 acc_val: 0.9136 time: 0.0900s\n",
            "Epoch: 0110 loss_train: 0.0047 acc_train: 1.0000 loss_val: 0.3836 acc_val: 0.9012 time: 0.0989s\n",
            "Epoch: 0111 loss_train: 0.0060 acc_train: 1.0000 loss_val: 0.3836 acc_val: 0.9012 time: 0.0873s\n",
            "Epoch: 0112 loss_train: 0.0063 acc_train: 1.0000 loss_val: 0.3856 acc_val: 0.9136 time: 0.0937s\n",
            "Epoch: 0113 loss_train: 0.0051 acc_train: 1.0000 loss_val: 0.3929 acc_val: 0.9012 time: 0.0923s\n",
            "Epoch: 0114 loss_train: 0.0054 acc_train: 1.0000 loss_val: 0.4004 acc_val: 0.9012 time: 0.0843s\n",
            "Epoch: 0115 loss_train: 0.0071 acc_train: 1.0000 loss_val: 0.3969 acc_val: 0.9012 time: 0.0854s\n",
            "Epoch: 0116 loss_train: 0.0048 acc_train: 1.0000 loss_val: 0.3950 acc_val: 0.9136 time: 0.0881s\n",
            "Epoch: 0117 loss_train: 0.0048 acc_train: 1.0000 loss_val: 0.3948 acc_val: 0.9136 time: 0.0937s\n",
            "Epoch: 0118 loss_train: 0.0046 acc_train: 1.0000 loss_val: 0.3960 acc_val: 0.9012 time: 0.0841s\n",
            "Epoch: 0119 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.3989 acc_val: 0.9012 time: 0.0948s\n",
            "Epoch: 0120 loss_train: 0.0078 acc_train: 1.0000 loss_val: 0.4008 acc_val: 0.9136 time: 0.1045s\n",
            "Epoch: 0121 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.4088 acc_val: 0.9136 time: 0.1005s\n",
            "Epoch: 0122 loss_train: 0.0045 acc_train: 1.0000 loss_val: 0.4210 acc_val: 0.9012 time: 0.1228s\n",
            "Epoch: 0123 loss_train: 0.0055 acc_train: 1.0000 loss_val: 0.4231 acc_val: 0.9012 time: 0.1313s\n",
            "Epoch: 0124 loss_train: 0.0053 acc_train: 1.0000 loss_val: 0.4166 acc_val: 0.9136 time: 0.1109s\n",
            "Epoch: 0125 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.4138 acc_val: 0.9136 time: 0.1264s\n",
            "Epoch: 0126 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.4148 acc_val: 0.9136 time: 0.1032s\n",
            "Epoch: 0127 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.4168 acc_val: 0.9012 time: 0.1042s\n",
            "Epoch: 0128 loss_train: 0.0044 acc_train: 1.0000 loss_val: 0.4179 acc_val: 0.9136 time: 0.0951s\n",
            "Epoch: 0129 loss_train: 0.0037 acc_train: 1.0000 loss_val: 0.4195 acc_val: 0.9136 time: 0.1253s\n",
            "Epoch: 0130 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.4229 acc_val: 0.9136 time: 0.1065s\n",
            "Epoch: 0131 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.4267 acc_val: 0.9136 time: 0.1311s\n",
            "Epoch: 0132 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.4270 acc_val: 0.9136 time: 0.0864s\n",
            "Epoch: 0133 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.4258 acc_val: 0.9136 time: 0.1576s\n",
            "Epoch: 0134 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.4245 acc_val: 0.9136 time: 0.1544s\n",
            "Epoch: 0135 loss_train: 0.0035 acc_train: 1.0000 loss_val: 0.4234 acc_val: 0.9136 time: 0.1769s\n",
            "Epoch: 0136 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.4229 acc_val: 0.9136 time: 0.1528s\n",
            "Epoch: 0137 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.4230 acc_val: 0.9136 time: 0.0937s\n",
            "Epoch: 0138 loss_train: 0.0041 acc_train: 1.0000 loss_val: 0.4240 acc_val: 0.9136 time: 0.1500s\n",
            "Epoch: 0139 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.4279 acc_val: 0.9136 time: 0.0902s\n",
            "Epoch: 0140 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.4328 acc_val: 0.9136 time: 0.0980s\n",
            "Epoch: 0141 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.4367 acc_val: 0.9136 time: 0.1018s\n",
            "Epoch: 0142 loss_train: 0.0034 acc_train: 1.0000 loss_val: 0.4368 acc_val: 0.9136 time: 0.1740s\n",
            "Epoch: 0143 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.4341 acc_val: 0.9136 time: 0.2248s\n",
            "Epoch: 0144 loss_train: 0.0033 acc_train: 1.0000 loss_val: 0.4304 acc_val: 0.9136 time: 0.1242s\n",
            "Epoch: 0145 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.4290 acc_val: 0.9136 time: 0.2282s\n",
            "Epoch: 0146 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.4298 acc_val: 0.9136 time: 0.1586s\n",
            "Epoch: 0147 loss_train: 0.0042 acc_train: 1.0000 loss_val: 0.4304 acc_val: 0.9136 time: 0.2125s\n",
            "Epoch: 0148 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.4323 acc_val: 0.9136 time: 0.1905s\n",
            "Epoch: 0149 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.4360 acc_val: 0.9136 time: 0.1939s\n",
            "Epoch: 0150 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.4423 acc_val: 0.9136 time: 0.2382s\n",
            "Epoch: 0151 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.4467 acc_val: 0.9136 time: 0.2513s\n",
            "Epoch: 0152 loss_train: 0.0043 acc_train: 1.0000 loss_val: 0.4410 acc_val: 0.9136 time: 0.1561s\n",
            "Epoch: 0153 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.4356 acc_val: 0.9136 time: 0.2306s\n",
            "Epoch: 0154 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.4340 acc_val: 0.9136 time: 0.2408s\n",
            "Epoch: 0155 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.4355 acc_val: 0.9136 time: 0.2794s\n",
            "Epoch: 0156 loss_train: 0.0029 acc_train: 1.0000 loss_val: 0.4361 acc_val: 0.9136 time: 0.2834s\n",
            "Epoch: 0157 loss_train: 0.0030 acc_train: 1.0000 loss_val: 0.4356 acc_val: 0.9136 time: 0.2560s\n",
            "Epoch: 0158 loss_train: 0.0031 acc_train: 1.0000 loss_val: 0.4389 acc_val: 0.9136 time: 0.2614s\n",
            "Epoch: 0159 loss_train: 0.0026 acc_train: 1.0000 loss_val: 0.4462 acc_val: 0.9136 time: 0.2203s\n",
            "Epoch: 0160 loss_train: 0.0028 acc_train: 1.0000 loss_val: 0.4513 acc_val: 0.9136 time: 0.3210s\n",
            "Epoch: 0161 loss_train: 0.0036 acc_train: 1.0000 loss_val: 0.4477 acc_val: 0.9136 time: 0.2665s\n",
            "Epoch: 0162 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.4433 acc_val: 0.9136 time: 0.2707s\n",
            "Epoch: 0163 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.4429 acc_val: 0.9136 time: 0.3453s\n",
            "Epoch: 0164 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.4454 acc_val: 0.9136 time: 0.1417s\n",
            "Epoch: 0165 loss_train: 0.0038 acc_train: 1.0000 loss_val: 0.4462 acc_val: 0.9136 time: 0.1469s\n",
            "Epoch: 0166 loss_train: 0.0027 acc_train: 1.0000 loss_val: 0.4484 acc_val: 0.9136 time: 0.2457s\n",
            "Epoch: 0167 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.4523 acc_val: 0.9136 time: 0.2552s\n",
            "Epoch: 0168 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.4574 acc_val: 0.9136 time: 0.2433s\n",
            "Epoch: 0169 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.4599 acc_val: 0.9136 time: 0.3671s\n",
            "Epoch: 0170 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.4602 acc_val: 0.9136 time: 0.2685s\n",
            "Epoch: 0171 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.4589 acc_val: 0.9136 time: 0.1211s\n",
            "Epoch: 0172 loss_train: 0.0024 acc_train: 1.0000 loss_val: 0.4585 acc_val: 0.9136 time: 0.1640s\n",
            "Epoch: 0173 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.4594 acc_val: 0.9136 time: 0.2128s\n",
            "Epoch: 0174 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.4611 acc_val: 0.9136 time: 0.1787s\n",
            "Epoch: 0175 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.4631 acc_val: 0.9136 time: 0.2075s\n",
            "Epoch: 0176 loss_train: 0.0023 acc_train: 1.0000 loss_val: 0.4663 acc_val: 0.9136 time: 0.2638s\n",
            "Epoch: 0177 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.4700 acc_val: 0.9136 time: 0.2880s\n",
            "Epoch: 0178 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.4723 acc_val: 0.9136 time: 0.1292s\n",
            "Epoch: 0179 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.4736 acc_val: 0.9136 time: 0.2386s\n",
            "Epoch: 0180 loss_train: 0.0025 acc_train: 1.0000 loss_val: 0.4721 acc_val: 0.9136 time: 0.3033s\n",
            "Epoch: 0181 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.4710 acc_val: 0.9136 time: 0.1693s\n",
            "Epoch: 0182 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.4709 acc_val: 0.9136 time: 0.1026s\n",
            "Epoch: 0183 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.4712 acc_val: 0.9136 time: 0.0844s\n",
            "Epoch: 0184 loss_train: 0.0032 acc_train: 1.0000 loss_val: 0.4714 acc_val: 0.9136 time: 0.0841s\n",
            "Epoch: 0185 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.4727 acc_val: 0.9136 time: 0.0953s\n",
            "Epoch: 0186 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.4745 acc_val: 0.9136 time: 0.0810s\n",
            "Epoch: 0187 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.4758 acc_val: 0.9136 time: 0.0891s\n",
            "Epoch: 0188 loss_train: 0.0021 acc_train: 1.0000 loss_val: 0.4742 acc_val: 0.9136 time: 0.0847s\n",
            "Epoch: 0189 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.4729 acc_val: 0.9136 time: 0.0959s\n",
            "Epoch: 0190 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.4729 acc_val: 0.9136 time: 0.1341s\n",
            "Epoch: 0191 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.4740 acc_val: 0.9136 time: 0.0905s\n",
            "Epoch: 0192 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.4758 acc_val: 0.9136 time: 0.1649s\n",
            "Epoch: 0193 loss_train: 0.0020 acc_train: 1.0000 loss_val: 0.4774 acc_val: 0.9136 time: 0.1145s\n",
            "Epoch: 0194 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.4779 acc_val: 0.9136 time: 0.0898s\n",
            "Epoch: 0195 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.4770 acc_val: 0.9136 time: 0.1037s\n",
            "Epoch: 0196 loss_train: 0.0022 acc_train: 1.0000 loss_val: 0.4761 acc_val: 0.9136 time: 0.1001s\n",
            "Epoch: 0197 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.4739 acc_val: 0.9136 time: 0.1181s\n",
            "Epoch: 0198 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.4739 acc_val: 0.9136 time: 0.1014s\n",
            "Epoch: 0199 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.4741 acc_val: 0.9136 time: 0.1132s\n",
            "Epoch: 0200 loss_train: 0.0017 acc_train: 1.0000 loss_val: 0.4742 acc_val: 0.9136 time: 0.1599s\n",
            "(0.898876404494382, 0.8986717267552182, 0.8989275739291729)\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "\n",
        "\n",
        "## GCN Layer\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features,  drop_out = 0, activation=None, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.zeros(1, out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters(in_features, out_features)\n",
        "        self.dropout = torch.nn.Dropout(drop_out)\n",
        "        self.activation =  activation\n",
        "\n",
        "    def reset_parameters(self,in_features, out_features):\n",
        "        stdv = np.sqrt(6.0/(in_features+out_features))\n",
        "        # stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        # if self.bias is not None:\n",
        "        #     torch.nn.init.zeros_(self.bias)\n",
        "            # self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "    def forward(self, input, adj, feature_less = False):\n",
        "        if feature_less:\n",
        "            support = self.weight\n",
        "            support = self.dropout(support)\n",
        "        else:\n",
        "            input = self.dropout(input)\n",
        "            support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "## GCN Model\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, n_layers = 2):\n",
        "        super(GCN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.gc_list = []\n",
        "        if n_layers >= 2:\n",
        "            self.gc1 = GraphConvolution(nfeat, nhid, dropout, activation = nn.ReLU())\n",
        "            self.gc_list = nn.ModuleList([GraphConvolution(nhid, nhid, dropout, activation = nn.ReLU()) for _ in range(self.n_layers-2)])\n",
        "            self.gcf = GraphConvolution(nhid, nclass, dropout)\n",
        "        else:\n",
        "            self.gc1 = GraphConvolution(nfeat, nclass, dropout)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        if self.n_layers>=2:\n",
        "            x = self.gc1(x, adj, feature_less = True)\n",
        "            for i in range(self.n_layers-2):\n",
        "                x = self.gc_list[i](x,adj)\n",
        "            x = self.gcf(x,adj)\n",
        "        else:\n",
        "            x = self.gc1(x, adj, feature_less = True)\n",
        "        return x\n",
        "\n",
        "\n",
        "def cal_accuracy(predictions,labels):\n",
        "    pred = torch.argmax(predictions,-1).cpu().tolist()\n",
        "    lab = labels.cpu().tolist()\n",
        "    cor = 0\n",
        "    for i in range(len(pred)):\n",
        "        if pred[i] == lab[i]:\n",
        "            cor += 1\n",
        "    return cor/len(pred)\n",
        "\n",
        "\n",
        "# Training\n",
        "\n",
        "\n",
        "## Initialize model\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "## Training and Validating\n",
        "\n",
        "\n",
        "def generate_train_val(train_pro=0.9):\n",
        "    real_train_size = int(train_pro*train_size)\n",
        "    val_size = train_size-real_train_size\n",
        "\n",
        "    idx_train = np.random.choice(train_size, real_train_size,replace=False)\n",
        "    idx_train.sort()\n",
        "    idx_val = []\n",
        "    pointer = 0\n",
        "    for v in range(train_size):\n",
        "        if pointer<len(idx_train) and idx_train[pointer] == v:\n",
        "            pointer +=1\n",
        "        else:\n",
        "            idx_val.append(v)\n",
        "    idx_test = range(train_size+vocab_length, node_size)\n",
        "    return idx_train, idx_val, idx_test\n",
        "\n",
        "idx_train, idx_val, idx_test = generate_train_val()\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def train_model(show_result = True):\n",
        "    val_loss = []\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output= model(features, adj)\n",
        "        loss_train = criterion(output[idx_train], labels[idx_train])\n",
        "        acc_train = cal_accuracy(output[idx_train], labels[idx_train])\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "        loss_val = criterion(output[idx_val], labels[idx_val])\n",
        "        val_loss.append(loss_val.item())\n",
        "        acc_val = cal_accuracy(output[idx_val], labels[idx_val])\n",
        "        if show_result:\n",
        "            print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "                    'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "                    'acc_train: {:.4f}'.format(acc_train),\n",
        "                    'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "                    'acc_val: {:.4f}'.format(acc_val),\n",
        "                    'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "        # if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
        "        #     if show_result:\n",
        "        #         print(\"Early Stopping...\")\n",
        "        #     break\n",
        "\n",
        "train_model()\n",
        "\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    predictions = torch.argmax(output[idx_test],-1).cpu().tolist()\n",
        "    acc = accuracy_score(test_labels,predictions)\n",
        "    f11 = f1_score(test_labels,predictions, average='macro')\n",
        "    f12 = f1_score(test_labels,predictions, average = 'weighted')\n",
        "    return acc, f11, f12\n",
        "\n",
        "print(test())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQpcw3SuSG3E"
      },
      "outputs": [],
      "source": [
        "# len(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKC4coLQcwNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90cf7d2a-94d2-4ff3-a7ae-5b8e630683ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8979\n",
            "Macro F1: 0.8974\n",
            "Weighted F1: 0.8978\n"
          ]
        }
      ],
      "source": [
        "# Test 10 times\n",
        "\n",
        "\n",
        "test_acc_list = []\n",
        "test_f11_list = []\n",
        "test_f12_list = []\n",
        "\n",
        "for t in range(11):\n",
        "    model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    idx_train, idx_val, idx_test = generate_train_val()\n",
        "    train_model(show_result=False)\n",
        "    acc, f11, f12 = test()\n",
        "    test_acc_list.append(acc)\n",
        "    test_f11_list.append(f11)\n",
        "    test_f12_list.append(f12)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\",np.round(np.mean(test_acc_list),4))\n",
        "print(\"Macro F1:\",np.round(np.mean(test_f11_list),4))\n",
        "print(\"Weighted F1:\",np.round(np.mean(test_f12_list),4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlk_iJEFSGzW"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import f1_score, accuracy_score\n",
        "# import pandas as pd\n",
        "\n",
        "# def test_and_save_results():\n",
        "#     model.eval()\n",
        "#     output = model(features, adj)\n",
        "#     predictions = torch.argmax(output[idx_test], -1).cpu().tolist()\n",
        "#     acc = accuracy_score(test_labels, predictions)\n",
        "#     f11 = f1_score(test_labels, predictions, average='macro')\n",
        "#     f12 = f1_score(test_labels, predictions, average='weighted')\n",
        "\n",
        "#     # Convert predictions and labels to a DataFrame\n",
        "#     results_df = pd.DataFrame({\n",
        "#         'Text': X_test.tolist(),\n",
        "#     'True Label': test_labels,\n",
        "#     'Predicted Label': predictions\n",
        "#     })\n",
        "\n",
        "#     # Save the DataFrame to an Excel file\n",
        "#     results_df.to_excel(\"/content/gdrive/MyDrive/dualemb.xlsx\", index=False)\n",
        "\n",
        "#     return acc, f11, f12\n",
        "\n",
        "# # Call the function\n",
        "# accuracy, f1_macro, f1_weighted = test_and_save_results()\n",
        "# print(f\"Accuracy: {accuracy}, F1-Macro: {f1_macro}, F1-Weighted: {f1_weighted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4AE-g1HU3oG"
      },
      "outputs": [],
      "source": [
        "# !pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWE8RTHiTTKl"
      },
      "outputs": [],
      "source": [
        "# #accuracy and loss graph\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Test 10 times\n",
        "# test_acc_list = []\n",
        "# train_loss_list = []\n",
        "\n",
        "# for t in range(10):\n",
        "#     model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT, n_layers=NUM_LAYERS).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "#     idx_train, idx_val, idx_test = generate_train_val()\n",
        "\n",
        "#     train_loss_per_run = []\n",
        "#     for epoch in range(NUM_EPOCHS):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(features, adj)\n",
        "#         loss_train = criterion(output[idx_train], labels[idx_train])\n",
        "#         loss_train.backward()\n",
        "#         optimizer.step()\n",
        "#         train_loss_per_run.append(loss_train.item())\n",
        "\n",
        "#     acc, _, _ = test()\n",
        "#     test_acc_list.append(acc)\n",
        "#     train_loss_list.append(train_loss_per_run)\n",
        "\n",
        "# # Plotting\n",
        "# plt.figure(figsize=(12, 5))\n",
        "\n",
        "# # Accuracy plot\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(np.arange(1, 11), test_acc_list, marker='o', linestyle='-')\n",
        "# plt.title('Accuracy over 10 runs')\n",
        "# plt.xlabel('Run')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.xticks(np.arange(1, 11))\n",
        "# plt.grid(True)\n",
        "\n",
        "# # Loss plot\n",
        "# plt.subplot(1, 2, 2)\n",
        "# for i, loss_vals in enumerate(train_loss_list):\n",
        "#     plt.plot(np.arange(1, NUM_EPOCHS + 1), loss_vals, label=f'Run {i+1}')\n",
        "# plt.title('Training Loss over Epochs')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v84cN7QSGt4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3zwoNYlSGqx"
      },
      "outputs": [],
      "source": [
        "# # only for twitter data\n",
        "# traindf = pd.read_excel('/content/gdrive/MyDrive/twitter15xlsl.xlsx')\n",
        "# #traindf = pd.read_excel('/content/gdrive/MyDrive/twitter16.xlsx')\n",
        "# # traindf = pd.read_excel('/content/gdrive/MyDrive/charlihbdo.xlsx')\n",
        "# X = traindf['Text']\n",
        "# #Y = traindf.Label\n",
        "# Y = traindf['Label']\n",
        "# print(type(Y))\n",
        "# #print(Y)\n",
        "# #SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
        "\n",
        "# # Input: \"reviewText\", \"rating\" and \"time\"\n",
        "# # Target: \"log_votes\"\n",
        "# X_train, X_test, y_train, y_test = train_test_split(traindf[\"Text\"],\n",
        "#                                                   traindf[\"Label\"],\n",
        "#                                                   test_size=0.3,\n",
        "#                                                   shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(traindf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C9UEuMHqQsP",
        "outputId": "176772dc-e9eb-4cdc-850e-e781f2323cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "890"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train=X_train.values.tolist()\n",
        "# test=X_test.values.tolist()\n",
        "# trainl=y_train.values.tolist()\n",
        "# testl=y_test.values.tolist()\n",
        "# original_train_sentences = train\n",
        "# original_labels_train = trainl\n",
        "# original_test_sentences = test\n",
        "# original_labels_test =  testl\n",
        "# train_size = len(original_train_sentences)\n",
        "# test_size = len(original_test_sentences)\n",
        "# sentences = original_train_sentences + original_test_sentences\n",
        "# print(len(original_train_sentences))\n",
        "# print(len(original_test_sentences))"
      ],
      "metadata": {
        "id": "HzL14mR0rHyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # testl=y_test.values.tolist()\n",
        "# # original_train_sentences = train\n",
        "# # original_labels_train = trainl\n",
        "# # original_test_sentences = test\n",
        "\n",
        "# EDGE = 2 # 0:d2w 1:d2w+w2w 2:d2w+w2w+d2d\n",
        "# NODE = 1 # 0:one-hot #1:BERT\n",
        "# NUM_LAYERS = 2\n",
        "\n",
        "# HIDDEN_DIM = 200\n",
        "# DROP_OUT = 0.5\n",
        "# LR = 0.02\n",
        "# WEIGHT_DECAY = 0\n",
        "# EARLY_STOPPING = 10\n",
        "# NUM_EPOCHS = 200\n",
        "\n",
        "\n",
        "# # Preprocess\n",
        "\n",
        "\n",
        "# ## Label Encoding\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# unique_labels=np.unique(original_labels_train)\n",
        "\n",
        "# num_class = len(unique_labels)\n",
        "# lEnc = LabelEncoder()\n",
        "# lEnc.fit(unique_labels)\n",
        "\n",
        "# print(unique_labels)\n",
        "# print(lEnc.transform(unique_labels))\n",
        "\n",
        "# train_labels = lEnc.transform(original_labels_train)\n",
        "# test_labels = lEnc.transform(original_labels_test)\n",
        "\n",
        "# import torch\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# labels = train_labels.tolist()+test_labels.tolist()\n",
        "# labels = torch.LongTensor(labels).to(device)\n",
        "\n",
        "\n",
        "# ## Remove Stopwords and less frequent words, tokenize sentences\n",
        "\n",
        "\n",
        "# from nltk.corpus import stopwords\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# import nltk\n",
        "# import re\n",
        "\n",
        "# nltk.download('stopwords')\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# remove_limit = 2\n",
        "\n",
        "\n",
        "# def clean_str(string):\n",
        "#     string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "#     string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "#     string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "#     string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "#     string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "#     string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "#     string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "#     string = re.sub(r\",\", \" , \", string)\n",
        "#     string = re.sub(r\"!\", \" ! \", string)\n",
        "#     string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "#     string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "#     string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "#     string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "#     return string.strip().lower()\n",
        "\n",
        "# original_word_freq = {}  # to remove rare words\n",
        "# for sentence in sentences:\n",
        "#     temp = clean_str(sentence)\n",
        "#     word_list = temp.split()\n",
        "#     for word in word_list:\n",
        "#         if word in original_word_freq:\n",
        "#             original_word_freq[word] += 1\n",
        "#         else:\n",
        "#             original_word_freq[word] = 1\n",
        "\n",
        "# tokenize_sentences = []\n",
        "# word_list_dict = {}\n",
        "# for sentence in sentences:\n",
        "#     temp = clean_str(sentence)\n",
        "#     word_list_temp = temp.split()\n",
        "#     doc_words = []\n",
        "#     for word in word_list_temp:\n",
        "#         if word in original_word_freq and word not in stop_words and original_word_freq[word] >= remove_limit:\n",
        "#             doc_words.append(word)\n",
        "#             word_list_dict[word] = 1\n",
        "#     tokenize_sentences.append(doc_words)\n",
        "# word_list = list(word_list_dict.keys())\n",
        "# vocab_length = len(word_list)\n",
        "\n",
        "# #word to id dict\n",
        "# word_id_map = {}\n",
        "# for i in range(vocab_length):\n",
        "#     word_id_map[word_list[i]] = i\n",
        "\n",
        "\n",
        "# node_size = train_size + vocab_length + test_size\n",
        "\n",
        "\n",
        "# # Model input\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "# ## Build Graph\n",
        "\n",
        "\n",
        "# from math import log\n",
        "# row = []\n",
        "# col = []\n",
        "# weight = []\n",
        "\n",
        "\n",
        "# ### word-word: PMI\n",
        "\n",
        "# if EDGE >= 1:\n",
        "#     window_size = 20\n",
        "#     total_W = 0\n",
        "#     word_occurrence = {}\n",
        "#     word_pair_occurrence = {}\n",
        "\n",
        "#     def ordered_word_pair(a, b):\n",
        "#         if a > b:\n",
        "#             return b, a\n",
        "#         else:\n",
        "#             return a, b\n",
        "\n",
        "#     def update_word_and_word_pair_occurrence(q):\n",
        "#         unique_q = list(set(q))\n",
        "#         for i in unique_q:\n",
        "#             try:\n",
        "#                 word_occurrence[i] += 1\n",
        "#             except:\n",
        "#                 word_occurrence[i] = 1\n",
        "#         for i in range(len(unique_q)):\n",
        "#             for j in range(i+1, len(unique_q)):\n",
        "#                 word1 = unique_q[i]\n",
        "#                 word2 = unique_q[j]\n",
        "#                 word1, word2 = ordered_word_pair(word1, word2)\n",
        "#                 try:\n",
        "#                     word_pair_occurrence[(word1, word2)] += 1\n",
        "#                 except:\n",
        "#                     word_pair_occurrence[(word1, word2)] = 1\n",
        "\n",
        "\n",
        "#     for ind in tqdm(range(train_size+test_size)):\n",
        "#         words = tokenize_sentences[ind]\n",
        "\n",
        "#         q = []\n",
        "#         # push the first (window_size) words into a queue\n",
        "#         for i in range(min(window_size, len(words))):\n",
        "#             q += [word_id_map[words[i]]]\n",
        "#         # update the total number of the sliding windows\n",
        "#         total_W += 1\n",
        "#         # update the number of sliding windows that contain each word and word pair\n",
        "#         update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "#         now_next_word_index = window_size\n",
        "#         # pop the first word out and let the next word in, keep doing this until the end of the document\n",
        "#         while now_next_word_index<len(words):\n",
        "#             q.pop(0)\n",
        "#             q += [word_id_map[words[now_next_word_index]]]\n",
        "#             now_next_word_index += 1\n",
        "#             # update the total number of the sliding windows\n",
        "#             total_W += 1\n",
        "#             # update the number of sliding windows that contain each word and word pair\n",
        "#             update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "#     for word_pair in word_pair_occurrence:\n",
        "#         i = word_pair[0]\n",
        "#         j = word_pair[1]\n",
        "#         count = word_pair_occurrence[word_pair]\n",
        "#         word_freq_i = word_occurrence[i]\n",
        "#         word_freq_j = word_occurrence[j]\n",
        "#         pmi = log((count * total_W) / (word_freq_i * word_freq_j))\n",
        "#         if pmi <=0:\n",
        "#             continue\n",
        "#         row.append(train_size + i)\n",
        "#         col.append(train_size + j)\n",
        "#         weight.append(pmi)\n",
        "#         row.append(train_size + j)\n",
        "#         col.append(train_size + i)\n",
        "#         weight.append(pmi)\n",
        "\n",
        "\n",
        "\n",
        "# ### doc-word: Tf-idf\n",
        "\n",
        "\n",
        "# #get each word appears in which document\n",
        "# word_doc_list = {}\n",
        "# for word in word_list:\n",
        "#     word_doc_list[word]=[]\n",
        "\n",
        "# for i in range(len(tokenize_sentences)):\n",
        "#     doc_words = tokenize_sentences[i]\n",
        "#     unique_words = set(doc_words)\n",
        "#     for word in unique_words:\n",
        "#         exsit_list = word_doc_list[word]\n",
        "#         exsit_list.append(i)\n",
        "#         word_doc_list[word] = exsit_list\n",
        "\n",
        "# #document frequency\n",
        "# word_doc_freq = {}\n",
        "# for word, doc_list in word_doc_list.items():\n",
        "#     word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "# # term frequency\n",
        "# doc_word_freq = {}\n",
        "\n",
        "# for doc_id in range(len(tokenize_sentences)):\n",
        "#     words = tokenize_sentences[doc_id]\n",
        "#     for word in words:\n",
        "#         word_id = word_id_map[word]\n",
        "#         doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "#         if doc_word_str in doc_word_freq:\n",
        "#             doc_word_freq[doc_word_str] += 1\n",
        "#         else:\n",
        "#             doc_word_freq[doc_word_str] = 1\n",
        "\n",
        "\n",
        "# for i in range(len(tokenize_sentences)):\n",
        "#     words = tokenize_sentences[i]\n",
        "#     doc_word_set = set()\n",
        "#     for word in words:\n",
        "#         if word in doc_word_set:\n",
        "#             continue\n",
        "#         j = word_id_map[word]\n",
        "#         key = str(i) + ',' + str(j)\n",
        "#         freq = doc_word_freq[key]\n",
        "#         if i < train_size:\n",
        "#             row.append(i)\n",
        "#         else:\n",
        "#             row.append(i + vocab_length)\n",
        "#         col.append(train_size + j)\n",
        "#         idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n",
        "#         weight.append(freq * idf)\n",
        "#         doc_word_set.add(word)\n",
        "\n",
        "\n",
        "# ### doc-doc: jaccard\n",
        "\n",
        "# import nltk\n",
        "\n",
        "# if EDGE>=2:\n",
        "#     tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
        "#     jaccard_threshold = 0.2\n",
        "#     for i in tqdm(range(len(tokenize_sentences))):\n",
        "#         for j in range(i+1, len(tokenize_sentences)):\n",
        "#             jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n",
        "#             if jaccard_w > jaccard_threshold:\n",
        "#                 if i < train_size:\n",
        "#                     row.append(i)\n",
        "#                 else:\n",
        "#                     row.append(i + vocab_length)\n",
        "#                 if j < train_size:\n",
        "#                     col.append(j)\n",
        "#                 else:\n",
        "#                     col.append(vocab_length + j)\n",
        "#                 weight.append(jaccard_w)\n",
        "#                 if j < train_size:\n",
        "#                     row.append(j)\n",
        "#                 else:\n",
        "#                     row.append(j + vocab_length)\n",
        "#                 if i < train_size:\n",
        "#                     col.append(i)\n",
        "#                 else:\n",
        "#                     col.append(vocab_length + i)\n",
        "#                 weight.append(jaccard_w)\n",
        "\n"
      ],
      "metadata": {
        "id": "DI0Kiyt5qgKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl7PE5zXq2bW",
        "outputId": "6e9c1e12-32dc-463a-96be-60994968e8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "890"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQncGV5QSGns"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ### Adjacent matrix\n",
        "\n",
        "\n",
        "# import scipy.sparse as sp\n",
        "# adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
        "\n",
        "# # build symmetric adjacency matrix\n",
        "# adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "\n",
        "# def normalize_adj(adj):\n",
        "#     \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "#     adj = sp.coo_matrix(adj)\n",
        "#     rowsum = np.array(adj.sum(1))\n",
        "#     d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "#     d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "#     d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "#     return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo(), d_inv_sqrt\n",
        "\n",
        "# adj, norm_item = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "\n",
        "# def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "#     \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "#     sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "#     indices = torch.from_numpy(\n",
        "#         np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "#     values = torch.from_numpy(sparse_mx.data)\n",
        "#     shape = torch.Size(sparse_mx.shape)\n",
        "#     return torch.sparse.FloatTensor(indices, values, shape).to(device)\n",
        "\n",
        "# adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "\n",
        "# #import numpy as np\n",
        "# import torch\n",
        "# import scipy.sparse as sp\n",
        "# from tqdm import tqdm\n",
        "# from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel\n",
        "\n",
        "# # Assuming these variables are defined earlier in your code\n",
        "# # device, node_size, train_size, test_size, tokenize_sentences, word_list\n",
        "\n",
        "# def preprocess_features(features):\n",
        "#     \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "#     rowsum = np.array(features.sum(1))\n",
        "#     r_inv = np.power(rowsum, -1).flatten()\n",
        "#     r_inv[np.isinf(r_inv)] = 0.\n",
        "#     r_mat_inv = sp.diags(r_inv)\n",
        "#     features = r_mat_inv.dot(features)\n",
        "#     return features\n",
        "\n",
        "\n",
        "# # # Function to pad or truncate embeddings\n",
        "# # def pad_or_truncate_embedding(embedding, max_length):\n",
        "# #     \"\"\"Pad or truncate embedding to a fixed length.\"\"\"\n",
        "# #     if isinstance(embedding, (np.ndarray, list)):\n",
        "# #         if len(embedding) > max_length:\n",
        "# #             return embedding[:max_length]\n",
        "# #         else:\n",
        "# #             padding = np.zeros((max_length - len(embedding),))\n",
        "# #             return np.concatenate((embedding, padding))\n",
        "# #     else:\n",
        "# #         return np.zeros((max_length,))\n",
        "# def pad_or_truncate_embedding(embedding, max_length):\n",
        "#     \"\"\"Pad or truncate embedding to a fixed length\"\"\"\n",
        "#     if len(embedding) > max_length:\n",
        "#         return embedding[:max_length]\n",
        "#     else:\n",
        "#         padding = np.zeros((max_length - len(embedding),))\n",
        "#         return np.concatenate((embedding, padding))\n",
        "\n",
        "# if NODE == 0:\n",
        "#     features = torch.arange(node_size, dtype=torch.float32).to(device)\n",
        "#     print(\"within if\")\n",
        "# else:\n",
        "#     print(\"within else\")\n",
        "\n",
        "#     # Load BERT and GPT-2 models and tokenizers\n",
        "#     gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "#     gpt_model = GPT2Model.from_pretrained('gpt2').to(device)\n",
        "\n",
        "#     bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#     bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "#     sent_embs = []\n",
        "#     word_embs = {}\n",
        "\n",
        "#     gpt_embedding_dim = gpt_model.config.hidden_size\n",
        "#     bert_embedding_dim = bert_model.config.hidden_size\n",
        "#     combined_embedding_dim = gpt_embedding_dim + bert_embedding_dim\n",
        "\n",
        "#     for ind in tqdm(range(train_size + test_size)):\n",
        "#         sent = tokenize_sentences[ind]\n",
        "#         sentence_str = \" \".join(sent[:512])\n",
        "\n",
        "#         # GPT-2 embeddings\n",
        "#         gpt_inputs = gpt_tokenizer(sentence_str, return_tensors='pt').to(device)\n",
        "#         with torch.no_grad():\n",
        "#             gpt_outputs = gpt_model(**gpt_inputs)\n",
        "#         gpt_sentence_embedding = gpt_outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "#         gpt_tokens = gpt_tokenizer.tokenize(sentence_str)\n",
        "#         gpt_token_embeddings = gpt_outputs.last_hidden_state.squeeze().cpu().numpy()\n",
        "\n",
        "#         # BERT embeddings\n",
        "#         bert_inputs = bert_tokenizer(sentence_str, return_tensors='pt').to(device)\n",
        "#         with torch.no_grad():\n",
        "#             bert_outputs = bert_model(**bert_inputs)\n",
        "#         bert_sentence_embedding = bert_outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "#         bert_tokens = bert_tokenizer.tokenize(sentence_str)\n",
        "#         bert_token_embeddings = bert_outputs.last_hidden_state.squeeze().cpu().numpy()\n",
        "\n",
        "#         # Combined sentence embedding\n",
        "#         combined_sentence_embedding = np.concatenate((gpt_sentence_embedding, bert_sentence_embedding))\n",
        "#         sent_embs.append(combined_sentence_embedding)\n",
        "\n",
        "#         # Combined word embeddings\n",
        "#         for gpt_token, gpt_embedding in zip(gpt_tokens, gpt_token_embeddings):\n",
        "#             gpt_embedding = pad_or_truncate_embedding(gpt_embedding, gpt_embedding_dim)\n",
        "#             if gpt_token not in word_embs:\n",
        "#                 word_embs[gpt_token] = gpt_embedding\n",
        "#             else:\n",
        "#                 word_embs[gpt_token] = np.minimum(word_embs[gpt_token], gpt_embedding)\n",
        "\n",
        "#         for bert_token, bert_embedding in zip(bert_tokens, bert_token_embeddings):\n",
        "#             bert_embedding = pad_or_truncate_embedding(bert_embedding, bert_embedding_dim)\n",
        "#             if bert_token not in word_embs:\n",
        "#                 word_embs[bert_token] = bert_embedding\n",
        "#             else:\n",
        "#                 word_embs[bert_token] = np.minimum(word_embs[bert_token], bert_embedding)\n",
        "\n",
        "#     word_embs_list = [pad_or_truncate_embedding(word_embs[word], combined_embedding_dim) for word in word_list if word in word_embs]\n",
        "\n",
        "#     # Ensure all combined embeddings have the same length\n",
        "#     sent_embs = [pad_or_truncate_embedding(emb, combined_embedding_dim) for emb in sent_embs]\n",
        "\n",
        "#     combined_features = np.array(sent_embs[:train_size] + word_embs_list + sent_embs[train_size:])\n",
        "\n",
        "#     features = preprocess_features(sp.csr_matrix(combined_features)).todense()\n",
        "#     features = torch.FloatTensor(features).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYAP1yHqHSIu"
      },
      "outputs": [],
      "source": [
        "# # Model\n",
        "\n",
        "# ## GCN Layer\n",
        "\n",
        "\n",
        "# import math\n",
        "\n",
        "# import torch\n",
        "\n",
        "# from torch.nn.parameter import Parameter\n",
        "# from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "# class GraphConvolution(Module):\n",
        "#     \"\"\"\n",
        "#     Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, in_features, out_features,  drop_out = 0, activation=None, bias=True):\n",
        "#         super(GraphConvolution, self).__init__()\n",
        "#         self.in_features = in_features\n",
        "#         self.out_features = out_features\n",
        "#         self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "#         if bias:\n",
        "#             self.bias = Parameter(torch.zeros(1, out_features))\n",
        "#         else:\n",
        "#             self.register_parameter('bias', None)\n",
        "#         self.reset_parameters(in_features, out_features)\n",
        "#         self.dropout = torch.nn.Dropout(drop_out)\n",
        "#         self.activation =  activation\n",
        "\n",
        "#     def reset_parameters(self,in_features, out_features):\n",
        "#         stdv = np.sqrt(6.0/(in_features+out_features))\n",
        "#         # stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "#         self.weight.data.uniform_(-stdv, stdv)\n",
        "#         # if self.bias is not None:\n",
        "#         #     torch.nn.init.zeros_(self.bias)\n",
        "#             # self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "#     def forward(self, input, adj, feature_less = False):\n",
        "#         if feature_less:\n",
        "#             support = self.weight\n",
        "#             support = self.dropout(support)\n",
        "#         else:\n",
        "#             input = self.dropout(input)\n",
        "#             support = torch.mm(input, self.weight)\n",
        "#         output = torch.spmm(adj, support)\n",
        "#         if self.bias is not None:\n",
        "#             output = output + self.bias\n",
        "#         if self.activation is not None:\n",
        "#             output = self.activation(output)\n",
        "#         return output\n",
        "\n",
        "#     def __repr__(self):\n",
        "#         return self.__class__.__name__ + ' (' \\\n",
        "#                + str(self.in_features) + ' -> ' \\\n",
        "#                + str(self.out_features) + ')'\n",
        "\n",
        "# ## GCN Model\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class GCN(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout, n_layers = 2):\n",
        "#         super(GCN, self).__init__()\n",
        "#         self.n_layers = n_layers\n",
        "#         self.gc_list = []\n",
        "#         if n_layers >= 2:\n",
        "#             self.gc1 = GraphConvolution(nfeat, nhid, dropout, activation = nn.ReLU())\n",
        "#             self.gc_list = nn.ModuleList([GraphConvolution(nhid, nhid, dropout, activation = nn.ReLU()) for _ in range(self.n_layers-2)])\n",
        "#             self.gcf = GraphConvolution(nhid, nclass, dropout)\n",
        "#         else:\n",
        "#             self.gc1 = GraphConvolution(nfeat, nclass, dropout)\n",
        "\n",
        "#     def forward(self, x, adj):\n",
        "#         if self.n_layers>=2:\n",
        "#             x = self.gc1(x, adj, feature_less = True)\n",
        "#             for i in range(self.n_layers-2):\n",
        "#                 x = self.gc_list[i](x,adj)\n",
        "#             x = self.gcf(x,adj)\n",
        "#         else:\n",
        "#             x = self.gc1(x, adj, feature_less = True)\n",
        "#         return x\n",
        "\n",
        "# def cal_accuracy(predictions,labels):\n",
        "#     pred = torch.argmax(predictions,-1).cpu().tolist()\n",
        "#     lab = labels.cpu().tolist()\n",
        "#     cor = 0\n",
        "#     for i in range(len(pred)):\n",
        "#         if pred[i] == lab[i]:\n",
        "#             cor += 1\n",
        "#     return cor/len(pred)\n",
        "# # Training\n",
        "\n",
        "# ## Initialize model\n",
        "\n",
        "# import torch.optim as optim\n",
        "\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# ## Training and Validating\n",
        "# def generate_train_val(train_pro=0.9):\n",
        "#     real_train_size = int(train_pro*train_size)\n",
        "#     val_size = train_size-real_train_size\n",
        "\n",
        "#     idx_train = np.random.choice(train_size, real_train_size,replace=False)\n",
        "#     idx_train.sort()\n",
        "#     idx_val = []\n",
        "#     pointer = 0\n",
        "#     for v in range(train_size):\n",
        "#         if pointer<len(idx_train) and idx_train[pointer] == v:\n",
        "#             pointer +=1\n",
        "#         else:\n",
        "#             idx_val.append(v)\n",
        "#     idx_test = range(train_size+vocab_length, node_size)\n",
        "#     return idx_train, idx_val, idx_test\n",
        "\n",
        "# idx_train, idx_val, idx_test = generate_train_val()\n",
        "# import time\n",
        "\n",
        "# def train_model(show_result = True):\n",
        "#     val_loss = []\n",
        "#     for epoch in range(NUM_EPOCHS):\n",
        "#         t = time.time()\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         output= model(features, adj)\n",
        "#         loss_train = criterion(output[idx_train], labels[idx_train])\n",
        "#         acc_train = cal_accuracy(output[idx_train], labels[idx_train])\n",
        "#         loss_train.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         model.eval()\n",
        "#         output = model(features, adj)\n",
        "\n",
        "#         loss_val = criterion(output[idx_val], labels[idx_val])\n",
        "#         val_loss.append(loss_val.item())\n",
        "#         acc_val = cal_accuracy(output[idx_val], labels[idx_val])\n",
        "#         if show_result:\n",
        "#             print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "#                     'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "#                     'acc_train: {:.4f}'.format(acc_train),\n",
        "#                     'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "#                     'acc_val: {:.4f}'.format(acc_val),\n",
        "#                     'time: {:.4f}s'.format(time.time() - t))\n",
        "#             #if (epoch==100):\n",
        "#         # if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
        "#         #     if show_result:\n",
        "#         #         print(\"Early Stopping...\")\n",
        "#         #     break\n",
        "\n",
        "# train_model()\n",
        "# ## Evaluation\n",
        "# from sklearn.metrics import f1_score, accuracy_score\n",
        "# def test():\n",
        "#     model.eval()\n",
        "#     output = model(features, adj)\n",
        "#     predictions = torch.argmax(output[idx_test],-1).cpu().tolist()\n",
        "#     acc = accuracy_score(test_labels,predictions)\n",
        "#     f11 = f1_score(test_labels,predictions, average='macro')\n",
        "#     f12 = f1_score(test_labels,predictions, average = 'weighted')\n",
        "#     return acc, f11, f12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGeLX-I-HSGA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsrrJ5UqNMhP"
      },
      "outputs": [],
      "source": [
        "# # Test 10 times\n",
        "\n",
        "\n",
        "# test_acc_list = []\n",
        "# test_f11_list = []\n",
        "# test_f12_list = []\n",
        "\n",
        "# for t in range(11):\n",
        "#     model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "#     idx_train, idx_val, idx_test = generate_train_val()\n",
        "#     train_model(show_result=False)\n",
        "#     acc, f11, f12 = test()\n",
        "#     test_acc_list.append(acc)\n",
        "#     test_f11_list.append(f11)\n",
        "#     test_f12_list.append(f12)\n",
        "\n",
        "\n",
        "# print(\"Accuracy:\",np.round(np.mean(test_acc_list),4))\n",
        "# print(\"Macro F1:\",np.round(np.mean(test_f11_list),4))\n",
        "# print(\"Weighted F1:\",np.round(np.mean(test_f12_list),4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aUTSRCfHSAk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWHf6odIO8R9"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import f1_score, accuracy_score\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "\n",
        "# def test_and_save_results():\n",
        "#     model.eval()\n",
        "\n",
        "#     try:\n",
        "#         # Generate predictions from the model\n",
        "#         with torch.no_grad():\n",
        "#             output = model(features, adj)\n",
        "#             # Ensure output is appropriately indexed for test samples\n",
        "#             predictions = torch.argmax(output[idx_test], -1).cpu().tolist()\n",
        "\n",
        "#         # Check lengths for debugging\n",
        "#         print(f\"Length of X_test: {len(X_test)}\")\n",
        "#         print(f\"Length of y_test: {len(y_test)}\")\n",
        "#         print(f\"Length of predictions: {len(predictions)}\")\n",
        "\n",
        "#         # Ensure lengths match before creating DataFrame\n",
        "#         if len(X_test) != len(y_test) or len(y_test) != len(predictions):\n",
        "#             raise ValueError(\"Mismatch in lengths of X_test, y_test, and predictions\")\n",
        "\n",
        "#         # Convert predictions and labels to a DataFrame\n",
        "#         results_df = pd.DataFrame({\n",
        "#             'Text': X_test.tolist(),\n",
        "#             'True Label': y_test.tolist(),\n",
        "#             'Predicted Label': predictions\n",
        "#         })\n",
        "\n",
        "#         # Save the DataFrame to an Excel file\n",
        "#         results_df.to_excel(\"/content/gdrive/MyDrive/dualembtwit15.xlsx\", index=False)\n",
        "\n",
        "#         # Calculate metrics\n",
        "#         acc = accuracy_score(y_test, predictions)\n",
        "#         f11 = f1_score(y_test, predictions, average='macro')\n",
        "#         f12 = f1_score(y_test, predictions, average='weighted')\n",
        "\n",
        "#         return acc, f11, f12\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error occurred: {e}\")\n",
        "#         return None, None, None\n",
        "\n",
        "# # Call the function\n",
        "# accuracy, f1_macro, f1_weighted = test_and_save_results()\n",
        "# if accuracy is not None:\n",
        "#     print(f\"Accuracy: {accuracy}, F1-Macro: {f1_macro}, F1-Weighted: {f1_weighted}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdUXRLtKNrmm"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import f1_score, accuracy_score\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "\n",
        "# def test_and_save_results():\n",
        "#     model.eval()\n",
        "\n",
        "#     # Generate predictions from the model\n",
        "#     with torch.no_grad():\n",
        "#         output = model(features, adj)\n",
        "#         # Ensure you are using the correct index for test samples\n",
        "#         predictions = torch.argmax(output[idx_test], -1).cpu().tolist()\n",
        "\n",
        "#     # Check lengths for debugging\n",
        "#     print(f\"Length of X_test: {len(X_test)}\")\n",
        "#     print(f\"Length of y_test: {len(y_test)}\")\n",
        "#     print(f\"Length of predictions: {len(predictions)}\")\n",
        "\n",
        "#     # Ensure lengths match before creating DataFrame\n",
        "#     if len(X_test) != len(y_test) or len(y_test) != len(predictions):\n",
        "#         raise ValueError(\"Mismatch in lengths of X_test, y_test, and predictions\")\n",
        "\n",
        "#     # Convert predictions and labels to a DataFrame\n",
        "#     results_df = pd.DataFrame({\n",
        "#         'Text': X_test.tolist(),\n",
        "#         'True Label': y_test.tolist(),\n",
        "#         'Predicted Label': predictions\n",
        "#     })\n",
        "\n",
        "#     # Save the DataFrame to an Excel file\n",
        "#     results_df.to_excel(\"/content/gdrive/MyDrive/dualembtwit15.xlsx\", index=False)\n",
        "\n",
        "#     # Calculate metrics\n",
        "#     acc = accuracy_score(y_test, predictions)\n",
        "#     f11 = f1_score(y_test, predictions, average='macro')\n",
        "#     f12 = f1_score(y_test, predictions, average='weighted')\n",
        "\n",
        "#     return acc, f11, f12\n",
        "\n",
        "# # Call the function\n",
        "# accuracy, f1_macro, f1_weighted = test_and_save_results()\n",
        "# print(f\"Accuracy: {accuracy}, F1-Macro: {f1_macro}, F1-Weighted: {f1_weighted}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBhrYm4RNO71"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh_9macdNO5o"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import torch\n",
        "# import pandas as pd\n",
        "# import scipy.sparse as sp\n",
        "# from tqdm import tqdm\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import f1_score, accuracy_score\n",
        "# from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel\n",
        "# from nltk.corpus import stopwords\n",
        "# import nltk\n",
        "# import re\n",
        "\n",
        "# # Ensure NLTK stopwords are downloaded\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# # Load data\n",
        "# traindf = pd.read_excel('/content/gdrive/MyDrive/twitter15xlsl.xlsx')\n",
        "# X = traindf['Text']\n",
        "# Y = traindf['Label']\n",
        "\n",
        "# # Split the dataset\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, shuffle=True)\n",
        "# X_train = X_train.reset_index(drop=True)\n",
        "# X_test = X_test.reset_index(drop=True)\n",
        "# y_train = y_train.reset_index(drop=True)\n",
        "# y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "# # Set parameters\n",
        "# EDGE = 2\n",
        "# NODE = 1\n",
        "# NUM_LAYERS = 2\n",
        "# HIDDEN_DIM = 200\n",
        "# DROP_OUT = 0.5\n",
        "# LR = 0.02\n",
        "# WEIGHT_DECAY = 0\n",
        "# EARLY_STOPPING = 10\n",
        "# NUM_EPOCHS = 200\n",
        "\n",
        "# # Label Encoding\n",
        "# unique_labels = np.unique(y_train)\n",
        "# lEnc = LabelEncoder()\n",
        "# lEnc.fit(unique_labels)\n",
        "# train_labels = lEnc.transform(y_train)\n",
        "# test_labels = lEnc.transform(y_test)\n",
        "\n",
        "# # Preprocess sentences\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# remove_limit = 2\n",
        "\n",
        "# def clean_str(string):\n",
        "#     string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "#     string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "#     return string.strip().lower()\n",
        "\n",
        "# def tokenize_and_filter(sentences):\n",
        "#     original_word_freq = {}\n",
        "#     for sentence in sentences:\n",
        "#         temp = clean_str(sentence)\n",
        "#         word_list = temp.split()\n",
        "#         for word in word_list:\n",
        "#             original_word_freq[word] = original_word_freq.get(word, 0) + 1\n",
        "\n",
        "#     tokenize_sentences = []\n",
        "#     word_list_dict = {}\n",
        "#     for sentence in sentences:\n",
        "#         temp = clean_str(sentence)\n",
        "#         word_list_temp = temp.split()\n",
        "#         doc_words = []\n",
        "#         for word in word_list_temp:\n",
        "#             if word in original_word_freq and word not in stop_words and original_word_freq[word] >= remove_limit:\n",
        "#                 doc_words.append(word)\n",
        "#                 word_list_dict[word] = 1\n",
        "#         tokenize_sentences.append(doc_words)\n",
        "#     word_list = list(word_list_dict.keys())\n",
        "#     return tokenize_sentences, word_list\n",
        "\n",
        "# tokenize_sentences, word_list = tokenize_and_filter(X_train.tolist() + X_test.tolist())\n",
        "\n",
        "# # Build Graph\n",
        "# def build_graph():\n",
        "#     row = []\n",
        "#     col = []\n",
        "#     weight = []\n",
        "\n",
        "#     if EDGE >= 1:\n",
        "#         window_size = 20\n",
        "#         total_W = 0\n",
        "#         word_occurrence = {}\n",
        "#         word_pair_occurrence = {}\n",
        "\n",
        "#         def ordered_word_pair(a, b):\n",
        "#             return (b, a) if a > b else (a, b)\n",
        "\n",
        "#         def update_word_and_word_pair_occurrence(q):\n",
        "#             unique_q = list(set(q))\n",
        "#             for i in unique_q:\n",
        "#                 word_occurrence[i] = word_occurrence.get(i, 0) + 1\n",
        "#             for i in range(len(unique_q)):\n",
        "#                 for j in range(i+1, len(unique_q)):\n",
        "#                     word1, word2 = ordered_word_pair(unique_q[i], unique_q[j])\n",
        "#                     word_pair_occurrence[(word1, word2)] = word_pair_occurrence.get((word1, word2), 0) + 1\n",
        "\n",
        "#         for ind in tqdm(range(len(tokenize_sentences))):\n",
        "#             words = tokenize_sentences[ind]\n",
        "#             q = [word_id_map[word] for word in words[:window_size]]\n",
        "#             total_W += 1\n",
        "#             update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "#             now_next_word_index = window_size\n",
        "#             while now_next_word_index < len(words):\n",
        "#                 q.pop(0)\n",
        "#                 q.append(word_id_map[words[now_next_word_index]])\n",
        "#                 now_next_word_index += 1\n",
        "#                 total_W += 1\n",
        "#                 update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "#         for word_pair, count in word_pair_occurrence.items():\n",
        "#             i, j = word_pair\n",
        "#             word_freq_i = word_occurrence[i]\n",
        "#             word_freq_j = word_occurrence[j]\n",
        "#             pmi = np.log((count * total_W) / (word_freq_i * word_freq_j))\n",
        "#             if pmi > 0:\n",
        "#                 row.extend([train_size + i, train_size + j])\n",
        "#                 col.extend([train_size + j, train_size + i])\n",
        "#                 weight.extend([pmi, pmi])\n",
        "\n",
        "#     if EDGE >= 2:\n",
        "#         tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
        "#         jaccard_threshold = 0.2\n",
        "#         for i in tqdm(range(len(tokenize_sentences))):\n",
        "#             for j in range(i+1, len(tokenize_sentences)):\n",
        "#                 jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n",
        "#                 if jaccard_w > jaccard_threshold:\n",
        "#                     row.extend([i + (vocab_length if i >= train_size else 0), j + (vocab_length if j >= train_size else 0)])\n",
        "#                     col.extend([j + (vocab_length if j >= train_size else 0), i + (vocab_length if i >= train_size else 0)])\n",
        "#                     weight.extend([jaccard_w, jaccard_w])\n",
        "\n",
        "#     return sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
        "\n",
        "# adj = build_graph()\n",
        "\n",
        "# def normalize_adj(adj):\n",
        "#     adj = sp.coo_matrix(adj)\n",
        "#     rowsum = np.array(adj.sum(1))\n",
        "#     d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "#     d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "#     d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "#     return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo(), d_inv_sqrt\n",
        "\n",
        "# adj, norm_item = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "# def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "#     sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "#     indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "#     values = torch.from_numpy(sparse_mx.data)\n",
        "#     shape = torch.Size(sparse_mx.shape)\n",
        "#     return torch.sparse.FloatTensor(indices, values, shape).to(device)\n",
        "\n",
        "# adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "# # Feature extraction\n",
        "# def preprocess_features(features):\n",
        "#     rowsum = np.array(features.sum(1))\n",
        "#     r_inv = np.power(rowsum, -1).flatten()\n",
        "#     r_inv[np.isinf(r_inv)] = 0.\n",
        "#     r_mat_inv = sp.diags(r_inv)\n",
        "#     features = r_mat_inv.dot(features)\n",
        "#     return features\n",
        "\n",
        "# def pad_or_truncate_embedding(embedding, max_length):\n",
        "#     if len(embedding) > max_length:\n",
        "#         return embedding[:max_length]\n",
        "#     else:\n",
        "#         padding = np.zeros((max_length - len(embedding),))\n",
        "#         return np.concatenate((embedding, padding))\n",
        "\n",
        "# if NODE == 0:\n",
        "#     features = torch.arange(node_size, dtype=torch.float32).to(device)\n",
        "# else:\n",
        "#     gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "#     gpt_model = GPT2Model.from_pretrained('gpt2').to(device)\n",
        "#     bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#     bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "#     sent_embs = []\n",
        "#     word_embs = {}\n",
        "\n",
        "#     gpt_embedding_dim = gpt_model.config.hidden_size\n",
        "#     bert_embedding_dim = bert_model.config.hidden_size\n",
        "#     combined_embedding_dim = gpt_embedding_dim + bert_embedding_dim\n",
        "\n",
        "#     for ind in tqdm(range(len(tokenize_sentences))):\n",
        "#         sent = tokenize_sentences[ind]\n",
        "#         sentence_str = \" \".join(sent[:512])\n",
        "\n",
        "#         # GPT-2 embeddings\n",
        "#         gpt_inputs = gpt_tokenizer(sentence_str, return_tensors='pt').to(device)\n",
        "#         with torch.no_grad():\n",
        "#             gpt_outputs = gpt_model(**gpt_inputs)\n",
        "#         gpt_sentence_embedding = gpt_outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "#         gpt_token_embeddings = gpt_outputs.last_hidden_state.squeeze().cpu().numpy()\n",
        "\n",
        "#         # BERT embeddings\n",
        "#         bert_inputs = bert_tokenizer(sentence_str, return_tensors='pt').to(device)\n",
        "#         with torch.no_grad():\n",
        "#             bert_outputs = bert_model(**bert_inputs)\n",
        "#         bert_sentence_embedding = bert_outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "#         bert_token_embeddings = bert_outputs.last_hidden_state.squeeze().cpu().numpy()\n",
        "\n",
        "#         # Combined sentence embedding\n",
        "#         combined_sentence_embedding = np.concatenate((gpt_sentence_embedding, bert_sentence_embedding))\n",
        "#         sent_embs.append(combined_sentence_embedding)\n",
        "\n",
        "#         # Combined word embeddings\n",
        "#         for gpt_token, gpt_embedding in zip(gpt_tokenizer.tokenize(sentence_str), gpt_token_embeddings):\n",
        "#             gpt_embedding = pad_or_truncate_embedding(gpt_embedding, 768)  # Adjust size if necessary\n",
        "#             word_embs[gpt_token] = np.concatenate((gpt_embedding, np.zeros(768)))  # Example of concatenation\n",
        "\n",
        "#         for bert_token, bert_embedding in zip(bert_tokenizer.tokenize(sentence_str), bert_token_embeddings):\n",
        "#             bert_embedding = pad_or_truncate_embedding(bert_embedding, 768)\n",
        "#             if bert_token in word_embs:\n",
        "#                 word_embs[bert_token] = np.concatenate((word_embs[bert_token], bert_embedding))\n",
        "#             else:\n",
        "#                 word_embs[bert_token] = np.concatenate((np.zeros(768), bert_embedding))\n",
        "\n",
        "#     feature_matrix = np.vstack(sent_embs)\n",
        "#     feature_matrix = preprocess_features(sp.csr_matrix(feature_matrix))\n",
        "#     features = torch.tensor(feature_matrix.todense()).float().to(device)\n",
        "\n",
        "# # Define your model class (example)\n",
        "# class GCNModel(torch.nn.Module):\n",
        "#     def __init__(self, in_features, out_features):\n",
        "#         super(GCNModel, self).__init__()\n",
        "#         self.gc1 = torch.nn.Linear(in_features, out_features)\n",
        "#         self.gc2 = torch.nn.Linear(out_features, len(unique_labels))\n",
        "\n",
        "#     def forward(self, features, adj):\n",
        "#         x = self.gc1(features)\n",
        "#         x = torch.relu(x)\n",
        "#         x = torch.dropout(x, p=DROP_OUT, train=self.training)\n",
        "#         x = torch.matmul(adj, x)\n",
        "#         x = self.gc2(x)\n",
        "#         return x\n",
        "\n",
        "# # Initialize model, optimizer, and loss function\n",
        "# model = GCNModel(features.size(1), HIDDEN_DIM).to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# def train():\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     output = model(features, adj)\n",
        "#     loss = criterion(output[train_idx], torch.tensor(train_labels).to(device))\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "# def evaluate():\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         output = model(features, adj)\n",
        "#         predictions = torch.argmax(output[idx_test], -1).cpu().tolist()\n",
        "#         acc = accuracy_score(test_labels, predictions)\n",
        "#         f11 = f1_score(test_labels, predictions, average='macro')\n",
        "#         f12 = f1_score(test_labels, predictions, average='weighted')\n",
        "\n",
        "#         # Convert predictions and labels to a DataFrame\n",
        "#         results_df = pd.DataFrame({\n",
        "#             'Text': X_test.tolist(),\n",
        "#             'True Label': y_test.tolist(),\n",
        "#             'Predicted Label': predictions\n",
        "#         })\n",
        "\n",
        "#         # Save the DataFrame to an Excel file\n",
        "#         results_df.to_excel(\"/content/gdrive/MyDrive/dualembtwit15.xlsx\", index=False)\n",
        "\n",
        "#         return acc, f11, f12\n",
        "\n",
        "# # Train the model\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     train()\n",
        "\n",
        "# # Evaluate and save results\n",
        "# accuracy, f1_macro, f1_weighted = evaluate()\n",
        "# print(f\"Accuracy: {accuracy}, F1-Macro: {f1_macro}, F1-Weighted: {f1_weighted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m8SJtlpNO3X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzSB16E5NO0v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpQGwiBBNOyH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfuo87JBNOv8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXr1FPpRNOts"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0QtYXNJNOrB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f35oEk-NOo_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVuvor1SNOmn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLN05v4mNOjv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgFuMQgMNOho"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1h-XJ4RNOce"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE1S-6nwNOaH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfjRMlPqNOXo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmdlpMbANOVf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejh2iNCsNOTH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qipiFiVNOQc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTY6c5bqNONc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1c5efec4c76459aac7a63e93c2f63b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a87e822c76324659b23fad3d3d9412d1",
              "IPY_MODEL_802796ba98e2414cb7898d5956b74bb8",
              "IPY_MODEL_b8b89ac4cd4f4428b1653eb0ee932b22"
            ],
            "layout": "IPY_MODEL_b4938aa948a6456ea0b520603374849e"
          }
        },
        "a87e822c76324659b23fad3d3d9412d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f8b34eabe6d48e28016f5a923019a2a",
            "placeholder": "​",
            "style": "IPY_MODEL_06100742bd6b4013bcd4db35ac30ddaa",
            "value": "100%"
          }
        },
        "802796ba98e2414cb7898d5956b74bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d858994e267440c900a3896524ae428",
            "max": 1221,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b54ebb8b9094262831ee077adb42542",
            "value": 1221
          }
        },
        "b8b89ac4cd4f4428b1653eb0ee932b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e8b34f741744305bf4c1472d2941fcc",
            "placeholder": "​",
            "style": "IPY_MODEL_c731ec43a7364bc39a15afdad4a9d8a7",
            "value": " 1221/1221 [00:00&lt;00:00, 12138.07it/s]"
          }
        },
        "b4938aa948a6456ea0b520603374849e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f8b34eabe6d48e28016f5a923019a2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06100742bd6b4013bcd4db35ac30ddaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d858994e267440c900a3896524ae428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b54ebb8b9094262831ee077adb42542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e8b34f741744305bf4c1472d2941fcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c731ec43a7364bc39a15afdad4a9d8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}